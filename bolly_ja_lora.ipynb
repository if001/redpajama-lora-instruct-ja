{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMAVhZPBNdE9XyiC2anid4P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c0cc5f555694493aedd0c7b509e9ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbee0291e0904068935648a879805b81",
              "IPY_MODEL_69acb7be4fa04568a06baf63687846e5",
              "IPY_MODEL_6160b61c98024923b3f7df554977fba0"
            ],
            "layout": "IPY_MODEL_e9482a0e1ffb4653ab0bd5ad4dc61e38"
          }
        },
        "dbee0291e0904068935648a879805b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd4dc1c0af724ecb9ffbc2b6fc9e6e9f",
            "placeholder": "​",
            "style": "IPY_MODEL_992551d49a2a4fccb1d17ca998fb3036",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "69acb7be4fa04568a06baf63687846e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7bbc096cff74869a7aed9480c488928",
            "max": 534,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca8ab9281fc24f6daf2ac065945c5f34",
            "value": 534
          }
        },
        "6160b61c98024923b3f7df554977fba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39aef34bbe0e4336b21a2640974ad4d7",
            "placeholder": "​",
            "style": "IPY_MODEL_7dba55e7c76143caa321b123bc376ad7",
            "value": " 534/534 [00:00&lt;00:00, 45.3kB/s]"
          }
        },
        "e9482a0e1ffb4653ab0bd5ad4dc61e38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4dc1c0af724ecb9ffbc2b6fc9e6e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "992551d49a2a4fccb1d17ca998fb3036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7bbc096cff74869a7aed9480c488928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca8ab9281fc24f6daf2ac065945c5f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39aef34bbe0e4336b21a2640974ad4d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dba55e7c76143caa321b123bc376ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f3e8570231a4ce6a84edaa8fb3cc983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db45a4fe56664d289c4143efc898166b",
              "IPY_MODEL_bc67e9c89abc48a8a4401cd20a40c750",
              "IPY_MODEL_a3368d5067354fafbec318e8ae8393ee"
            ],
            "layout": "IPY_MODEL_2ebdb5f00ae744888f9a70c6eab9ce72"
          }
        },
        "db45a4fe56664d289c4143efc898166b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4308843af5b1495aac90eae6945b000f",
            "placeholder": "​",
            "style": "IPY_MODEL_7ef77eb18f0e4a77934397da6ee773ca",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "bc67e9c89abc48a8a4401cd20a40c750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44cfdf235a4b4b4cb6ace8cd389e0448",
            "max": 7365670537,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29cf990756aa4e50a6394f11849e250c",
            "value": 7365670537
          }
        },
        "a3368d5067354fafbec318e8ae8393ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f83fde88524e6691975071d4888fcc",
            "placeholder": "​",
            "style": "IPY_MODEL_14e8c77e8c1244f2a86361ca8cc92036",
            "value": " 7.37G/7.37G [00:54&lt;00:00, 141MB/s]"
          }
        },
        "2ebdb5f00ae744888f9a70c6eab9ce72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4308843af5b1495aac90eae6945b000f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ef77eb18f0e4a77934397da6ee773ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44cfdf235a4b4b4cb6ace8cd389e0448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29cf990756aa4e50a6394f11849e250c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35f83fde88524e6691975071d4888fcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14e8c77e8c1244f2a86361ca8cc92036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "672c4cf8b2f442569edc517ef51fbfbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0f78db418e24b3a9cb3221d4939bdc5",
              "IPY_MODEL_28c12cca0a014b9d942285b4d76f10f1",
              "IPY_MODEL_15f5f7449f4245d08e8a270d8c7de891"
            ],
            "layout": "IPY_MODEL_4feb275ab45c4383bc94f9e3434deaa7"
          }
        },
        "f0f78db418e24b3a9cb3221d4939bdc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61d7638fccf541109e354e974b2effe9",
            "placeholder": "​",
            "style": "IPY_MODEL_ed3e3d97999945d6946571f1b256b71d",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "28c12cca0a014b9d942285b4d76f10f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3345d1fb223c4d74ae68571113fa9cea",
            "max": 604,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24618ecd0ff14381823f62c4a3336c5f",
            "value": 604
          }
        },
        "15f5f7449f4245d08e8a270d8c7de891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3009e290b85d47fdbe565dd14e01e36b",
            "placeholder": "​",
            "style": "IPY_MODEL_85530c38e9134f10972a963e2e09c091",
            "value": " 604/604 [00:00&lt;00:00, 55.2kB/s]"
          }
        },
        "4feb275ab45c4383bc94f9e3434deaa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61d7638fccf541109e354e974b2effe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed3e3d97999945d6946571f1b256b71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3345d1fb223c4d74ae68571113fa9cea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24618ecd0ff14381823f62c4a3336c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3009e290b85d47fdbe565dd14e01e36b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85530c38e9134f10972a963e2e09c091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc4e3cc1df0b4375ac3db82af02f0e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9a67f3df53243f887a7ce74eb58c223",
              "IPY_MODEL_41b4d3df97be4641a5284b3007e7b88e",
              "IPY_MODEL_2a0df80edfc04305a19ab114c5d77d63"
            ],
            "layout": "IPY_MODEL_adf0e353f15045f39601281eab678ec4"
          }
        },
        "e9a67f3df53243f887a7ce74eb58c223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1acf1f5660e54e29af708aa2b223256f",
            "placeholder": "​",
            "style": "IPY_MODEL_4db3b92873d74935adc97dde748da509",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "41b4d3df97be4641a5284b3007e7b88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f8bf6bab144451c84f985edacb4208d",
            "max": 5686113497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ac01904091c4f539ca45d4a252026f8",
            "value": 5686113497
          }
        },
        "2a0df80edfc04305a19ab114c5d77d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a3aa8128c6488fbef88240fa342b95",
            "placeholder": "​",
            "style": "IPY_MODEL_d5fddd65782549c4bf2063406842c85f",
            "value": " 5.69G/5.69G [04:12&lt;00:00, 23.3MB/s]"
          }
        },
        "adf0e353f15045f39601281eab678ec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1acf1f5660e54e29af708aa2b223256f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db3b92873d74935adc97dde748da509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f8bf6bab144451c84f985edacb4208d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac01904091c4f539ca45d4a252026f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09a3aa8128c6488fbef88240fa342b95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5fddd65782549c4bf2063406842c85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65748d8bb0c94b2ea2a58eef7eba0288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e34ac4304ffa484a93564506d75f40dc",
              "IPY_MODEL_ed17ecad2f954c44b6fea83c17388289",
              "IPY_MODEL_8d2adaed69ed4891a989540975bf8edb"
            ],
            "layout": "IPY_MODEL_210c87cef9d74d2797a8ebd1e7857073"
          }
        },
        "e34ac4304ffa484a93564506d75f40dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60e6a476b12a4ce78b8293bf4cd71523",
            "placeholder": "​",
            "style": "IPY_MODEL_da2d9b5d0a8a4269908be5beb63fc530",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "ed17ecad2f954c44b6fea83c17388289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d5c06003e264669b78e439f7aba39f9",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5b9091f59314cd5a6970ae939fce48a",
            "value": 111
          }
        },
        "8d2adaed69ed4891a989540975bf8edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11c253764f61418f86fc7f448889903e",
            "placeholder": "​",
            "style": "IPY_MODEL_c74aff2e39dd4b8ab5beb2b928a4c729",
            "value": " 111/111 [00:00&lt;00:00, 8.68kB/s]"
          }
        },
        "210c87cef9d74d2797a8ebd1e7857073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e6a476b12a4ce78b8293bf4cd71523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2d9b5d0a8a4269908be5beb63fc530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d5c06003e264669b78e439f7aba39f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5b9091f59314cd5a6970ae939fce48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11c253764f61418f86fc7f448889903e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74aff2e39dd4b8ab5beb2b928a4c729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1fd6db4a0c245eaa56701c176487f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bdadf248d7649cea4886965c1804a37",
              "IPY_MODEL_b2a5032befc549339aec851bcf83ebce",
              "IPY_MODEL_e99536e5f4754d2695c6c06241e7d833"
            ],
            "layout": "IPY_MODEL_912cad6bb1fb46cd838abaf461a1bc0c"
          }
        },
        "4bdadf248d7649cea4886965c1804a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f76c91684e41c5839ff59ce45b1be9",
            "placeholder": "​",
            "style": "IPY_MODEL_78270174984243efb67db5bb57d58cd6",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "b2a5032befc549339aec851bcf83ebce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebb7dcaf2ffb49f298525f944dafc127",
            "max": 534,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2dd3c48788f4d708664d6ee615aa186",
            "value": 534
          }
        },
        "e99536e5f4754d2695c6c06241e7d833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c01ae7dfaa54ace9dec47b4d2ccfdd7",
            "placeholder": "​",
            "style": "IPY_MODEL_c4d0fcccfea84b7faf15f4e53ab91c37",
            "value": " 534/534 [00:00&lt;00:00, 42.4kB/s]"
          }
        },
        "912cad6bb1fb46cd838abaf461a1bc0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43f76c91684e41c5839ff59ce45b1be9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78270174984243efb67db5bb57d58cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebb7dcaf2ffb49f298525f944dafc127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2dd3c48788f4d708664d6ee615aa186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c01ae7dfaa54ace9dec47b4d2ccfdd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4d0fcccfea84b7faf15f4e53ab91c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1daa1075e08429b8864ea7e94ba94da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6573a0ec4d3496daa14623628ed37f1",
              "IPY_MODEL_f688f8c26c1e45f89db98a7854ee2bf7",
              "IPY_MODEL_cae0c6cb41294e5babf532805f06694c"
            ],
            "layout": "IPY_MODEL_03b9683133b745baaa6ca65c4ba23f63"
          }
        },
        "a6573a0ec4d3496daa14623628ed37f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_734fde80218f45c59457c779f4592e17",
            "placeholder": "​",
            "style": "IPY_MODEL_812a1cf090ba4285940cb3010ad6f838",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "f688f8c26c1e45f89db98a7854ee2bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5368175d7b52453bae2e3ec3c47296dd",
            "max": 7365670537,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_504fea8dae19402182726b6b39d395ea",
            "value": 7365670537
          }
        },
        "cae0c6cb41294e5babf532805f06694c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6190fcdce0f6453bb399d3da7140bab4",
            "placeholder": "​",
            "style": "IPY_MODEL_bb50053d3e374641829c22c2dabd8497",
            "value": " 7.37G/7.37G [00:38&lt;00:00, 172MB/s]"
          }
        },
        "03b9683133b745baaa6ca65c4ba23f63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "734fde80218f45c59457c779f4592e17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812a1cf090ba4285940cb3010ad6f838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5368175d7b52453bae2e3ec3c47296dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "504fea8dae19402182726b6b39d395ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6190fcdce0f6453bb399d3da7140bab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb50053d3e374641829c22c2dabd8497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65eb5c3a92d849c1bd4815fcd5bfd44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf2ef0b7773945a8a5f0b9c9c58b5088",
              "IPY_MODEL_746eae67445c4fd093f9130e72eba2c2",
              "IPY_MODEL_8280f2da3fbd4e71b27ea703682369e0"
            ],
            "layout": "IPY_MODEL_490271b3ae674207b2066da00f244645"
          }
        },
        "cf2ef0b7773945a8a5f0b9c9c58b5088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_258a6c67f6a443559b8f6295a1188e87",
            "placeholder": "​",
            "style": "IPY_MODEL_bb4be8fef0f14433b07a1d8c8936b95f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "746eae67445c4fd093f9130e72eba2c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81b0daa69d14407a8b6640b54068762d",
            "max": 45,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58926460f03047e294352470981c9e9a",
            "value": 45
          }
        },
        "8280f2da3fbd4e71b27ea703682369e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7530f4e6b38e4ef6a1366e523cb671f8",
            "placeholder": "​",
            "style": "IPY_MODEL_b1b7dbf43da1409786daedddb1f492f3",
            "value": " 45/45 [01:05&lt;00:00,  1.52s/it]"
          }
        },
        "490271b3ae674207b2066da00f244645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "258a6c67f6a443559b8f6295a1188e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4be8fef0f14433b07a1d8c8936b95f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b0daa69d14407a8b6640b54068762d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58926460f03047e294352470981c9e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7530f4e6b38e4ef6a1366e523cb671f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b7dbf43da1409786daedddb1f492f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69a4c67d8c484de080d7ac62dfe7d72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a046d5ce7e6a45f990958d013e83f84c",
              "IPY_MODEL_593957576f004b7d8f99c1fae6e7d1f6",
              "IPY_MODEL_16528f2753cf498eaf00da0258fb427e"
            ],
            "layout": "IPY_MODEL_013972997f6c407d99983fffe7329938"
          }
        },
        "a046d5ce7e6a45f990958d013e83f84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf9a8f672eb24a03b5c644376ae36ce0",
            "placeholder": "​",
            "style": "IPY_MODEL_ddb3f448451e4d028bd00818dfc3b0da",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "593957576f004b7d8f99c1fae6e7d1f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fac3844a2dd488bb377aebd0252847e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5526bd223c5245a4b82247783c749ea6",
            "value": 2
          }
        },
        "16528f2753cf498eaf00da0258fb427e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3551c1b30926408599d4c73fce71fd53",
            "placeholder": "​",
            "style": "IPY_MODEL_381650067cbf400289234a1a5d64aa40",
            "value": " 2/2 [00:55&lt;00:00, 25.73s/it]"
          }
        },
        "013972997f6c407d99983fffe7329938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf9a8f672eb24a03b5c644376ae36ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddb3f448451e4d028bd00818dfc3b0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fac3844a2dd488bb377aebd0252847e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5526bd223c5245a4b82247783c749ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3551c1b30926408599d4c73fce71fd53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381650067cbf400289234a1a5d64aa40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/if001/redpajama-lora-instruct-ja/blob/main/bolly_ja_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setup\n"
      ],
      "metadata": {
        "id": "jGR_uqR7C6kT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdenrVxsAM_h",
        "outputId": "1698f360-3df2-4475-d0d7-d841799b1e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'redpajama-lora-instruct-ja'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 102 (delta 46), reused 82 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 20.87 MiB | 17.36 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "/content/redpajama-lora-instruct-ja\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 6))\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-53jjw2nt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-53jjw2nt\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 0fcc30dd43a23081018570fc7a76e4b3cf282c23\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets (from -r requirements.txt (line 1))\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.0.1+cu118)\n",
            "Collecting accelerate (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from -r requirements.txt (line 5))\n",
            "  Downloading bitsandbytes-0.38.1-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire (from -r requirements.txt (line 7))\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-dotenv (from -r requirements.txt (line 8))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 9))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (4.65.0)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (2023.4.0)\n",
            "Collecting aiohttp (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (23.1)\n",
            "Collecting responses<0.19 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 2)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 3)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 3)) (16.0.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 1))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 1))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 1))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 1))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 1))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Building wheels for collected packages: peft, fire\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=56858 sha256=a35af8044890f49a5c5120b3d56c0fea824d431f6f3aece690a9278968b3addb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hf6rsphn/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=e8b4f8e2f0f29551c8b5250b45fed82878b7eab811b115f6bfb3705de3ec111a\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built peft fire\n",
            "Installing collected packages: tokenizers, sentencepiece, bitsandbytes, xxhash, python-dotenv, multidict, frozenlist, fire, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, accelerate, peft\n",
            "Successfully installed accelerate-0.19.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.38.1 datasets-2.12.0 dill-0.3.6 fire-0.5.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 peft-0.4.0.dev0 python-dotenv-1.0.0 responses-0.18.0 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/if001/redpajama-lora-instruct-ja.git\n",
        "repo_dir='/content/redpajama-lora-instruct-ja'\n",
        "%cd $repo_dir\n",
        "\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Google Drive Options { display-mode: \"form\" }\n",
        "save_models_to_drive = True #@param {type:\"boolean\"}\n",
        "drive_mount = '/content/drive' #@param {type:\"string\"}\n",
        "\n",
        "# output_dir = 'rinna_dolly_3B' #@param {type:\"string\"}\n",
        "# output_dir = 'calm_dolly_ja' #@param {type:\"string\"}\n",
        "# output_dir = 'rinna_dolly_3B_rinna_format' #@param {type:\"string\"}\n",
        "output_dir = 'calm_dolly_7B' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "import os\n",
        "if save_models_to_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount(drive_mount)\n",
        "    output_path = f\"{drive_mount}/MyDrive/models/{output_dir}\" if save_models_to_drive else f\"/content/{output_dir}\"\n",
        "else:\n",
        "    output_path = \"/content\"\n",
        "\n",
        "\n",
        "print(f\"Saving models to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoTPxcWNDDJa",
        "outputId": "9c057c1c-a28c-4823-9e97-01b16203d160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saving models to /content/drive/MyDrive/models/calm_dolly_7B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train"
      ],
      "metadata": {
        "id": "ht5PCHTjDReK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "3Ve5jVprkBLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $repo_dir\n",
        "!git pull\n",
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9dFlsv-b0a_",
        "outputId": "cd0348b0-3481-4eaa-949b-0d38d7bed4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/redpajama-lora-instruct-ja\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), 588 bytes | 588.00 KiB/s, done.\n",
            "From https://github.com/if001/redpajama-lora-instruct-ja\n",
            "   7df09a6..5fe6036  main       -> origin/main\n",
            "Updating 7df09a6..5fe6036\n",
            "Fast-forward\n",
            " templates/dolly_ja_rinna.json | 6 \u001b[32m++++++\u001b[m\n",
            " 1 file changed, 6 insertions(+)\n",
            " create mode 100644 templates/dolly_ja_rinna.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $repo_dir\n",
        "\n",
        "# base_model=\"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
        "# base_model=\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\"\n",
        "# base_model=\"cyberagent/open-calm-3b\"\n",
        "base_model=\"rinna/japanese-gpt-neox-3.6b-instruction-sft\"\n",
        "data_path=\"kunishou/databricks-dolly-15k-ja\"\n",
        "template = 'dolly_ja'\n",
        "template = 'dolly_ja_rinna'\n",
        "\n",
        "\n",
        "!python3 finetune.py \\\n",
        "--base_model=$base_model \\\n",
        "--batch_size=128 \\\n",
        "--micro_batch_size=2 \\\n",
        "--prompt_template_name=$template \\\n",
        "--cutoff_len=1024 \\\n",
        "--output_dir=$output_path \\\n",
        "--num_epochs=2 \\\n",
        "--data_path=$data_path \\\n",
        "--gradient_checkpointing=False \\\n",
        "--resume_from_checkpoint=$output_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZVkbn4YDS8w",
        "outputId": "4ffe2a9a-c123-4c6a-bd38-6c122a8e2862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/redpajama-lora-instruct-ja\n",
            "2023-05-21 04:39:22.180603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-hm-2yjx46ui7v93n --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "\n",
            "\n",
            "LoRA fine-tuning model with params:\n",
            "base_model: rinna/japanese-gpt-neox-3.6b-instruction-sft\n",
            "data_path: kunishou/databricks-dolly-15k-ja\n",
            "output_dir: /content/drive/MyDrive/models/rinna_dolly_3B_rinna_format\n",
            "batch_size: 128\n",
            "micro_batch_size: 2\n",
            "num_epochs: 2\n",
            "learning_rate: 0.0003\n",
            "cutoff_len: 1024\n",
            "val_set_size: 2000\n",
            "lora_r: 8\n",
            "lora_alpha: 16\n",
            "lora_dropout: 0.05\n",
            "lora_target_modules: ['query_key_value']\n",
            "train_on_inputs: True\n",
            "add_eos_token: False\n",
            "group_by_length: False\n",
            "resume_from_checkpoint: /content/drive/MyDrive/models/rinna_dolly_3B_rinna_format\n",
            "prompt template: dolly_ja_rinna\n",
            "\n",
            "device map: auto\n",
            "Downloading (…)lve/main/config.json: 100% 534/534 [00:00<00:00, 3.45MB/s]\n",
            "Downloading pytorch_model.bin: 100% 7.37G/7.37G [02:01<00:00, 60.9MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 284/284 [00:00<00:00, 2.08MB/s]\n",
            "Downloading spiece.model: 100% 786k/786k [00:00<00:00, 6.24MB/s]\n",
            "Downloading readme: 100% 355/355 [00:00<00:00, 428kB/s]\n",
            "Downloading and preparing dataset json/kunishou--databricks-dolly-15k-ja to /root/.cache/huggingface/datasets/kunishou___json/kunishou--databricks-dolly-15k-ja-e353786ca55017da/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/17.1M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   1% 117k/17.1M [00:00<00:14, 1.16MB/s]\u001b[A\n",
            "Downloading data:   2% 264k/17.1M [00:00<00:12, 1.34MB/s]\u001b[A\n",
            "Downloading data:   3% 436k/17.1M [00:00<00:11, 1.51MB/s]\u001b[A\n",
            "Downloading data:   4% 651k/17.1M [00:00<00:09, 1.75MB/s]\u001b[A\n",
            "Downloading data:   5% 908k/17.1M [00:00<00:07, 2.04MB/s]\u001b[A\n",
            "Downloading data:   7% 1.23M/17.1M [00:00<00:06, 2.43MB/s]\u001b[A\n",
            "Downloading data:   9% 1.60M/17.1M [00:00<00:05, 2.84MB/s]\u001b[A\n",
            "Downloading data:  12% 2.08M/17.1M [00:00<00:04, 3.45MB/s]\u001b[A\n",
            "Downloading data:  15% 2.64M/17.1M [00:00<00:03, 4.14MB/s]\u001b[A\n",
            "Downloading data:  19% 3.29M/17.1M [00:01<00:02, 4.87MB/s]\u001b[A\n",
            "Downloading data:  24% 4.14M/17.1M [00:01<00:02, 5.97MB/s]\u001b[A\n",
            "Downloading data:  30% 5.15M/17.1M [00:01<00:01, 7.21MB/s]\u001b[A\n",
            "Downloading data:  37% 6.27M/17.1M [00:01<00:01, 8.33MB/s]\u001b[A\n",
            "Downloading data:  45% 7.73M/17.1M [00:01<00:00, 10.2MB/s]\u001b[A\n",
            "Downloading data:  56% 9.52M/17.1M [00:01<00:00, 12.5MB/s]\u001b[A\n",
            "Downloading data:  69% 11.7M/17.1M [00:01<00:00, 15.3MB/s]\u001b[A\n",
            "Downloading data: 100% 17.1M/17.1M [00:01<00:00, 9.53MB/s]\n",
            "Downloading data files: 100% 1/1 [00:02<00:00,  2.44s/it]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1317.72it/s]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/kunishou___json/kunishou--databricks-dolly-15k-ja-e353786ca55017da/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 192.82it/s]\n",
            "Checkpoint /content/drive/MyDrive/models/rinna_dolly_3B_rinna_format/adapter_model.bin not found\n",
            "trainable params: 3244032 || all params: 3610489344 || trainable%: 0.08985020286491116\n",
            "  0% 0/202 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:229: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
            "  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n",
            "{'loss': 2.6646, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.1}\n",
            "{'loss': 2.6156, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.2}\n",
            "{'loss': 2.4977, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.3}\n",
            "{'loss': 2.2753, 'learning_rate': 0.00011999999999999999, 'epoch': 0.39}\n",
            "{'loss': 2.1145, 'learning_rate': 0.00015, 'epoch': 0.49}\n",
            "{'loss': 2.0228, 'learning_rate': 0.00017999999999999998, 'epoch': 0.59}\n",
            "{'loss': 1.9893, 'learning_rate': 0.00020999999999999998, 'epoch': 0.69}\n",
            "{'loss': 1.9691, 'learning_rate': 0.00023999999999999998, 'epoch': 0.79}\n",
            "{'loss': 1.9097, 'learning_rate': 0.00027, 'epoch': 0.89}\n",
            "{'loss': 1.9041, 'learning_rate': 0.0003, 'epoch': 0.98}\n",
            "{'loss': 1.9046, 'learning_rate': 0.0002705882352941176, 'epoch': 1.08}\n",
            "{'loss': 1.9206, 'learning_rate': 0.00024117647058823527, 'epoch': 1.18}\n",
            "{'loss': 1.8807, 'learning_rate': 0.00021176470588235295, 'epoch': 1.28}\n",
            "{'loss': 1.8589, 'learning_rate': 0.00018235294117647055, 'epoch': 1.38}\n",
            "{'loss': 1.872, 'learning_rate': 0.00015294117647058822, 'epoch': 1.48}\n",
            "{'loss': 1.8914, 'learning_rate': 0.00012352941176470587, 'epoch': 1.57}\n",
            "{'loss': 1.852, 'learning_rate': 9.411764705882352e-05, 'epoch': 1.67}\n",
            "{'loss': 1.8485, 'learning_rate': 6.470588235294117e-05, 'epoch': 1.77}\n",
            "{'loss': 1.873, 'learning_rate': 3.529411764705882e-05, 'epoch': 1.87}\n",
            "{'loss': 1.8502, 'learning_rate': 5.88235294117647e-06, 'epoch': 1.97}\n",
            " 99% 200/202 [8:37:54<05:20, 160.04s/it]\n",
            "  0% 0/250 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/250 [00:03<06:22,  1.54s/it]\u001b[A\n",
            "  1% 3/250 [00:07<11:16,  2.74s/it]\u001b[A\n",
            "  2% 4/250 [00:10<12:09,  2.96s/it]\u001b[A\n",
            "  2% 5/250 [00:14<13:08,  3.22s/it]\u001b[A\n",
            "  2% 6/250 [00:15<10:27,  2.57s/it]\u001b[A\n",
            "  3% 7/250 [00:21<14:12,  3.51s/it]\u001b[A\n",
            "  3% 8/250 [00:26<16:45,  4.15s/it]\u001b[A\n",
            "  4% 9/250 [00:29<15:03,  3.75s/it]\u001b[A\n",
            "  4% 10/250 [00:31<13:08,  3.29s/it]\u001b[A\n",
            "  4% 11/250 [00:33<11:13,  2.82s/it]\u001b[A\n",
            "  5% 12/250 [00:39<14:23,  3.63s/it]\u001b[A\n",
            "  5% 13/250 [00:41<13:20,  3.38s/it]\u001b[A\n",
            "  6% 14/250 [00:44<11:44,  2.99s/it]\u001b[A\n",
            "  6% 15/250 [00:45<10:23,  2.65s/it]\u001b[A\n",
            "  6% 16/250 [00:47<09:07,  2.34s/it]\u001b[A\n",
            "  7% 17/250 [00:50<09:47,  2.52s/it]\u001b[A\n",
            "  7% 18/250 [00:52<08:51,  2.29s/it]\u001b[A\n",
            "  8% 19/250 [00:55<09:33,  2.48s/it]\u001b[A\n",
            "  8% 20/250 [00:57<09:25,  2.46s/it]\u001b[A\n",
            "  8% 21/250 [00:59<08:23,  2.20s/it]\u001b[A\n",
            "  9% 22/250 [01:01<08:00,  2.11s/it]\u001b[A\n",
            "  9% 23/250 [01:03<08:15,  2.18s/it]\u001b[A\n",
            " 10% 24/250 [01:08<11:57,  3.18s/it]\u001b[A\n",
            " 10% 25/250 [01:13<13:15,  3.54s/it]\u001b[A\n",
            " 10% 26/250 [01:14<10:55,  2.92s/it]\u001b[A\n",
            " 11% 27/250 [01:16<09:18,  2.50s/it]\u001b[A\n",
            " 11% 28/250 [01:18<08:26,  2.28s/it]\u001b[A\n",
            " 12% 29/250 [01:20<08:10,  2.22s/it]\u001b[A\n",
            " 12% 30/250 [01:22<08:31,  2.33s/it]\u001b[A\n",
            " 12% 31/250 [01:25<08:47,  2.41s/it]\u001b[A\n",
            " 13% 32/250 [01:26<07:06,  1.96s/it]\u001b[A\n",
            " 13% 33/250 [01:27<06:26,  1.78s/it]\u001b[A\n",
            " 14% 34/250 [01:30<07:38,  2.12s/it]\u001b[A\n",
            " 14% 35/250 [01:34<10:04,  2.81s/it]\u001b[A\n",
            " 14% 36/250 [01:37<09:48,  2.75s/it]\u001b[A\n",
            " 15% 37/250 [01:38<07:53,  2.22s/it]\u001b[A\n",
            " 15% 38/250 [01:41<08:26,  2.39s/it]\u001b[A\n",
            " 16% 39/250 [01:44<09:12,  2.62s/it]\u001b[A\n",
            " 16% 40/250 [01:46<08:45,  2.50s/it]\u001b[A\n",
            " 16% 41/250 [01:48<08:04,  2.32s/it]\u001b[A\n",
            " 17% 42/250 [01:51<09:05,  2.62s/it]\u001b[A\n",
            " 17% 43/250 [01:54<08:30,  2.47s/it]\u001b[A\n",
            " 18% 44/250 [01:56<08:56,  2.61s/it]\u001b[A\n",
            " 18% 45/250 [01:58<07:34,  2.22s/it]\u001b[A\n",
            " 18% 46/250 [02:03<10:52,  3.20s/it]\u001b[A\n",
            " 19% 47/250 [02:05<09:25,  2.79s/it]\u001b[A\n",
            " 19% 48/250 [02:07<08:09,  2.42s/it]\u001b[A\n",
            " 20% 49/250 [02:09<07:36,  2.27s/it]\u001b[A\n",
            " 20% 50/250 [02:10<06:49,  2.05s/it]\u001b[A\n",
            " 20% 51/250 [02:12<06:26,  1.94s/it]\u001b[A\n",
            " 21% 52/250 [02:14<06:31,  1.97s/it]\u001b[A\n",
            " 21% 53/250 [02:16<06:15,  1.90s/it]\u001b[A\n",
            " 22% 54/250 [02:18<06:51,  2.10s/it]\u001b[A\n",
            " 22% 55/250 [02:21<07:40,  2.36s/it]\u001b[A\n",
            " 22% 56/250 [02:26<09:52,  3.06s/it]\u001b[A\n",
            " 23% 57/250 [02:29<10:06,  3.14s/it]\u001b[A\n",
            " 23% 58/250 [02:30<08:06,  2.53s/it]\u001b[A\n",
            " 24% 59/250 [02:34<09:21,  2.94s/it]\u001b[A\n",
            " 24% 60/250 [02:40<11:46,  3.72s/it]\u001b[A\n",
            " 24% 61/250 [02:43<11:10,  3.55s/it]\u001b[A\n",
            " 25% 62/250 [02:44<08:58,  2.86s/it]\u001b[A\n",
            " 25% 63/250 [02:46<08:10,  2.63s/it]\u001b[A\n",
            " 26% 64/250 [02:50<09:39,  3.12s/it]\u001b[A\n",
            " 26% 65/250 [02:52<08:04,  2.62s/it]\u001b[A\n",
            " 26% 66/250 [02:57<10:06,  3.30s/it]\u001b[A\n",
            " 27% 67/250 [02:58<08:25,  2.76s/it]\u001b[A\n",
            " 27% 68/250 [03:00<07:35,  2.51s/it]\u001b[A\n",
            " 28% 69/250 [03:06<10:15,  3.40s/it]\u001b[A\n",
            " 28% 70/250 [03:09<09:51,  3.29s/it]\u001b[A\n",
            " 28% 71/250 [03:13<10:49,  3.63s/it]\u001b[A\n",
            " 29% 72/250 [03:15<08:51,  2.99s/it]\u001b[A\n",
            " 29% 73/250 [03:20<11:01,  3.74s/it]\u001b[A\n",
            " 30% 74/250 [03:22<09:15,  3.16s/it]\u001b[A\n",
            " 30% 75/250 [03:24<08:06,  2.78s/it]\u001b[A\n",
            " 30% 76/250 [03:25<06:59,  2.41s/it]\u001b[A\n",
            " 31% 77/250 [03:26<05:31,  1.91s/it]\u001b[A\n",
            " 31% 78/250 [03:27<04:42,  1.64s/it]\u001b[A\n",
            " 32% 79/250 [03:28<04:14,  1.49s/it]\u001b[A\n",
            " 32% 80/250 [03:30<04:41,  1.66s/it]\u001b[A\n",
            " 32% 81/250 [03:32<04:36,  1.64s/it]\u001b[A\n",
            " 33% 82/250 [03:37<07:48,  2.79s/it]\u001b[A\n",
            " 33% 83/250 [03:42<09:11,  3.30s/it]\u001b[A\n",
            " 34% 84/250 [03:45<09:10,  3.32s/it]\u001b[A\n",
            " 34% 85/250 [03:47<08:00,  2.91s/it]\u001b[A\n",
            " 34% 86/250 [03:49<06:39,  2.43s/it]\u001b[A\n",
            " 35% 87/250 [03:52<07:37,  2.80s/it]\u001b[A\n",
            " 35% 88/250 [03:58<09:46,  3.62s/it]\u001b[A\n",
            " 36% 89/250 [04:00<08:14,  3.07s/it]\u001b[A\n",
            " 36% 90/250 [04:03<08:24,  3.15s/it]\u001b[A\n",
            " 36% 91/250 [04:06<08:21,  3.15s/it]\u001b[A\n",
            " 37% 92/250 [04:09<07:59,  3.04s/it]\u001b[A\n",
            " 37% 93/250 [04:11<06:58,  2.66s/it]\u001b[A\n",
            " 38% 94/250 [04:12<06:13,  2.39s/it]\u001b[A\n",
            " 38% 95/250 [04:18<08:35,  3.32s/it]\u001b[A\n",
            " 38% 96/250 [04:20<07:37,  2.97s/it]\u001b[A\n",
            " 39% 97/250 [04:23<07:34,  2.97s/it]\u001b[A\n",
            " 39% 98/250 [04:25<06:35,  2.60s/it]\u001b[A\n",
            " 40% 99/250 [04:30<08:17,  3.30s/it]\u001b[A\n",
            " 40% 100/250 [04:35<09:55,  3.97s/it]\u001b[A\n",
            " 40% 101/250 [04:36<07:35,  3.06s/it]\u001b[A\n",
            " 41% 102/250 [04:37<06:18,  2.56s/it]\u001b[A\n",
            " 41% 103/250 [04:39<05:10,  2.11s/it]\u001b[A\n",
            " 42% 104/250 [04:41<05:21,  2.21s/it]\u001b[A\n",
            " 42% 105/250 [04:46<07:43,  3.19s/it]\u001b[A\n",
            " 42% 106/250 [04:49<07:01,  2.92s/it]\u001b[A\n",
            " 43% 107/250 [04:52<07:08,  3.00s/it]\u001b[A\n",
            " 43% 108/250 [04:54<06:06,  2.58s/it]\u001b[A\n",
            " 44% 109/250 [04:56<05:44,  2.44s/it]\u001b[A\n",
            " 44% 110/250 [04:57<04:45,  2.04s/it]\u001b[A\n",
            " 44% 111/250 [04:59<05:05,  2.20s/it]\u001b[A\n",
            " 45% 112/250 [05:02<05:04,  2.21s/it]\u001b[A\n",
            " 45% 113/250 [05:04<04:52,  2.13s/it]\u001b[A\n",
            " 46% 114/250 [05:06<04:54,  2.16s/it]\u001b[A\n",
            " 46% 115/250 [05:07<04:35,  2.04s/it]\u001b[A\n",
            " 46% 116/250 [05:10<04:40,  2.09s/it]\u001b[A\n",
            " 47% 117/250 [05:11<03:49,  1.73s/it]\u001b[A\n",
            " 47% 118/250 [05:12<03:53,  1.77s/it]\u001b[A\n",
            " 48% 119/250 [05:14<03:57,  1.81s/it]\u001b[A\n",
            " 48% 120/250 [05:17<04:12,  1.95s/it]\u001b[A\n",
            " 48% 121/250 [05:20<05:24,  2.52s/it]\u001b[A\n",
            " 49% 122/250 [05:26<07:18,  3.42s/it]\u001b[A\n",
            " 49% 123/250 [05:28<06:01,  2.85s/it]\u001b[A\n",
            " 50% 124/250 [05:29<04:57,  2.36s/it]\u001b[A\n",
            " 50% 125/250 [05:30<04:20,  2.08s/it]\u001b[A\n",
            " 50% 126/250 [05:32<04:05,  1.98s/it]\u001b[A\n",
            " 51% 127/250 [05:37<06:13,  3.04s/it]\u001b[A\n",
            " 51% 128/250 [05:39<05:14,  2.58s/it]\u001b[A\n",
            " 52% 129/250 [05:42<05:37,  2.79s/it]\u001b[A\n",
            " 52% 130/250 [05:46<06:21,  3.18s/it]\u001b[A\n",
            " 52% 131/250 [05:49<06:11,  3.12s/it]\u001b[A\n",
            " 53% 132/250 [05:51<05:31,  2.81s/it]\u001b[A\n",
            " 53% 133/250 [05:57<07:03,  3.62s/it]\u001b[A\n",
            " 54% 134/250 [05:59<05:55,  3.07s/it]\u001b[A\n",
            " 54% 135/250 [06:01<05:19,  2.78s/it]\u001b[A\n",
            " 54% 136/250 [06:02<04:25,  2.32s/it]\u001b[A\n",
            " 55% 137/250 [06:04<04:08,  2.20s/it]\u001b[A\n",
            " 55% 138/250 [06:05<03:28,  1.86s/it]\u001b[A\n",
            " 56% 139/250 [06:07<03:16,  1.77s/it]\u001b[A\n",
            " 56% 140/250 [06:10<03:53,  2.12s/it]\u001b[A\n",
            " 56% 141/250 [06:11<03:39,  2.02s/it]\u001b[A\n",
            " 57% 142/250 [06:13<03:40,  2.04s/it]\u001b[A\n",
            " 57% 143/250 [06:15<03:28,  1.95s/it]\u001b[A\n",
            " 58% 144/250 [06:17<03:19,  1.88s/it]\u001b[A\n",
            " 58% 145/250 [06:18<02:53,  1.66s/it]\u001b[A\n",
            " 58% 146/250 [06:21<03:20,  1.93s/it]\u001b[A\n",
            " 59% 147/250 [06:22<03:13,  1.88s/it]\u001b[A\n",
            " 59% 148/250 [06:26<04:01,  2.37s/it]\u001b[A\n",
            " 60% 149/250 [06:30<04:44,  2.82s/it]\u001b[A\n",
            " 60% 150/250 [06:32<04:20,  2.61s/it]\u001b[A\n",
            " 60% 151/250 [06:36<05:07,  3.10s/it]\u001b[A\n",
            " 61% 152/250 [06:39<04:54,  3.01s/it]\u001b[A\n",
            " 61% 153/250 [06:42<05:07,  3.17s/it]\u001b[A\n",
            " 62% 154/250 [06:47<05:41,  3.55s/it]\u001b[A\n",
            " 62% 155/250 [06:49<04:56,  3.13s/it]\u001b[A\n",
            " 62% 156/250 [06:54<06:00,  3.84s/it]\u001b[A\n",
            " 63% 157/250 [06:58<05:37,  3.63s/it]\u001b[A\n",
            " 63% 158/250 [06:59<04:22,  2.86s/it]\u001b[A\n",
            " 64% 159/250 [07:00<03:45,  2.48s/it]\u001b[A\n",
            " 64% 160/250 [07:05<04:35,  3.06s/it]\u001b[A\n",
            " 64% 161/250 [07:07<04:06,  2.77s/it]\u001b[A\n",
            " 65% 162/250 [07:08<03:30,  2.40s/it]\u001b[A\n",
            " 65% 163/250 [07:11<03:32,  2.44s/it]\u001b[A\n",
            " 66% 164/250 [07:16<04:48,  3.36s/it]\u001b[A\n",
            " 66% 165/250 [07:18<03:53,  2.75s/it]\u001b[A\n",
            " 66% 166/250 [07:19<03:14,  2.31s/it]\u001b[A\n",
            " 67% 167/250 [07:20<02:43,  1.97s/it]\u001b[A\n",
            " 67% 168/250 [07:22<02:39,  1.95s/it]\u001b[A\n",
            " 68% 169/250 [07:23<02:13,  1.64s/it]\u001b[A\n",
            " 68% 170/250 [07:24<02:01,  1.52s/it]\u001b[A\n",
            " 68% 171/250 [07:25<01:53,  1.44s/it]\u001b[A\n",
            " 69% 172/250 [07:29<02:32,  1.95s/it]\u001b[A\n",
            " 69% 173/250 [07:30<02:08,  1.67s/it]\u001b[A\n",
            " 70% 174/250 [07:33<02:55,  2.31s/it]\u001b[A\n",
            " 70% 175/250 [07:35<02:44,  2.20s/it]\u001b[A\n",
            " 70% 176/250 [07:38<02:43,  2.20s/it]\u001b[A\n",
            " 71% 177/250 [07:40<02:48,  2.31s/it]\u001b[A\n",
            " 71% 178/250 [07:42<02:37,  2.19s/it]\u001b[A\n",
            " 72% 179/250 [07:48<03:46,  3.18s/it]\u001b[A\n",
            " 72% 180/250 [07:53<04:32,  3.89s/it]\u001b[A\n",
            " 72% 181/250 [07:55<03:41,  3.22s/it]\u001b[A\n",
            " 73% 182/250 [08:00<04:24,  3.89s/it]\u001b[A\n",
            " 73% 183/250 [08:05<04:29,  4.03s/it]\u001b[A\n",
            " 74% 184/250 [08:07<03:48,  3.46s/it]\u001b[A\n",
            " 74% 185/250 [08:09<03:27,  3.20s/it]\u001b[A\n",
            " 74% 186/250 [08:11<03:02,  2.85s/it]\u001b[A\n",
            " 75% 187/250 [08:12<02:24,  2.29s/it]\u001b[A\n",
            " 75% 188/250 [08:15<02:26,  2.37s/it]\u001b[A\n",
            " 76% 189/250 [08:18<02:31,  2.48s/it]\u001b[A\n",
            " 76% 190/250 [08:20<02:33,  2.56s/it]\u001b[A\n",
            " 76% 191/250 [08:22<02:14,  2.27s/it]\u001b[A\n",
            " 77% 192/250 [08:25<02:23,  2.47s/it]\u001b[A\n",
            " 77% 193/250 [08:28<02:25,  2.56s/it]\u001b[A\n",
            " 78% 194/250 [08:29<02:09,  2.32s/it]\u001b[A\n",
            " 78% 195/250 [08:33<02:27,  2.69s/it]\u001b[A\n",
            " 78% 196/250 [08:38<03:10,  3.53s/it]\u001b[A\n",
            " 79% 197/250 [08:40<02:36,  2.95s/it]\u001b[A\n",
            " 79% 198/250 [08:41<02:09,  2.49s/it]\u001b[A\n",
            " 80% 199/250 [08:44<02:10,  2.57s/it]\u001b[A\n",
            " 80% 200/250 [08:50<02:52,  3.44s/it]\u001b[A\n",
            " 80% 201/250 [08:54<02:58,  3.65s/it]\u001b[A\n",
            " 81% 202/250 [08:59<03:21,  4.21s/it]\u001b[A\n",
            " 81% 203/250 [09:01<02:43,  3.49s/it]\u001b[A\n",
            " 82% 204/250 [09:05<02:40,  3.48s/it]\u001b[A\n",
            " 82% 205/250 [09:08<02:31,  3.37s/it]\u001b[A\n",
            " 82% 206/250 [09:13<02:56,  4.01s/it]\u001b[A\n",
            " 83% 207/250 [09:19<03:12,  4.47s/it]\u001b[A\n",
            " 83% 208/250 [09:22<02:57,  4.22s/it]\u001b[A\n",
            " 84% 209/250 [09:26<02:45,  4.03s/it]\u001b[A\n",
            " 84% 210/250 [09:28<02:14,  3.35s/it]\u001b[A\n",
            " 84% 211/250 [09:31<02:07,  3.27s/it]\u001b[A\n",
            " 85% 212/250 [09:32<01:42,  2.69s/it]\u001b[A\n",
            " 85% 213/250 [09:34<01:25,  2.32s/it]\u001b[A\n",
            " 86% 214/250 [09:35<01:16,  2.13s/it]\u001b[A\n",
            " 86% 215/250 [09:37<01:05,  1.88s/it]\u001b[A\n",
            " 86% 216/250 [09:40<01:20,  2.36s/it]\u001b[A\n",
            " 87% 217/250 [09:42<01:13,  2.24s/it]\u001b[A\n",
            " 87% 218/250 [09:43<01:02,  1.96s/it]\u001b[A\n",
            " 88% 219/250 [09:49<01:33,  3.02s/it]\u001b[A\n",
            " 88% 220/250 [09:51<01:19,  2.65s/it]\u001b[A\n",
            " 88% 221/250 [09:54<01:22,  2.86s/it]\u001b[A\n",
            " 89% 222/250 [09:55<01:04,  2.30s/it]\u001b[A\n",
            " 89% 223/250 [10:00<01:27,  3.26s/it]\u001b[A\n",
            " 90% 224/250 [10:02<01:11,  2.74s/it]\u001b[A\n",
            " 90% 225/250 [10:04<00:59,  2.39s/it]\u001b[A\n",
            " 90% 226/250 [10:05<00:52,  2.19s/it]\u001b[A\n",
            " 91% 227/250 [10:08<00:54,  2.35s/it]\u001b[A\n",
            " 91% 228/250 [10:13<01:08,  3.12s/it]\u001b[A\n",
            " 92% 229/250 [10:15<00:57,  2.72s/it]\u001b[A\n",
            " 92% 230/250 [10:16<00:45,  2.29s/it]\u001b[A\n",
            " 92% 231/250 [10:17<00:38,  2.04s/it]\u001b[A\n",
            " 93% 232/250 [10:19<00:35,  1.95s/it]\u001b[A\n",
            " 93% 233/250 [10:22<00:37,  2.21s/it]\u001b[A\n",
            " 94% 234/250 [10:23<00:31,  1.98s/it]\u001b[A\n",
            " 94% 235/250 [10:25<00:26,  1.78s/it]\u001b[A\n",
            " 94% 236/250 [10:27<00:26,  1.91s/it]\u001b[A\n",
            " 95% 237/250 [10:28<00:21,  1.68s/it]\u001b[A\n",
            " 95% 238/250 [10:34<00:33,  2.82s/it]\u001b[A\n",
            " 96% 239/250 [10:39<00:40,  3.64s/it]\u001b[A\n",
            " 96% 240/250 [10:41<00:29,  2.96s/it]\u001b[A\n",
            " 96% 241/250 [10:43<00:24,  2.70s/it]\u001b[A\n",
            " 97% 242/250 [10:48<00:28,  3.54s/it]\u001b[A\n",
            " 97% 243/250 [10:50<00:21,  3.02s/it]\u001b[A\n",
            " 98% 244/250 [10:52<00:16,  2.78s/it]\u001b[A\n",
            " 98% 245/250 [10:56<00:15,  3.06s/it]\u001b[A\n",
            " 98% 246/250 [10:58<00:11,  2.76s/it]\u001b[A\n",
            " 99% 247/250 [11:00<00:07,  2.56s/it]\u001b[A\n",
            " 99% 248/250 [11:01<00:04,  2.21s/it]\u001b[A\n",
            "100% 249/250 [11:04<00:02,  2.32s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.0065646171569824, 'eval_runtime': 671.1086, 'eval_samples_per_second': 2.98, 'eval_steps_per_second': 0.373, 'epoch': 1.97}\n",
            " 99% 200/202 [8:49:05<05:20, 160.04s/it]\n",
            "100% 250/250 [11:07<00:00,  2.51s/it]\u001b[A\n",
            "                                     \u001b[A/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "100% 202/202 [8:53:59<00:00, 294.71s/it]There were missing keys in the checkpoint model loaded: ['base_model.model.gpt_neox.embed_in.weight', 'base_model.model.gpt_neox.layers.0.input_layernorm.weight', 'base_model.model.gpt_neox.layers.0.input_layernorm.bias', 'base_model.model.gpt_neox.layers.0.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.0.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.0.attention.bias', 'base_model.model.gpt_neox.layers.0.attention.masked_bias', 'base_model.model.gpt_neox.layers.0.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.0.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.0.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.0.attention.dense.weight', 'base_model.model.gpt_neox.layers.0.attention.dense.bias', 'base_model.model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.1.input_layernorm.weight', 'base_model.model.gpt_neox.layers.1.input_layernorm.bias', 'base_model.model.gpt_neox.layers.1.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.1.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.1.attention.bias', 'base_model.model.gpt_neox.layers.1.attention.masked_bias', 'base_model.model.gpt_neox.layers.1.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.1.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.1.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.1.attention.dense.weight', 'base_model.model.gpt_neox.layers.1.attention.dense.bias', 'base_model.model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.2.input_layernorm.weight', 'base_model.model.gpt_neox.layers.2.input_layernorm.bias', 'base_model.model.gpt_neox.layers.2.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.2.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.2.attention.bias', 'base_model.model.gpt_neox.layers.2.attention.masked_bias', 'base_model.model.gpt_neox.layers.2.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.2.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.2.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.2.attention.dense.weight', 'base_model.model.gpt_neox.layers.2.attention.dense.bias', 'base_model.model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.3.input_layernorm.weight', 'base_model.model.gpt_neox.layers.3.input_layernorm.bias', 'base_model.model.gpt_neox.layers.3.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.3.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.3.attention.bias', 'base_model.model.gpt_neox.layers.3.attention.masked_bias', 'base_model.model.gpt_neox.layers.3.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.3.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.3.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.3.attention.dense.weight', 'base_model.model.gpt_neox.layers.3.attention.dense.bias', 'base_model.model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.4.input_layernorm.weight', 'base_model.model.gpt_neox.layers.4.input_layernorm.bias', 'base_model.model.gpt_neox.layers.4.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.4.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.4.attention.bias', 'base_model.model.gpt_neox.layers.4.attention.masked_bias', 'base_model.model.gpt_neox.layers.4.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.4.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.4.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.4.attention.dense.weight', 'base_model.model.gpt_neox.layers.4.attention.dense.bias', 'base_model.model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.5.input_layernorm.weight', 'base_model.model.gpt_neox.layers.5.input_layernorm.bias', 'base_model.model.gpt_neox.layers.5.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.5.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.5.attention.bias', 'base_model.model.gpt_neox.layers.5.attention.masked_bias', 'base_model.model.gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.5.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.5.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.5.attention.dense.weight', 'base_model.model.gpt_neox.layers.5.attention.dense.bias', 'base_model.model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.6.input_layernorm.weight', 'base_model.model.gpt_neox.layers.6.input_layernorm.bias', 'base_model.model.gpt_neox.layers.6.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.6.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.6.attention.bias', 'base_model.model.gpt_neox.layers.6.attention.masked_bias', 'base_model.model.gpt_neox.layers.6.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.6.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.6.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.6.attention.dense.weight', 'base_model.model.gpt_neox.layers.6.attention.dense.bias', 'base_model.model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.7.input_layernorm.weight', 'base_model.model.gpt_neox.layers.7.input_layernorm.bias', 'base_model.model.gpt_neox.layers.7.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.7.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.7.attention.bias', 'base_model.model.gpt_neox.layers.7.attention.masked_bias', 'base_model.model.gpt_neox.layers.7.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.7.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.7.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.7.attention.dense.weight', 'base_model.model.gpt_neox.layers.7.attention.dense.bias', 'base_model.model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.8.input_layernorm.weight', 'base_model.model.gpt_neox.layers.8.input_layernorm.bias', 'base_model.model.gpt_neox.layers.8.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.8.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.8.attention.bias', 'base_model.model.gpt_neox.layers.8.attention.masked_bias', 'base_model.model.gpt_neox.layers.8.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.8.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.8.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.8.attention.dense.weight', 'base_model.model.gpt_neox.layers.8.attention.dense.bias', 'base_model.model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.9.input_layernorm.weight', 'base_model.model.gpt_neox.layers.9.input_layernorm.bias', 'base_model.model.gpt_neox.layers.9.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.9.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.9.attention.bias', 'base_model.model.gpt_neox.layers.9.attention.masked_bias', 'base_model.model.gpt_neox.layers.9.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.9.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.9.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.9.attention.dense.weight', 'base_model.model.gpt_neox.layers.9.attention.dense.bias', 'base_model.model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.10.input_layernorm.weight', 'base_model.model.gpt_neox.layers.10.input_layernorm.bias', 'base_model.model.gpt_neox.layers.10.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.10.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.10.attention.bias', 'base_model.model.gpt_neox.layers.10.attention.masked_bias', 'base_model.model.gpt_neox.layers.10.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.10.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.10.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.10.attention.dense.weight', 'base_model.model.gpt_neox.layers.10.attention.dense.bias', 'base_model.model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.11.input_layernorm.weight', 'base_model.model.gpt_neox.layers.11.input_layernorm.bias', 'base_model.model.gpt_neox.layers.11.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.11.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.11.attention.bias', 'base_model.model.gpt_neox.layers.11.attention.masked_bias', 'base_model.model.gpt_neox.layers.11.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.11.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.11.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.11.attention.dense.weight', 'base_model.model.gpt_neox.layers.11.attention.dense.bias', 'base_model.model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.12.input_layernorm.weight', 'base_model.model.gpt_neox.layers.12.input_layernorm.bias', 'base_model.model.gpt_neox.layers.12.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.12.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.12.attention.bias', 'base_model.model.gpt_neox.layers.12.attention.masked_bias', 'base_model.model.gpt_neox.layers.12.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.12.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.12.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.12.attention.dense.weight', 'base_model.model.gpt_neox.layers.12.attention.dense.bias', 'base_model.model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.13.input_layernorm.weight', 'base_model.model.gpt_neox.layers.13.input_layernorm.bias', 'base_model.model.gpt_neox.layers.13.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.13.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.13.attention.bias', 'base_model.model.gpt_neox.layers.13.attention.masked_bias', 'base_model.model.gpt_neox.layers.13.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.13.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.13.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.13.attention.dense.weight', 'base_model.model.gpt_neox.layers.13.attention.dense.bias', 'base_model.model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.14.input_layernorm.weight', 'base_model.model.gpt_neox.layers.14.input_layernorm.bias', 'base_model.model.gpt_neox.layers.14.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.14.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.14.attention.bias', 'base_model.model.gpt_neox.layers.14.attention.masked_bias', 'base_model.model.gpt_neox.layers.14.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.14.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.14.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.14.attention.dense.weight', 'base_model.model.gpt_neox.layers.14.attention.dense.bias', 'base_model.model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.15.input_layernorm.weight', 'base_model.model.gpt_neox.layers.15.input_layernorm.bias', 'base_model.model.gpt_neox.layers.15.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.15.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.15.attention.bias', 'base_model.model.gpt_neox.layers.15.attention.masked_bias', 'base_model.model.gpt_neox.layers.15.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.15.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.15.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.15.attention.dense.weight', 'base_model.model.gpt_neox.layers.15.attention.dense.bias', 'base_model.model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.16.input_layernorm.weight', 'base_model.model.gpt_neox.layers.16.input_layernorm.bias', 'base_model.model.gpt_neox.layers.16.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.16.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.16.attention.bias', 'base_model.model.gpt_neox.layers.16.attention.masked_bias', 'base_model.model.gpt_neox.layers.16.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.16.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.16.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.16.attention.dense.weight', 'base_model.model.gpt_neox.layers.16.attention.dense.bias', 'base_model.model.gpt_neox.layers.16.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.16.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.16.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.16.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.17.input_layernorm.weight', 'base_model.model.gpt_neox.layers.17.input_layernorm.bias', 'base_model.model.gpt_neox.layers.17.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.17.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.17.attention.bias', 'base_model.model.gpt_neox.layers.17.attention.masked_bias', 'base_model.model.gpt_neox.layers.17.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.17.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.17.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.17.attention.dense.weight', 'base_model.model.gpt_neox.layers.17.attention.dense.bias', 'base_model.model.gpt_neox.layers.17.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.17.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.17.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.17.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.18.input_layernorm.weight', 'base_model.model.gpt_neox.layers.18.input_layernorm.bias', 'base_model.model.gpt_neox.layers.18.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.18.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.18.attention.bias', 'base_model.model.gpt_neox.layers.18.attention.masked_bias', 'base_model.model.gpt_neox.layers.18.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.18.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.18.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.18.attention.dense.weight', 'base_model.model.gpt_neox.layers.18.attention.dense.bias', 'base_model.model.gpt_neox.layers.18.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.18.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.18.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.18.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.19.input_layernorm.weight', 'base_model.model.gpt_neox.layers.19.input_layernorm.bias', 'base_model.model.gpt_neox.layers.19.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.19.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.19.attention.bias', 'base_model.model.gpt_neox.layers.19.attention.masked_bias', 'base_model.model.gpt_neox.layers.19.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.19.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.19.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.19.attention.dense.weight', 'base_model.model.gpt_neox.layers.19.attention.dense.bias', 'base_model.model.gpt_neox.layers.19.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.19.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.19.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.19.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.20.input_layernorm.weight', 'base_model.model.gpt_neox.layers.20.input_layernorm.bias', 'base_model.model.gpt_neox.layers.20.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.20.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.20.attention.bias', 'base_model.model.gpt_neox.layers.20.attention.masked_bias', 'base_model.model.gpt_neox.layers.20.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.20.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.20.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.20.attention.dense.weight', 'base_model.model.gpt_neox.layers.20.attention.dense.bias', 'base_model.model.gpt_neox.layers.20.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.20.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.20.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.20.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.21.input_layernorm.weight', 'base_model.model.gpt_neox.layers.21.input_layernorm.bias', 'base_model.model.gpt_neox.layers.21.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.21.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.21.attention.bias', 'base_model.model.gpt_neox.layers.21.attention.masked_bias', 'base_model.model.gpt_neox.layers.21.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.21.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.21.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.21.attention.dense.weight', 'base_model.model.gpt_neox.layers.21.attention.dense.bias', 'base_model.model.gpt_neox.layers.21.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.21.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.21.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.21.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.22.input_layernorm.weight', 'base_model.model.gpt_neox.layers.22.input_layernorm.bias', 'base_model.model.gpt_neox.layers.22.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.22.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.22.attention.bias', 'base_model.model.gpt_neox.layers.22.attention.masked_bias', 'base_model.model.gpt_neox.layers.22.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.22.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.22.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.22.attention.dense.weight', 'base_model.model.gpt_neox.layers.22.attention.dense.bias', 'base_model.model.gpt_neox.layers.22.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.22.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.22.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.22.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.23.input_layernorm.weight', 'base_model.model.gpt_neox.layers.23.input_layernorm.bias', 'base_model.model.gpt_neox.layers.23.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.23.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.23.attention.bias', 'base_model.model.gpt_neox.layers.23.attention.masked_bias', 'base_model.model.gpt_neox.layers.23.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.23.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.23.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.23.attention.dense.weight', 'base_model.model.gpt_neox.layers.23.attention.dense.bias', 'base_model.model.gpt_neox.layers.23.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.23.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.23.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.23.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.24.input_layernorm.weight', 'base_model.model.gpt_neox.layers.24.input_layernorm.bias', 'base_model.model.gpt_neox.layers.24.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.24.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.24.attention.bias', 'base_model.model.gpt_neox.layers.24.attention.masked_bias', 'base_model.model.gpt_neox.layers.24.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.24.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.24.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.24.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.24.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.24.attention.dense.weight', 'base_model.model.gpt_neox.layers.24.attention.dense.bias', 'base_model.model.gpt_neox.layers.24.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.24.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.24.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.24.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.25.input_layernorm.weight', 'base_model.model.gpt_neox.layers.25.input_layernorm.bias', 'base_model.model.gpt_neox.layers.25.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.25.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.25.attention.bias', 'base_model.model.gpt_neox.layers.25.attention.masked_bias', 'base_model.model.gpt_neox.layers.25.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.25.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.25.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.25.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.25.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.25.attention.dense.weight', 'base_model.model.gpt_neox.layers.25.attention.dense.bias', 'base_model.model.gpt_neox.layers.25.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.25.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.25.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.25.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.26.input_layernorm.weight', 'base_model.model.gpt_neox.layers.26.input_layernorm.bias', 'base_model.model.gpt_neox.layers.26.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.26.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.26.attention.bias', 'base_model.model.gpt_neox.layers.26.attention.masked_bias', 'base_model.model.gpt_neox.layers.26.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.26.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.26.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.26.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.26.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.26.attention.dense.weight', 'base_model.model.gpt_neox.layers.26.attention.dense.bias', 'base_model.model.gpt_neox.layers.26.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.26.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.26.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.26.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.27.input_layernorm.weight', 'base_model.model.gpt_neox.layers.27.input_layernorm.bias', 'base_model.model.gpt_neox.layers.27.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.27.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.27.attention.bias', 'base_model.model.gpt_neox.layers.27.attention.masked_bias', 'base_model.model.gpt_neox.layers.27.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.27.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.27.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.27.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.27.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.27.attention.dense.weight', 'base_model.model.gpt_neox.layers.27.attention.dense.bias', 'base_model.model.gpt_neox.layers.27.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.27.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.27.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.27.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.28.input_layernorm.weight', 'base_model.model.gpt_neox.layers.28.input_layernorm.bias', 'base_model.model.gpt_neox.layers.28.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.28.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.28.attention.bias', 'base_model.model.gpt_neox.layers.28.attention.masked_bias', 'base_model.model.gpt_neox.layers.28.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.28.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.28.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.28.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.28.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.28.attention.dense.weight', 'base_model.model.gpt_neox.layers.28.attention.dense.bias', 'base_model.model.gpt_neox.layers.28.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.28.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.28.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.28.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.29.input_layernorm.weight', 'base_model.model.gpt_neox.layers.29.input_layernorm.bias', 'base_model.model.gpt_neox.layers.29.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.29.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.29.attention.bias', 'base_model.model.gpt_neox.layers.29.attention.masked_bias', 'base_model.model.gpt_neox.layers.29.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.29.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.29.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.29.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.29.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.29.attention.dense.weight', 'base_model.model.gpt_neox.layers.29.attention.dense.bias', 'base_model.model.gpt_neox.layers.29.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.29.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.29.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.29.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.30.input_layernorm.weight', 'base_model.model.gpt_neox.layers.30.input_layernorm.bias', 'base_model.model.gpt_neox.layers.30.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.30.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.30.attention.bias', 'base_model.model.gpt_neox.layers.30.attention.masked_bias', 'base_model.model.gpt_neox.layers.30.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.30.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.30.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.30.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.30.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.30.attention.dense.weight', 'base_model.model.gpt_neox.layers.30.attention.dense.bias', 'base_model.model.gpt_neox.layers.30.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.30.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.30.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.30.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.31.input_layernorm.weight', 'base_model.model.gpt_neox.layers.31.input_layernorm.bias', 'base_model.model.gpt_neox.layers.31.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.31.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.31.attention.bias', 'base_model.model.gpt_neox.layers.31.attention.masked_bias', 'base_model.model.gpt_neox.layers.31.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.31.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.31.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.31.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.31.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.31.attention.dense.weight', 'base_model.model.gpt_neox.layers.31.attention.dense.bias', 'base_model.model.gpt_neox.layers.31.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.31.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.31.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.31.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.32.input_layernorm.weight', 'base_model.model.gpt_neox.layers.32.input_layernorm.bias', 'base_model.model.gpt_neox.layers.32.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.32.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.32.attention.bias', 'base_model.model.gpt_neox.layers.32.attention.masked_bias', 'base_model.model.gpt_neox.layers.32.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.32.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.32.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.32.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.32.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.32.attention.dense.weight', 'base_model.model.gpt_neox.layers.32.attention.dense.bias', 'base_model.model.gpt_neox.layers.32.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.32.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.32.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.32.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.33.input_layernorm.weight', 'base_model.model.gpt_neox.layers.33.input_layernorm.bias', 'base_model.model.gpt_neox.layers.33.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.33.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.33.attention.bias', 'base_model.model.gpt_neox.layers.33.attention.masked_bias', 'base_model.model.gpt_neox.layers.33.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.33.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.33.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.33.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.33.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.33.attention.dense.weight', 'base_model.model.gpt_neox.layers.33.attention.dense.bias', 'base_model.model.gpt_neox.layers.33.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.33.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.33.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.33.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.34.input_layernorm.weight', 'base_model.model.gpt_neox.layers.34.input_layernorm.bias', 'base_model.model.gpt_neox.layers.34.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.34.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.34.attention.bias', 'base_model.model.gpt_neox.layers.34.attention.masked_bias', 'base_model.model.gpt_neox.layers.34.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.34.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.34.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.34.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.34.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.34.attention.dense.weight', 'base_model.model.gpt_neox.layers.34.attention.dense.bias', 'base_model.model.gpt_neox.layers.34.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.34.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.34.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.34.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.layers.35.input_layernorm.weight', 'base_model.model.gpt_neox.layers.35.input_layernorm.bias', 'base_model.model.gpt_neox.layers.35.post_attention_layernorm.weight', 'base_model.model.gpt_neox.layers.35.post_attention_layernorm.bias', 'base_model.model.gpt_neox.layers.35.attention.bias', 'base_model.model.gpt_neox.layers.35.attention.masked_bias', 'base_model.model.gpt_neox.layers.35.attention.rotary_emb.inv_freq', 'base_model.model.gpt_neox.layers.35.attention.query_key_value.weight', 'base_model.model.gpt_neox.layers.35.attention.query_key_value.bias', 'base_model.model.gpt_neox.layers.35.attention.query_key_value.lora_A.default.weight', 'base_model.model.gpt_neox.layers.35.attention.query_key_value.lora_B.default.weight', 'base_model.model.gpt_neox.layers.35.attention.dense.weight', 'base_model.model.gpt_neox.layers.35.attention.dense.bias', 'base_model.model.gpt_neox.layers.35.mlp.dense_h_to_4h.weight', 'base_model.model.gpt_neox.layers.35.mlp.dense_h_to_4h.bias', 'base_model.model.gpt_neox.layers.35.mlp.dense_4h_to_h.weight', 'base_model.model.gpt_neox.layers.35.mlp.dense_4h_to_h.bias', 'base_model.model.gpt_neox.final_layer_norm.weight', 'base_model.model.gpt_neox.final_layer_norm.bias', 'base_model.model.embed_out.weight'].\n",
            "There were unexpected keys in the checkpoint model loaded: ['base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.24.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.24.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.25.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.25.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.26.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.26.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.27.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.27.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.28.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.28.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.29.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.29.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.30.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.30.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.31.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.31.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.32.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.32.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.33.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.33.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.34.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.34.attention.query_key_value.lora_B.weight', 'base_model.model.gpt_neox.layers.35.attention.query_key_value.lora_A.weight', 'base_model.model.gpt_neox.layers.35.attention.query_key_value.lora_B.weight'].\n",
            "{'train_runtime': 32040.0149, 'train_samples_per_second': 0.812, 'train_steps_per_second': 0.006, 'train_loss': 2.0331085094130867, 'epoch': 1.99}\n",
            "100% 202/202 [8:53:59<00:00, 158.61s/it]\n",
            "\n",
            " If there's a warning about missing keys above, please disregard :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## row infarence"
      ],
      "metadata": {
        "id": "Dr65tqVAJFlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-sft\", use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-sft\")\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    model = model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "F4dYa30RJHw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(qa):\n",
        "  PROMPT_FORMAT = '以下は、ユーザーの入力です。適切な応答を出力してください。<NL><NL>ユーザー: {instruction}<NL><NL>システム: <NL>'\n",
        "  PROMPT_FORMAT = '以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\\n\\n### 指示:\\n{instruction}\\n\\n### 応答:\\n'\n",
        "  prompt = PROMPT_FORMAT.format(instruction=qa)\n",
        "  return prompt\n",
        "  # return f\"ユーザー: {qa}<NL>システム:\"\n",
        "\n",
        "def gen(qa):\n",
        "  prompt = create_prompt(qa)\n",
        "  print(prompt)\n",
        "  token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  token_ids = token_ids.to(model.device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "          token_ids,\n",
        "          do_sample=True,\n",
        "          max_new_tokens=128,\n",
        "          temperature=0.9,\n",
        "          pad_token_id=tokenizer.pad_token_id,\n",
        "          bos_token_id=tokenizer.bos_token_id,\n",
        "          eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "  output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
        "  # output = output.replace(\"<NL>\", \"\\n\")\n",
        "  print(output)"
      ],
      "metadata": {
        "id": "6GdouuOxJN2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen('日本で一番高い山を教えてください。')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vW8dk90JkaX",
        "outputId": "f3fdb813-cd89-47be-b357-e7a5172b6db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\n",
            "\n",
            "### 指示:\n",
            "日本で一番高い山を教えてください。\n",
            "\n",
            "### 応答:\n",
            "\n",
            "北から順番に並べると、次のようになります: 9,8と称された峰から、最高峰の槍ヶ岳と次に高い山は御嶽山です。 この峰は、海抜21,003フィートの日本で2番目に高い山です。 したがって、答えは御嶽山です。</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen('色の三原色を教えてください。')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8yqWWg6bCV-",
        "outputId": "6516a14a-0964-487b-980c-f237970acc8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\n",
            "\n",
            "### 指示:\n",
            "色の三原色を教えてください。\n",
            "\n",
            "### 応答:\n",
            "\n",
            "はい、こちらになります。</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## infarence\n"
      ],
      "metadata": {
        "id": "d2L11q1duoOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "\n",
        "lora_weights = output_path\n",
        "# lora_weights = '/content/drive/MyDrive/models/rinna_dolly_3B'\n",
        "lora_weights = '/content/drive/MyDrive/models/rinna_dolly_3B_rinna_format'\n",
        "\n",
        "\n",
        "base_model = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
        "base_model=\"togethercomputer/RedPajama-INCITE-Base-7B-v0.1\"\n",
        "base_model=\"cyberagent/open-calm-3b\"\n",
        "base_model=\"rinna/japanese-gpt-neox-3.6b-instruction-sft\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# device = None\n",
        "device = \"cuda\"\n",
        "\n",
        "if device == \"cuda\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model,\n",
        "                load_in_8bit=True,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\",\n",
        "            )\n",
        "    model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            lora_weights,\n",
        "            torch_dtype=torch.float16,\n",
        "        )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            lora_weights,\n",
        "            device_map={\"\": device},\n",
        "        )\n",
        "    \n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7g1TaNvupca",
        "outputId": "a1ff6cea-8636-4091-a668-ead7c80fced4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "# tokenizer.padding_side = \"left\"  # Allow batched inference\n",
        "\n",
        "\n",
        "def gen_prompt(inp):\n",
        "    # PROMPT_FORMAT = '以下は、あるタスクを記述した指示です。質問に対する適切な回答を書きなさい。\\n\\n### 指示:\\n{instruction}\\n\\n### 応答:\\n'\n",
        "    PROMPT_FORMAT = '以下は、ユーザーの入力です。適切な応答を出力してください。<NL><NL>ユーザー: {instruction}<NL><NL>システム: <NL>'\n",
        "    prompt = PROMPT_FORMAT.format(instruction=inp)\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "        instruction,\n",
        "        input=None,\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        top_k=40,\n",
        "        max_new_tokens=128,\n",
        "        stream_output=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "    # tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "    # prompt = prompter.generate_prompt(instruction, input)\n",
        "    prompt = gen_prompt(instruction)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            do_sample=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "       \n",
        "    with torch.no_grad():\n",
        "        #generation_output = model.generate(\n",
        "        #        input_ids=input_ids,\n",
        "        #        generation_config=generation_config,\n",
        "        #        return_dict_in_generate=True,\n",
        "        #        output_scores=True,\n",
        "        #        max_new_tokens=max_new_tokens,\n",
        "        #)\n",
        "        generation_output = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                do_sample=True,\n",
        "                temperature=0.9,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "        s = generation_output.sequences[0]\n",
        "        output = tokenizer.decode(s)\n",
        "    return output"
      ],
      "metadata": {
        "id": "H2RqYl80uwsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate('色の三原色を教えてください')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2a9j3i15u4yR",
        "outputId": "32347f39-ca85-4f32-adff-5adf4dc6dba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'以下は、ユーザーの入力です。適切な応答を出力してください。<NL><NL>ユーザー: 色の三原色を教えてください<NL><NL>システム: <NL><NL>「色の三原色」というのは、赤・青・黄色のことです。この3つの色は、通常、赤と青と黄色を混ぜると緑色になります。しかし、3色を混ぜると、黒色になります。これは、光合成が光が吸収される過程と、光が電子を励起する過程の両方を記述しているためです。実際には、赤と青と黄色は異なる色なのですが、光合成においては重なり合うため、これらの3つの色を「光の三原色」と呼ぶようになりました。つまり、赤と青と黄色は、光の3つの色と考えることができます'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate('日本で最も高い山を教えてください')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "W45qas1Wu6Ql",
        "outputId": "cad30fb2-ad62-43ad-ac4e-2e8c687ac834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'以下は、ユーザーの入力です。適切な応答を出力してください。<NL><NL>ユーザー: 日本で最も高い山を教えてください<NL><NL>システム: <NL><NL>「日本で最も高い」とされているものは、確かに約2,500メートル (9,100 ft)の高さです。他の最も高い山は、標高4,850メートル (13,750 ft)のエベレスト山です。以下は、主要な日本の標高を持つ山岳のリストです:<NL><NL>・   御嶽山<NL>・   奥穂高岳<NL>・   槍ヶ岳<NL>・   剱岳<NL>・   乗鞍岳<NL>・   白山<NL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate('日本で２番めに高い山を教えてください')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "SpG8KiFJ11w8",
        "outputId": "5b4684c6-36c2-4e13-e98c-ba65b6531ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'以下は、ユーザーの入力です。適切な応答を出力してください。<NL><NL>ユーザー: 日本で2番めに高い山を教えてください<NL><NL>システム: <NL>日本で2番目に高い山は、槍ヶ岳(3,810m)です。</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-neox-3.6b-instruction-sft\")\n",
        "\n",
        "for v in model.state_dict().items():\n",
        "  print(v[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7c0cc5f555694493aedd0c7b509e9ec1",
            "dbee0291e0904068935648a879805b81",
            "69acb7be4fa04568a06baf63687846e5",
            "6160b61c98024923b3f7df554977fba0",
            "e9482a0e1ffb4653ab0bd5ad4dc61e38",
            "dd4dc1c0af724ecb9ffbc2b6fc9e6e9f",
            "992551d49a2a4fccb1d17ca998fb3036",
            "e7bbc096cff74869a7aed9480c488928",
            "ca8ab9281fc24f6daf2ac065945c5f34",
            "39aef34bbe0e4336b21a2640974ad4d7",
            "7dba55e7c76143caa321b123bc376ad7",
            "9f3e8570231a4ce6a84edaa8fb3cc983",
            "db45a4fe56664d289c4143efc898166b",
            "bc67e9c89abc48a8a4401cd20a40c750",
            "a3368d5067354fafbec318e8ae8393ee",
            "2ebdb5f00ae744888f9a70c6eab9ce72",
            "4308843af5b1495aac90eae6945b000f",
            "7ef77eb18f0e4a77934397da6ee773ca",
            "44cfdf235a4b4b4cb6ace8cd389e0448",
            "29cf990756aa4e50a6394f11849e250c",
            "35f83fde88524e6691975071d4888fcc",
            "14e8c77e8c1244f2a86361ca8cc92036"
          ]
        },
        "id": "Cq2wNiTw7ZLl",
        "outputId": "722cc9c5-7f1c-4279-ef8e-2ae5c41c0fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/534 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c0cc5f555694493aedd0c7b509e9ec1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/7.37G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f3e8570231a4ce6a84edaa8fb3cc983"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt_neox.embed_in.weight\n",
            "gpt_neox.layers.0.input_layernorm.weight\n",
            "gpt_neox.layers.0.input_layernorm.bias\n",
            "gpt_neox.layers.0.post_attention_layernorm.weight\n",
            "gpt_neox.layers.0.post_attention_layernorm.bias\n",
            "gpt_neox.layers.0.attention.bias\n",
            "gpt_neox.layers.0.attention.masked_bias\n",
            "gpt_neox.layers.0.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.0.attention.query_key_value.weight\n",
            "gpt_neox.layers.0.attention.query_key_value.bias\n",
            "gpt_neox.layers.0.attention.dense.weight\n",
            "gpt_neox.layers.0.attention.dense.bias\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.1.input_layernorm.weight\n",
            "gpt_neox.layers.1.input_layernorm.bias\n",
            "gpt_neox.layers.1.post_attention_layernorm.weight\n",
            "gpt_neox.layers.1.post_attention_layernorm.bias\n",
            "gpt_neox.layers.1.attention.bias\n",
            "gpt_neox.layers.1.attention.masked_bias\n",
            "gpt_neox.layers.1.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.1.attention.query_key_value.weight\n",
            "gpt_neox.layers.1.attention.query_key_value.bias\n",
            "gpt_neox.layers.1.attention.dense.weight\n",
            "gpt_neox.layers.1.attention.dense.bias\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.2.input_layernorm.weight\n",
            "gpt_neox.layers.2.input_layernorm.bias\n",
            "gpt_neox.layers.2.post_attention_layernorm.weight\n",
            "gpt_neox.layers.2.post_attention_layernorm.bias\n",
            "gpt_neox.layers.2.attention.bias\n",
            "gpt_neox.layers.2.attention.masked_bias\n",
            "gpt_neox.layers.2.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.2.attention.query_key_value.weight\n",
            "gpt_neox.layers.2.attention.query_key_value.bias\n",
            "gpt_neox.layers.2.attention.dense.weight\n",
            "gpt_neox.layers.2.attention.dense.bias\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.3.input_layernorm.weight\n",
            "gpt_neox.layers.3.input_layernorm.bias\n",
            "gpt_neox.layers.3.post_attention_layernorm.weight\n",
            "gpt_neox.layers.3.post_attention_layernorm.bias\n",
            "gpt_neox.layers.3.attention.bias\n",
            "gpt_neox.layers.3.attention.masked_bias\n",
            "gpt_neox.layers.3.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.3.attention.query_key_value.weight\n",
            "gpt_neox.layers.3.attention.query_key_value.bias\n",
            "gpt_neox.layers.3.attention.dense.weight\n",
            "gpt_neox.layers.3.attention.dense.bias\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.4.input_layernorm.weight\n",
            "gpt_neox.layers.4.input_layernorm.bias\n",
            "gpt_neox.layers.4.post_attention_layernorm.weight\n",
            "gpt_neox.layers.4.post_attention_layernorm.bias\n",
            "gpt_neox.layers.4.attention.bias\n",
            "gpt_neox.layers.4.attention.masked_bias\n",
            "gpt_neox.layers.4.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.4.attention.query_key_value.weight\n",
            "gpt_neox.layers.4.attention.query_key_value.bias\n",
            "gpt_neox.layers.4.attention.dense.weight\n",
            "gpt_neox.layers.4.attention.dense.bias\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.5.input_layernorm.weight\n",
            "gpt_neox.layers.5.input_layernorm.bias\n",
            "gpt_neox.layers.5.post_attention_layernorm.weight\n",
            "gpt_neox.layers.5.post_attention_layernorm.bias\n",
            "gpt_neox.layers.5.attention.bias\n",
            "gpt_neox.layers.5.attention.masked_bias\n",
            "gpt_neox.layers.5.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.5.attention.query_key_value.weight\n",
            "gpt_neox.layers.5.attention.query_key_value.bias\n",
            "gpt_neox.layers.5.attention.dense.weight\n",
            "gpt_neox.layers.5.attention.dense.bias\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.6.input_layernorm.weight\n",
            "gpt_neox.layers.6.input_layernorm.bias\n",
            "gpt_neox.layers.6.post_attention_layernorm.weight\n",
            "gpt_neox.layers.6.post_attention_layernorm.bias\n",
            "gpt_neox.layers.6.attention.bias\n",
            "gpt_neox.layers.6.attention.masked_bias\n",
            "gpt_neox.layers.6.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.6.attention.query_key_value.weight\n",
            "gpt_neox.layers.6.attention.query_key_value.bias\n",
            "gpt_neox.layers.6.attention.dense.weight\n",
            "gpt_neox.layers.6.attention.dense.bias\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.7.input_layernorm.weight\n",
            "gpt_neox.layers.7.input_layernorm.bias\n",
            "gpt_neox.layers.7.post_attention_layernorm.weight\n",
            "gpt_neox.layers.7.post_attention_layernorm.bias\n",
            "gpt_neox.layers.7.attention.bias\n",
            "gpt_neox.layers.7.attention.masked_bias\n",
            "gpt_neox.layers.7.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.7.attention.query_key_value.weight\n",
            "gpt_neox.layers.7.attention.query_key_value.bias\n",
            "gpt_neox.layers.7.attention.dense.weight\n",
            "gpt_neox.layers.7.attention.dense.bias\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.8.input_layernorm.weight\n",
            "gpt_neox.layers.8.input_layernorm.bias\n",
            "gpt_neox.layers.8.post_attention_layernorm.weight\n",
            "gpt_neox.layers.8.post_attention_layernorm.bias\n",
            "gpt_neox.layers.8.attention.bias\n",
            "gpt_neox.layers.8.attention.masked_bias\n",
            "gpt_neox.layers.8.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.8.attention.query_key_value.weight\n",
            "gpt_neox.layers.8.attention.query_key_value.bias\n",
            "gpt_neox.layers.8.attention.dense.weight\n",
            "gpt_neox.layers.8.attention.dense.bias\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.9.input_layernorm.weight\n",
            "gpt_neox.layers.9.input_layernorm.bias\n",
            "gpt_neox.layers.9.post_attention_layernorm.weight\n",
            "gpt_neox.layers.9.post_attention_layernorm.bias\n",
            "gpt_neox.layers.9.attention.bias\n",
            "gpt_neox.layers.9.attention.masked_bias\n",
            "gpt_neox.layers.9.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.9.attention.query_key_value.weight\n",
            "gpt_neox.layers.9.attention.query_key_value.bias\n",
            "gpt_neox.layers.9.attention.dense.weight\n",
            "gpt_neox.layers.9.attention.dense.bias\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.10.input_layernorm.weight\n",
            "gpt_neox.layers.10.input_layernorm.bias\n",
            "gpt_neox.layers.10.post_attention_layernorm.weight\n",
            "gpt_neox.layers.10.post_attention_layernorm.bias\n",
            "gpt_neox.layers.10.attention.bias\n",
            "gpt_neox.layers.10.attention.masked_bias\n",
            "gpt_neox.layers.10.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.10.attention.query_key_value.weight\n",
            "gpt_neox.layers.10.attention.query_key_value.bias\n",
            "gpt_neox.layers.10.attention.dense.weight\n",
            "gpt_neox.layers.10.attention.dense.bias\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.11.input_layernorm.weight\n",
            "gpt_neox.layers.11.input_layernorm.bias\n",
            "gpt_neox.layers.11.post_attention_layernorm.weight\n",
            "gpt_neox.layers.11.post_attention_layernorm.bias\n",
            "gpt_neox.layers.11.attention.bias\n",
            "gpt_neox.layers.11.attention.masked_bias\n",
            "gpt_neox.layers.11.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.11.attention.query_key_value.weight\n",
            "gpt_neox.layers.11.attention.query_key_value.bias\n",
            "gpt_neox.layers.11.attention.dense.weight\n",
            "gpt_neox.layers.11.attention.dense.bias\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.12.input_layernorm.weight\n",
            "gpt_neox.layers.12.input_layernorm.bias\n",
            "gpt_neox.layers.12.post_attention_layernorm.weight\n",
            "gpt_neox.layers.12.post_attention_layernorm.bias\n",
            "gpt_neox.layers.12.attention.bias\n",
            "gpt_neox.layers.12.attention.masked_bias\n",
            "gpt_neox.layers.12.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.12.attention.query_key_value.weight\n",
            "gpt_neox.layers.12.attention.query_key_value.bias\n",
            "gpt_neox.layers.12.attention.dense.weight\n",
            "gpt_neox.layers.12.attention.dense.bias\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.13.input_layernorm.weight\n",
            "gpt_neox.layers.13.input_layernorm.bias\n",
            "gpt_neox.layers.13.post_attention_layernorm.weight\n",
            "gpt_neox.layers.13.post_attention_layernorm.bias\n",
            "gpt_neox.layers.13.attention.bias\n",
            "gpt_neox.layers.13.attention.masked_bias\n",
            "gpt_neox.layers.13.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.13.attention.query_key_value.weight\n",
            "gpt_neox.layers.13.attention.query_key_value.bias\n",
            "gpt_neox.layers.13.attention.dense.weight\n",
            "gpt_neox.layers.13.attention.dense.bias\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.14.input_layernorm.weight\n",
            "gpt_neox.layers.14.input_layernorm.bias\n",
            "gpt_neox.layers.14.post_attention_layernorm.weight\n",
            "gpt_neox.layers.14.post_attention_layernorm.bias\n",
            "gpt_neox.layers.14.attention.bias\n",
            "gpt_neox.layers.14.attention.masked_bias\n",
            "gpt_neox.layers.14.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.14.attention.query_key_value.weight\n",
            "gpt_neox.layers.14.attention.query_key_value.bias\n",
            "gpt_neox.layers.14.attention.dense.weight\n",
            "gpt_neox.layers.14.attention.dense.bias\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.15.input_layernorm.weight\n",
            "gpt_neox.layers.15.input_layernorm.bias\n",
            "gpt_neox.layers.15.post_attention_layernorm.weight\n",
            "gpt_neox.layers.15.post_attention_layernorm.bias\n",
            "gpt_neox.layers.15.attention.bias\n",
            "gpt_neox.layers.15.attention.masked_bias\n",
            "gpt_neox.layers.15.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.15.attention.query_key_value.weight\n",
            "gpt_neox.layers.15.attention.query_key_value.bias\n",
            "gpt_neox.layers.15.attention.dense.weight\n",
            "gpt_neox.layers.15.attention.dense.bias\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.16.input_layernorm.weight\n",
            "gpt_neox.layers.16.input_layernorm.bias\n",
            "gpt_neox.layers.16.post_attention_layernorm.weight\n",
            "gpt_neox.layers.16.post_attention_layernorm.bias\n",
            "gpt_neox.layers.16.attention.bias\n",
            "gpt_neox.layers.16.attention.masked_bias\n",
            "gpt_neox.layers.16.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.16.attention.query_key_value.weight\n",
            "gpt_neox.layers.16.attention.query_key_value.bias\n",
            "gpt_neox.layers.16.attention.dense.weight\n",
            "gpt_neox.layers.16.attention.dense.bias\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.17.input_layernorm.weight\n",
            "gpt_neox.layers.17.input_layernorm.bias\n",
            "gpt_neox.layers.17.post_attention_layernorm.weight\n",
            "gpt_neox.layers.17.post_attention_layernorm.bias\n",
            "gpt_neox.layers.17.attention.bias\n",
            "gpt_neox.layers.17.attention.masked_bias\n",
            "gpt_neox.layers.17.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.17.attention.query_key_value.weight\n",
            "gpt_neox.layers.17.attention.query_key_value.bias\n",
            "gpt_neox.layers.17.attention.dense.weight\n",
            "gpt_neox.layers.17.attention.dense.bias\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.18.input_layernorm.weight\n",
            "gpt_neox.layers.18.input_layernorm.bias\n",
            "gpt_neox.layers.18.post_attention_layernorm.weight\n",
            "gpt_neox.layers.18.post_attention_layernorm.bias\n",
            "gpt_neox.layers.18.attention.bias\n",
            "gpt_neox.layers.18.attention.masked_bias\n",
            "gpt_neox.layers.18.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.18.attention.query_key_value.weight\n",
            "gpt_neox.layers.18.attention.query_key_value.bias\n",
            "gpt_neox.layers.18.attention.dense.weight\n",
            "gpt_neox.layers.18.attention.dense.bias\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.19.input_layernorm.weight\n",
            "gpt_neox.layers.19.input_layernorm.bias\n",
            "gpt_neox.layers.19.post_attention_layernorm.weight\n",
            "gpt_neox.layers.19.post_attention_layernorm.bias\n",
            "gpt_neox.layers.19.attention.bias\n",
            "gpt_neox.layers.19.attention.masked_bias\n",
            "gpt_neox.layers.19.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.19.attention.query_key_value.weight\n",
            "gpt_neox.layers.19.attention.query_key_value.bias\n",
            "gpt_neox.layers.19.attention.dense.weight\n",
            "gpt_neox.layers.19.attention.dense.bias\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.20.input_layernorm.weight\n",
            "gpt_neox.layers.20.input_layernorm.bias\n",
            "gpt_neox.layers.20.post_attention_layernorm.weight\n",
            "gpt_neox.layers.20.post_attention_layernorm.bias\n",
            "gpt_neox.layers.20.attention.bias\n",
            "gpt_neox.layers.20.attention.masked_bias\n",
            "gpt_neox.layers.20.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.20.attention.query_key_value.weight\n",
            "gpt_neox.layers.20.attention.query_key_value.bias\n",
            "gpt_neox.layers.20.attention.dense.weight\n",
            "gpt_neox.layers.20.attention.dense.bias\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.21.input_layernorm.weight\n",
            "gpt_neox.layers.21.input_layernorm.bias\n",
            "gpt_neox.layers.21.post_attention_layernorm.weight\n",
            "gpt_neox.layers.21.post_attention_layernorm.bias\n",
            "gpt_neox.layers.21.attention.bias\n",
            "gpt_neox.layers.21.attention.masked_bias\n",
            "gpt_neox.layers.21.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.21.attention.query_key_value.weight\n",
            "gpt_neox.layers.21.attention.query_key_value.bias\n",
            "gpt_neox.layers.21.attention.dense.weight\n",
            "gpt_neox.layers.21.attention.dense.bias\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.22.input_layernorm.weight\n",
            "gpt_neox.layers.22.input_layernorm.bias\n",
            "gpt_neox.layers.22.post_attention_layernorm.weight\n",
            "gpt_neox.layers.22.post_attention_layernorm.bias\n",
            "gpt_neox.layers.22.attention.bias\n",
            "gpt_neox.layers.22.attention.masked_bias\n",
            "gpt_neox.layers.22.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.22.attention.query_key_value.weight\n",
            "gpt_neox.layers.22.attention.query_key_value.bias\n",
            "gpt_neox.layers.22.attention.dense.weight\n",
            "gpt_neox.layers.22.attention.dense.bias\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.23.input_layernorm.weight\n",
            "gpt_neox.layers.23.input_layernorm.bias\n",
            "gpt_neox.layers.23.post_attention_layernorm.weight\n",
            "gpt_neox.layers.23.post_attention_layernorm.bias\n",
            "gpt_neox.layers.23.attention.bias\n",
            "gpt_neox.layers.23.attention.masked_bias\n",
            "gpt_neox.layers.23.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.23.attention.query_key_value.weight\n",
            "gpt_neox.layers.23.attention.query_key_value.bias\n",
            "gpt_neox.layers.23.attention.dense.weight\n",
            "gpt_neox.layers.23.attention.dense.bias\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.24.input_layernorm.weight\n",
            "gpt_neox.layers.24.input_layernorm.bias\n",
            "gpt_neox.layers.24.post_attention_layernorm.weight\n",
            "gpt_neox.layers.24.post_attention_layernorm.bias\n",
            "gpt_neox.layers.24.attention.bias\n",
            "gpt_neox.layers.24.attention.masked_bias\n",
            "gpt_neox.layers.24.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.24.attention.query_key_value.weight\n",
            "gpt_neox.layers.24.attention.query_key_value.bias\n",
            "gpt_neox.layers.24.attention.dense.weight\n",
            "gpt_neox.layers.24.attention.dense.bias\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.25.input_layernorm.weight\n",
            "gpt_neox.layers.25.input_layernorm.bias\n",
            "gpt_neox.layers.25.post_attention_layernorm.weight\n",
            "gpt_neox.layers.25.post_attention_layernorm.bias\n",
            "gpt_neox.layers.25.attention.bias\n",
            "gpt_neox.layers.25.attention.masked_bias\n",
            "gpt_neox.layers.25.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.25.attention.query_key_value.weight\n",
            "gpt_neox.layers.25.attention.query_key_value.bias\n",
            "gpt_neox.layers.25.attention.dense.weight\n",
            "gpt_neox.layers.25.attention.dense.bias\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.26.input_layernorm.weight\n",
            "gpt_neox.layers.26.input_layernorm.bias\n",
            "gpt_neox.layers.26.post_attention_layernorm.weight\n",
            "gpt_neox.layers.26.post_attention_layernorm.bias\n",
            "gpt_neox.layers.26.attention.bias\n",
            "gpt_neox.layers.26.attention.masked_bias\n",
            "gpt_neox.layers.26.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.26.attention.query_key_value.weight\n",
            "gpt_neox.layers.26.attention.query_key_value.bias\n",
            "gpt_neox.layers.26.attention.dense.weight\n",
            "gpt_neox.layers.26.attention.dense.bias\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.27.input_layernorm.weight\n",
            "gpt_neox.layers.27.input_layernorm.bias\n",
            "gpt_neox.layers.27.post_attention_layernorm.weight\n",
            "gpt_neox.layers.27.post_attention_layernorm.bias\n",
            "gpt_neox.layers.27.attention.bias\n",
            "gpt_neox.layers.27.attention.masked_bias\n",
            "gpt_neox.layers.27.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.27.attention.query_key_value.weight\n",
            "gpt_neox.layers.27.attention.query_key_value.bias\n",
            "gpt_neox.layers.27.attention.dense.weight\n",
            "gpt_neox.layers.27.attention.dense.bias\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.28.input_layernorm.weight\n",
            "gpt_neox.layers.28.input_layernorm.bias\n",
            "gpt_neox.layers.28.post_attention_layernorm.weight\n",
            "gpt_neox.layers.28.post_attention_layernorm.bias\n",
            "gpt_neox.layers.28.attention.bias\n",
            "gpt_neox.layers.28.attention.masked_bias\n",
            "gpt_neox.layers.28.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.28.attention.query_key_value.weight\n",
            "gpt_neox.layers.28.attention.query_key_value.bias\n",
            "gpt_neox.layers.28.attention.dense.weight\n",
            "gpt_neox.layers.28.attention.dense.bias\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.29.input_layernorm.weight\n",
            "gpt_neox.layers.29.input_layernorm.bias\n",
            "gpt_neox.layers.29.post_attention_layernorm.weight\n",
            "gpt_neox.layers.29.post_attention_layernorm.bias\n",
            "gpt_neox.layers.29.attention.bias\n",
            "gpt_neox.layers.29.attention.masked_bias\n",
            "gpt_neox.layers.29.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.29.attention.query_key_value.weight\n",
            "gpt_neox.layers.29.attention.query_key_value.bias\n",
            "gpt_neox.layers.29.attention.dense.weight\n",
            "gpt_neox.layers.29.attention.dense.bias\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.30.input_layernorm.weight\n",
            "gpt_neox.layers.30.input_layernorm.bias\n",
            "gpt_neox.layers.30.post_attention_layernorm.weight\n",
            "gpt_neox.layers.30.post_attention_layernorm.bias\n",
            "gpt_neox.layers.30.attention.bias\n",
            "gpt_neox.layers.30.attention.masked_bias\n",
            "gpt_neox.layers.30.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.30.attention.query_key_value.weight\n",
            "gpt_neox.layers.30.attention.query_key_value.bias\n",
            "gpt_neox.layers.30.attention.dense.weight\n",
            "gpt_neox.layers.30.attention.dense.bias\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.31.input_layernorm.weight\n",
            "gpt_neox.layers.31.input_layernorm.bias\n",
            "gpt_neox.layers.31.post_attention_layernorm.weight\n",
            "gpt_neox.layers.31.post_attention_layernorm.bias\n",
            "gpt_neox.layers.31.attention.bias\n",
            "gpt_neox.layers.31.attention.masked_bias\n",
            "gpt_neox.layers.31.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.31.attention.query_key_value.weight\n",
            "gpt_neox.layers.31.attention.query_key_value.bias\n",
            "gpt_neox.layers.31.attention.dense.weight\n",
            "gpt_neox.layers.31.attention.dense.bias\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.32.input_layernorm.weight\n",
            "gpt_neox.layers.32.input_layernorm.bias\n",
            "gpt_neox.layers.32.post_attention_layernorm.weight\n",
            "gpt_neox.layers.32.post_attention_layernorm.bias\n",
            "gpt_neox.layers.32.attention.bias\n",
            "gpt_neox.layers.32.attention.masked_bias\n",
            "gpt_neox.layers.32.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.32.attention.query_key_value.weight\n",
            "gpt_neox.layers.32.attention.query_key_value.bias\n",
            "gpt_neox.layers.32.attention.dense.weight\n",
            "gpt_neox.layers.32.attention.dense.bias\n",
            "gpt_neox.layers.32.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.32.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.32.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.32.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.33.input_layernorm.weight\n",
            "gpt_neox.layers.33.input_layernorm.bias\n",
            "gpt_neox.layers.33.post_attention_layernorm.weight\n",
            "gpt_neox.layers.33.post_attention_layernorm.bias\n",
            "gpt_neox.layers.33.attention.bias\n",
            "gpt_neox.layers.33.attention.masked_bias\n",
            "gpt_neox.layers.33.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.33.attention.query_key_value.weight\n",
            "gpt_neox.layers.33.attention.query_key_value.bias\n",
            "gpt_neox.layers.33.attention.dense.weight\n",
            "gpt_neox.layers.33.attention.dense.bias\n",
            "gpt_neox.layers.33.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.33.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.33.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.33.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.34.input_layernorm.weight\n",
            "gpt_neox.layers.34.input_layernorm.bias\n",
            "gpt_neox.layers.34.post_attention_layernorm.weight\n",
            "gpt_neox.layers.34.post_attention_layernorm.bias\n",
            "gpt_neox.layers.34.attention.bias\n",
            "gpt_neox.layers.34.attention.masked_bias\n",
            "gpt_neox.layers.34.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.34.attention.query_key_value.weight\n",
            "gpt_neox.layers.34.attention.query_key_value.bias\n",
            "gpt_neox.layers.34.attention.dense.weight\n",
            "gpt_neox.layers.34.attention.dense.bias\n",
            "gpt_neox.layers.34.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.34.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.34.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.34.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.35.input_layernorm.weight\n",
            "gpt_neox.layers.35.input_layernorm.bias\n",
            "gpt_neox.layers.35.post_attention_layernorm.weight\n",
            "gpt_neox.layers.35.post_attention_layernorm.bias\n",
            "gpt_neox.layers.35.attention.bias\n",
            "gpt_neox.layers.35.attention.masked_bias\n",
            "gpt_neox.layers.35.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.35.attention.query_key_value.weight\n",
            "gpt_neox.layers.35.attention.query_key_value.bias\n",
            "gpt_neox.layers.35.attention.dense.weight\n",
            "gpt_neox.layers.35.attention.dense.bias\n",
            "gpt_neox.layers.35.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.35.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.35.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.35.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.final_layer_norm.weight\n",
            "gpt_neox.final_layer_norm.bias\n",
            "embed_out.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cpp (fail)"
      ],
      "metadata": {
        "id": "KRT0pa20x4a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "#!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "#cpp_repo=\"/content/llama.cpp\"\n",
        "\n",
        "!git clone https://github.com/ggerganov/ggml\n",
        "cpp_repo=\"/content/ggml\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex8OQKIny9JK",
        "outputId": "b2a55237-b104-4964-ea79-f58cc14548e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'ggml'...\n",
            "remote: Enumerating objects: 1512, done.\u001b[K\n",
            "remote: Counting objects: 100% (639/639), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 1512 (delta 540), reused 553 (delta 489), pack-reused 873\u001b[K\n",
            "Receiving objects: 100% (1512/1512), 3.94 MiB | 6.16 MiB/s, done.\n",
            "Resolving deltas: 100% (982/982), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $cpp_repo\n",
        "!mkdir -p build\n",
        "%cd $cpp_repo/build\n",
        "!cmake ..\n",
        "# !cmake --build . --config Release\n",
        "!make -j\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvbESVoSzDAS",
        "outputId": "618f59f8-f456-43d7-bb03-05f059647e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ggml\n",
            "/content/ggml/build\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.25.1\") \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Linux detected\n",
            "-- x86 detected\n",
            "-- Linux detected\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/ggml/build\n",
            "[  1%] \u001b[32mBuilding CXX object examples/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object src/CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking C static library libggml.a\u001b[0m\n",
            "[  5%] Built target ggml\n",
            "[  7%] \u001b[32mBuilding C object tests/CMakeFiles/test-vec0.dir/test-vec0.c.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding C object tests/CMakeFiles/test-vec1.dir/test-vec1.c.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding C object tests/CMakeFiles/test-grad0.dir/test-grad0.c.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding C object tests/CMakeFiles/test-opt.dir/test-opt.c.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding C object tests/CMakeFiles/test-mul-mat0.dir/test-mul-mat0.c.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding C object tests/CMakeFiles/test-mul-mat2.dir/test-mul-mat2.c.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding C object tests/CMakeFiles/test1.dir/test1.c.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding C object tests/CMakeFiles/test3.dir/test3.c.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding C object tests/CMakeFiles/test0.dir/test0.c.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding C object tests/CMakeFiles/test2.dir/test2.c.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object examples/CMakeFiles/common-ggml.dir/common-ggml.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object examples/whisper/CMakeFiles/whisper-cpp.dir/whisper.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcheck_gradient\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:135:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  135 |                 printf(\"%s: ndims=%d, i=%d, k=\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, g0=%f, g1=%f, error_abs=%f, error_rel=%f\\n\",\n",
            "      |                                               \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                  \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                  \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                               \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  136 |                         op_name, ndims, i, \u001b[32m\u001b[Kk\u001b[m\u001b[K, g0, g1, error_abs, error_rel);\n",
            "      |                                            \u001b[32m\u001b[K~\u001b[m\u001b[K      \n",
            "      |                                            \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                            \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kcheck_mat_mul\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:179:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  179 |     printf(\"x0: [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld]\\n\", \u001b[32m\u001b[Kn00\u001b[m\u001b[K, n10, n20, n30);\n",
            "      |                  \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                        \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                     \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K            \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                  \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:179:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  179 |     printf(\"x0: [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld]\\n\", n00, \u001b[32m\u001b[Kn10\u001b[m\u001b[K, n20, n30);\n",
            "      |                        \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                       \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                           \u001b[01;35m\u001b[K|\u001b[m\u001b[K                       \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                           \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K           \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                        \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:179:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  179 |     printf(\"x0: [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld]\\n\", n00, n10, \u001b[32m\u001b[Kn20\u001b[m\u001b[K, n30);\n",
            "      |                              \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                      \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                                 \u001b[01;35m\u001b[K|\u001b[m\u001b[K                      \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                 \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K          \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:179:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  179 |     printf(\"x0: [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K]\\n\", n00, n10, n20, \u001b[32m\u001b[Kn30\u001b[m\u001b[K);\n",
            "      |                                    \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                     \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K                     \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                       \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K         \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                                    \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:188:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  188 |     printf(\"x1: [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld]\\n\", \u001b[32m\u001b[Kn01\u001b[m\u001b[K, n11, n21, n31);\n",
            "      |                  \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                        \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                     \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K            \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                  \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:188:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  188 |     printf(\"x1: [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld]\\n\", n01, \u001b[32m\u001b[Kn11\u001b[m\u001b[K, n21, n31);\n",
            "      |                        \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                       \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                           \u001b[01;35m\u001b[K|\u001b[m\u001b[K                       \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                           \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K           \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                        \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:188:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  188 |     printf(\"x1: [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld]\\n\", n01, n11, \u001b[32m\u001b[Kn21\u001b[m\u001b[K, n31);\n",
            "      |                              \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                      \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                                 \u001b[01;35m\u001b[K|\u001b[m\u001b[K                      \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                 \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K          \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:188:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  188 |     printf(\"x1: [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K]\\n\", n01, n11, n21, \u001b[32m\u001b[Kn31\u001b[m\u001b[K);\n",
            "      |                                    \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                     \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K                     \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                       \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K         \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                                    \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:197:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  197 |     printf(\"y: [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld]\\n\", \u001b[32m\u001b[Kn02\u001b[m\u001b[K, n12, n22, n32);\n",
            "      |                 \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                        \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                    \u001b[01;35m\u001b[K|\u001b[m\u001b[K                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                    \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K            \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                 \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:197:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  197 |     printf(\"y: [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld]\\n\", n02, \u001b[32m\u001b[Kn12\u001b[m\u001b[K, n22, n32);\n",
            "      |                       \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                       \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K                       \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K           \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                       \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:197:32:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  197 |     printf(\"y: [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld]\\n\", n02, n12, \u001b[32m\u001b[Kn22\u001b[m\u001b[K, n32);\n",
            "      |                             \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                      \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                                \u001b[01;35m\u001b[K|\u001b[m\u001b[K                      \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K          \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                             \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:197:38:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst long int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  197 |     printf(\"y: [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K]\\n\", n02, n12, n22, \u001b[32m\u001b[Kn32\u001b[m\u001b[K);\n",
            "      |                                   \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K                     \u001b[32m\u001b[K~~~\u001b[m\u001b[K\n",
            "      |                                      \u001b[01;35m\u001b[K|\u001b[m\u001b[K                     \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                      \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K         \u001b[32m\u001b[Kint64_t {aka const long int}\u001b[m\u001b[K\n",
            "      |                                   \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                            \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                               \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                               \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                            \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  268 |                            \u001b[32m\u001b[Km->ne[0]\u001b[m\u001b[K,    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "      |                            \u001b[32m\u001b[K~~~~~~~~\u001b[m\u001b[K            \n",
            "      |                                 \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                 \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                  \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                     \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                  \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  268 |                            m->ne[0],    \u001b[32m\u001b[Km->ne[1]\u001b[m\u001b[K,    m->ne[2],    m->ne[3],\n",
            "      |                                         \u001b[32m\u001b[K~~~~~~~~\u001b[m\u001b[K     \n",
            "      |                                              \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                              \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                        \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                           \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                           \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                        \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  268 |                            m->ne[0],    m->ne[1],    \u001b[32m\u001b[Km->ne[2]\u001b[m\u001b[K,    m->ne[3],\n",
            "      |                                                      \u001b[32m\u001b[K~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                                           \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                           \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:65:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                              \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                 \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                 \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  268 |                            m->ne[0],    m->ne[1],    m->ne[2],    \u001b[32m\u001b[Km->ne[3]\u001b[m\u001b[K,\n",
            "      |                                                                   \u001b[32m\u001b[K~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                                                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                        \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:74:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 6 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                       \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                          \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                       \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  268 |                            m->ne[0],    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "  269 |                         \u001b[32m\u001b[Kx[1]->ne[0]\u001b[m\u001b[K, x[1]->ne[1], x[1]->ne[2], x[1]->ne[3],\n",
            "      |                         \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                       \n",
            "      |                                 \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                 \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:80:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 7 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                             \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                             \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  268 |                            m->ne[0],    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "  269 |                         x[1]->ne[0], \u001b[32m\u001b[Kx[1]->ne[1]\u001b[m\u001b[K, x[1]->ne[2], x[1]->ne[3],\n",
            "      |                                      \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                \n",
            "      |                                              \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                              \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:86:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 8 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                                   \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                      \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                      \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                   \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  268 |                            m->ne[0],    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "  269 |                         x[1]->ne[0], x[1]->ne[1], \u001b[32m\u001b[Kx[1]->ne[2]\u001b[m\u001b[K, x[1]->ne[3],\n",
            "      |                                                   \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                         \n",
            "      |                                                           \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                           \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:92:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 9 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                                         \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                            \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                            \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                         \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  268 |                            m->ne[0],    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "  269 |                         x[1]->ne[0], x[1]->ne[1], x[1]->ne[2], \u001b[32m\u001b[Kx[1]->ne[3]\u001b[m\u001b[K,\n",
            "      |                                                                \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                  \n",
            "      |                                                                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                        \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 10 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                                     \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                                  \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "......\n",
            "  270 |                         \u001b[32m\u001b[Kx[0]->ne[0]\u001b[m\u001b[K, x[0]->ne[1], x[0]->ne[2], x[0]->ne[3]);\n",
            "      |                         \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                                                  \n",
            "      |                                 \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                 \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:107:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 11 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld]\\n\",\n",
            "      |                                                                                                        \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                                           \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                                           \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                                        \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "......\n",
            "  270 |                         x[0]->ne[0], \u001b[32m\u001b[Kx[0]->ne[1]\u001b[m\u001b[K, x[0]->ne[2], x[0]->ne[3]);\n",
            "      |                                      \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                                           \n",
            "      |                                              \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                              \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:113:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 12 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld]\\n\",\n",
            "      |                                                                                                              \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                                                 \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                                                 \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "......\n",
            "  270 |                         x[0]->ne[0], x[0]->ne[1], \u001b[32m\u001b[Kx[0]->ne[2]\u001b[m\u001b[K, x[0]->ne[3]);\n",
            "      |                                                   \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                                    \n",
            "      |                                                           \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                           \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:267:119:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 13 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  267 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K]\\n\",\n",
            "      |                                                                                                                    \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                                                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                                                       \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                                                    \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "......\n",
            "  270 |                         x[0]->ne[0], x[0]->ne[1], x[0]->ne[2], \u001b[32m\u001b[Kx[0]->ne[3]\u001b[m\u001b[K);\n",
            "      |                                                                \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                             \n",
            "      |                                                                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                        \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 2 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                            \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                               \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                               \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                            \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  304 |                            \u001b[32m\u001b[Km->ne[0]\u001b[m\u001b[K,    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "      |                            \u001b[32m\u001b[K~~~~~~~~\u001b[m\u001b[K            \n",
            "      |                                 \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                 \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:53:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 3 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                  \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                     \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                  \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  304 |                            m->ne[0],    \u001b[32m\u001b[Km->ne[1]\u001b[m\u001b[K,    m->ne[2],    m->ne[3],\n",
            "      |                                         \u001b[32m\u001b[K~~~~~~~~\u001b[m\u001b[K     \n",
            "      |                                              \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                              \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 4 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                        \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                           \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                           \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                        \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  304 |                            m->ne[0],    m->ne[1],    \u001b[32m\u001b[Km->ne[2]\u001b[m\u001b[K,    m->ne[3],\n",
            "      |                                                      \u001b[32m\u001b[K~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                                           \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                           \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:65:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 5 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                              \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                 \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                 \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  304 |                            m->ne[0],    m->ne[1],    m->ne[2],    \u001b[32m\u001b[Km->ne[3]\u001b[m\u001b[K,\n",
            "      |                                                                   \u001b[32m\u001b[K~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                                                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                        \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:74:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 6 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                       \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                          \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                       \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  304 |                            m->ne[0],    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "  305 |                         \u001b[32m\u001b[Kx[1]->ne[0]\u001b[m\u001b[K, x[1]->ne[1], x[1]->ne[2], x[1]->ne[3],\n",
            "      |                         \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                       \n",
            "      |                                 \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                 \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:80:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 7 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                             \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                             \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  304 |                            m->ne[0],    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "  305 |                         x[1]->ne[0], \u001b[32m\u001b[Kx[1]->ne[1]\u001b[m\u001b[K, x[1]->ne[2], x[1]->ne[3],\n",
            "      |                                      \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                \n",
            "      |                                              \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                              \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:86:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 8 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                                   \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                      \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                      \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                   \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  304 |                            m->ne[0],    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "  305 |                         x[1]->ne[0], x[1]->ne[1], \u001b[32m\u001b[Kx[1]->ne[2]\u001b[m\u001b[K, x[1]->ne[3],\n",
            "      |                                                   \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                         \n",
            "      |                                                           \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                           \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:92:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 9 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K] * [%lld, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                                         \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                            \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                            \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                         \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "  304 |                            m->ne[0],    m->ne[1],    m->ne[2],    m->ne[3],\n",
            "  305 |                         x[1]->ne[0], x[1]->ne[1], x[1]->ne[2], \u001b[32m\u001b[Kx[1]->ne[3]\u001b[m\u001b[K,\n",
            "      |                                                                \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                  \n",
            "      |                                                                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                        \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:101:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 10 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [\u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld, %lld]\\n\",\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                                     \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                                  \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "......\n",
            "  306 |                         \u001b[32m\u001b[Kx[0]->ne[0]\u001b[m\u001b[K, x[0]->ne[1], x[0]->ne[2], x[0]->ne[3]);\n",
            "      |                         \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                                                  \n",
            "      |                                 \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                 \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:107:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 11 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld, %lld]\\n\",\n",
            "      |                                                                                                        \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                                           \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                                           \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                                        \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "......\n",
            "  306 |                         x[0]->ne[0], \u001b[32m\u001b[Kx[0]->ne[1]\u001b[m\u001b[K, x[0]->ne[2], x[0]->ne[3]);\n",
            "      |                                      \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                                           \n",
            "      |                                              \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                              \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:113:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 12 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K, %lld]\\n\",\n",
            "      |                                                                                                              \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                                                 \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                                                 \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                                              \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "......\n",
            "  306 |                         x[0]->ne[0], x[0]->ne[1], \u001b[32m\u001b[Kx[0]->ne[2]\u001b[m\u001b[K, x[0]->ne[3]);\n",
            "      |                                                   \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                                    \n",
            "      |                                                           \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                           \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/ggml/tests/test-mul-mat0.c:303:119:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat ‘\u001b[01m\u001b[K%lld\u001b[m\u001b[K’ expects argument of type ‘\u001b[01m\u001b[Klong long int\u001b[m\u001b[K’, but argument 13 has type ‘\u001b[01m\u001b[Kint64_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wformat=\u001b[m\u001b[K]\n",
            "  303 |                 printf(\"testing: mul_mat, [%lld, %lld, %lld, %lld] = [%lld, %lld, %lld, %lld] * [%lld, %lld, %lld, \u001b[01;35m\u001b[K%lld\u001b[m\u001b[K]\\n\",\n",
            "      |                                                                                                                    \u001b[01;35m\u001b[K~~~^\u001b[m\u001b[K\n",
            "      |                                                                                                                       \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                                                                       \u001b[01;35m\u001b[Klong long int\u001b[m\u001b[K\n",
            "      |                                                                                                                    \u001b[32m\u001b[K%ld\u001b[m\u001b[K\n",
            "......\n",
            "  306 |                         x[0]->ne[0], x[0]->ne[1], x[0]->ne[2], \u001b[32m\u001b[Kx[0]->ne[3]\u001b[m\u001b[K);\n",
            "      |                                                                \u001b[32m\u001b[K~~~~~~~~~~~\u001b[m\u001b[K                                             \n",
            "      |                                                                        \u001b[32m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                                                        \u001b[32m\u001b[Kint64_t {aka long int}\u001b[m\u001b[K\n",
            "[ 29%] \u001b[32m\u001b[1mLinking C executable ../bin/test0\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking C executable ../bin/test-vec0\u001b[0m\n",
            "[ 31%] Built target test0\n",
            "[ 33%] \u001b[32m\u001b[1mLinking C executable ../bin/test3\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking C executable ../bin/test-opt\u001b[0m\n",
            "[ 35%] Built target test-vec0\n",
            "[ 37%] \u001b[32m\u001b[1mLinking C executable ../bin/test2\u001b[0m\n",
            "[ 37%] Built target test-opt\n",
            "[ 37%] Built target test2\n",
            "[ 37%] Built target test3\n",
            "[ 38%] \u001b[32m\u001b[1mLinking C executable ../bin/test1\u001b[0m\n",
            "[ 38%] Built target test1\n",
            "[ 40%] \u001b[32m\u001b[1mLinking C executable ../bin/test-mul-mat0\u001b[0m\n",
            "[ 40%] Built target test-mul-mat0\n",
            "[ 42%] \u001b[32m\u001b[1mLinking C executable ../bin/test-vec1\u001b[0m\n",
            "[ 42%] Built target test-vec1\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 44%] Built target common\n",
            "[ 46%] \u001b[32mBuilding CXX object examples/mnist/CMakeFiles/mnist.dir/main.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking C executable ../bin/test-mul-mat2\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking C executable ../bin/test-grad0\u001b[0m\n",
            "[ 50%] Built target test-mul-mat2\n",
            "[ 50%] Built target test-grad0\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/mnist\u001b[0m\n",
            "[ 51%] Built target mnist\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX static library libcommon-ggml.a\u001b[0m\n",
            "[ 53%] Built target common-ggml\n",
            "[ 55%] \u001b[32mBuilding CXX object examples/gpt-2/CMakeFiles/gpt-2.dir/main.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object examples/gpt-2/CMakeFiles/gpt-2-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/gpt-j/CMakeFiles/gpt-j.dir/main.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/whisper/CMakeFiles/whisper-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/gpt-j/CMakeFiles/gpt-j-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/gpt-neox/CMakeFiles/gpt-neox.dir/main.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/gpt-neox/CMakeFiles/gpt-neox-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/dolly-v2/CMakeFiles/dollyv2.dir/main.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/starcoder/CMakeFiles/starcoder.dir/main.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/dolly-v2/CMakeFiles/dollyv2-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/starcoder/CMakeFiles/starcoder-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-j-quantize\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-2-quantize\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/starcoder-quantize\u001b[0m\n",
            "[ 79%] Built target gpt-j-quantize\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/dollyv2-quantize\u001b[0m\n",
            "[ 81%] Built target gpt-2-quantize\n",
            "[ 81%] Built target starcoder-quantize\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/whisper-quantize\u001b[0m\n",
            "[ 83%] Built target whisper-quantize\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-neox-quantize\u001b[0m\n",
            "[ 85%] Built target dollyv2-quantize\n",
            "[ 85%] Built target gpt-neox-quantize\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-j\u001b[0m\n",
            "[ 87%] Built target gpt-j\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-neox\u001b[0m\n",
            "[ 88%] Built target gpt-neox\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gpt-2\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/starcoder\u001b[0m\n",
            "[ 92%] Built target gpt-2\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/dollyv2\u001b[0m\n",
            "[ 94%] Built target starcoder\n",
            "[ 94%] Built target dollyv2\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX static library libwhisper-cpp.a\u001b[0m\n",
            "[ 96%] Built target whisper-cpp\n",
            "[ 98%] \u001b[32mBuilding CXX object examples/whisper/CMakeFiles/whisper.dir/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/whisper\u001b[0m\n",
            "[100%] Built target whisper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p $cpp_repo/models/3B\n",
        "%cd $cpp_repo/models/3B\n",
        "\n",
        "!wget \"https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/pytorch_model.bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u999laLT6AhT",
        "outputId": "386e6aaa-12ab-49e8-d08c-f8caa4e13504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/models/3B\n",
            "--2023-05-17 09:39:17--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.32.121.122, 13.32.121.103, 13.32.121.37, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.32.121.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/0c6124c628f8ecc29be1b6ee0625670062340f5b99cfe543ccf049fa90e6207b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1684574501&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvMGM2MTI0YzYyOGY4ZWNjMjliZTFiNmVlMDYyNTY3MDA2MjM0MGY1Yjk5Y2ZlNTQzY2NmMDQ5ZmE5MGU2MjA3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ1NzQ1MDF9fX1dfQ__&Signature=lljTTKk2RXByetw91sgQdtrkZgh-HJip-kXgSVRA2JC0nht6g3Diu6kDH9arW2Zooy1Xn1thpg6ouMZpwqVCN7HDutHAHoEWr5rBvqv0dVwhc4V3eYvXXIVB211ROCCbYOlD1pDCFQqBh-45tn5fTpb%7EPsaY77PhEm9R0FhFvG-jU1sXGODUkX18dckmD0jFd0aBZtTUrWwpKO6Y1YJE-4suUCW0UiB6ENIVM2ei31DSF3BiFao2Cz8I0DTQLu8-9BNvwAcf2AvuE-H1sxbzTYigxPX8lGgxEVBt2CzksG1JQ8XOz0qnl6KdQtL4Www9%7EmGK5PcXzjLFwYiTQ4bpuA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-17 09:39:17--  https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/0c6124c628f8ecc29be1b6ee0625670062340f5b99cfe543ccf049fa90e6207b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1684574501&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvMGM2MTI0YzYyOGY4ZWNjMjliZTFiNmVlMDYyNTY3MDA2MjM0MGY1Yjk5Y2ZlNTQzY2NmMDQ5ZmE5MGU2MjA3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ1NzQ1MDF9fX1dfQ__&Signature=lljTTKk2RXByetw91sgQdtrkZgh-HJip-kXgSVRA2JC0nht6g3Diu6kDH9arW2Zooy1Xn1thpg6ouMZpwqVCN7HDutHAHoEWr5rBvqv0dVwhc4V3eYvXXIVB211ROCCbYOlD1pDCFQqBh-45tn5fTpb%7EPsaY77PhEm9R0FhFvG-jU1sXGODUkX18dckmD0jFd0aBZtTUrWwpKO6Y1YJE-4suUCW0UiB6ENIVM2ei31DSF3BiFao2Cz8I0DTQLu8-9BNvwAcf2AvuE-H1sxbzTYigxPX8lGgxEVBt2CzksG1JQ8XOz0qnl6KdQtL4Www9%7EmGK5PcXzjLFwYiTQ4bpuA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.156.60.112, 108.156.60.109, 108.156.60.44, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.156.60.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7365670537 (6.9G) [application/octet-stream]\n",
            "Saving to: ‘pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>]   6.86G   248MB/s    in 33s     \n",
            "\n",
            "2023-05-17 09:39:50 (214 MB/s) - ‘pytorch_model.bin’ saved [7365670537/7365670537]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p $cpp_repo/models\n",
        "%cd $cpp_repo/models\n",
        "\n",
        "!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.model\n",
        "!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.vocab\n",
        "!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpz2zzTM5HGc",
        "outputId": "40334157-b9d5-4638-8f61-adc10f31cc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ggml/models\n",
            "--2023-05-17 14:54:53--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.model\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.24.18, 13.35.24.126, 13.35.24.127, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.24.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/7d78ab344146700112cd41628ac7ce54b79c0868fe0c7c201750d8237b54dbb4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27spiece.model%3B+filename%3D%22spiece.model%22%3B&Expires=1684592612&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvN2Q3OGFiMzQ0MTQ2NzAwMTEyY2Q0MTYyOGFjN2NlNTRiNzljMDg2OGZlMGM3YzIwMTc1MGQ4MjM3YjU0ZGJiND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ1OTI2MTJ9fX1dfQ__&Signature=LzIpHSkqB-T4LBKloB8yk-TSh5ThgMJsCk2s8uezecbhQ-G8W0Y7yNg-RUoXTcvE3p-uZK7NgPrwa-FI8MdVX8S3nwBi6Yyzl1nIg7pBNPpY1JODV7A9Pk2QZmztCt4DvdEMEwzM3wTjgKCUmffetliB3reJO9J2P-XjrbMgkC5P0pWhsjelt36OWXqZSH-8X-tOOpjjdIrM4XLe%7EcVzOYJRdRNNTlkicXBZkFKBzNyrvwbX0GwHlvd1ocS5JocG7MBWWXSB%7EjA%7ECFhwvrGXhsLxoN1G32V8Ldp-hP%7E25j4woieooMb9%7EHzZq5quM48syUv4FWIkdhXHRtJ5WHF3Jw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-17 14:55:03--  https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/7d78ab344146700112cd41628ac7ce54b79c0868fe0c7c201750d8237b54dbb4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27spiece.model%3B+filename%3D%22spiece.model%22%3B&Expires=1684592612&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvN2Q3OGFiMzQ0MTQ2NzAwMTEyY2Q0MTYyOGFjN2NlNTRiNzljMDg2OGZlMGM3YzIwMTc1MGQ4MjM3YjU0ZGJiND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ1OTI2MTJ9fX1dfQ__&Signature=LzIpHSkqB-T4LBKloB8yk-TSh5ThgMJsCk2s8uezecbhQ-G8W0Y7yNg-RUoXTcvE3p-uZK7NgPrwa-FI8MdVX8S3nwBi6Yyzl1nIg7pBNPpY1JODV7A9Pk2QZmztCt4DvdEMEwzM3wTjgKCUmffetliB3reJO9J2P-XjrbMgkC5P0pWhsjelt36OWXqZSH-8X-tOOpjjdIrM4XLe%7EcVzOYJRdRNNTlkicXBZkFKBzNyrvwbX0GwHlvd1ocS5JocG7MBWWXSB%7EjA%7ECFhwvrGXhsLxoN1G32V8Ldp-hP%7E25j4woieooMb9%7EHzZq5quM48syUv4FWIkdhXHRtJ5WHF3Jw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.24.128, 13.35.24.38, 13.35.24.76, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.24.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 786216 (768K) [binary/octet-stream]\n",
            "Saving to: ‘spiece.model’\n",
            "\n",
            "spiece.model        100%[===================>] 767.79K   897KB/s    in 0.9s    \n",
            "\n",
            "2023-05-17 14:55:05 (897 KB/s) - ‘spiece.model’ saved [786216/786216]\n",
            "\n",
            "--2023-05-17 14:55:05--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.vocab\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.24.126, 13.35.24.127, 13.35.24.56, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.24.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 574430 (561K) [text/plain]\n",
            "Saving to: ‘spiece.vocab’\n",
            "\n",
            "spiece.vocab        100%[===================>] 560.97K   820KB/s    in 0.7s    \n",
            "\n",
            "2023-05-17 14:55:14 (820 KB/s) - ‘spiece.vocab’ saved [574430/574430]\n",
            "\n",
            "--2023-05-17 14:55:14--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/config.json\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.24.18, 13.35.24.56, 13.35.24.127, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.24.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 534 [text/plain]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "config.json         100%[===================>]     534  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-17 14:55:23 (144 MB/s) - ‘config.json’ saved [534/534]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $cpp_repo\n",
        "!pip install -r requirements.txt\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBZPAu566Xqe",
        "outputId": "43f3f4d7-b780-49a3-bccf-a1195d33c484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "base_model=\"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "for v in model.state_dict().items():\n",
        "  print(v[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "672c4cf8b2f442569edc517ef51fbfbd",
            "f0f78db418e24b3a9cb3221d4939bdc5",
            "28c12cca0a014b9d942285b4d76f10f1",
            "15f5f7449f4245d08e8a270d8c7de891",
            "4feb275ab45c4383bc94f9e3434deaa7",
            "61d7638fccf541109e354e974b2effe9",
            "ed3e3d97999945d6946571f1b256b71d",
            "3345d1fb223c4d74ae68571113fa9cea",
            "24618ecd0ff14381823f62c4a3336c5f",
            "3009e290b85d47fdbe565dd14e01e36b",
            "85530c38e9134f10972a963e2e09c091",
            "fc4e3cc1df0b4375ac3db82af02f0e5b",
            "e9a67f3df53243f887a7ce74eb58c223",
            "41b4d3df97be4641a5284b3007e7b88e",
            "2a0df80edfc04305a19ab114c5d77d63",
            "adf0e353f15045f39601281eab678ec4",
            "1acf1f5660e54e29af708aa2b223256f",
            "4db3b92873d74935adc97dde748da509",
            "1f8bf6bab144451c84f985edacb4208d",
            "1ac01904091c4f539ca45d4a252026f8",
            "09a3aa8128c6488fbef88240fa342b95",
            "d5fddd65782549c4bf2063406842c85f",
            "65748d8bb0c94b2ea2a58eef7eba0288",
            "e34ac4304ffa484a93564506d75f40dc",
            "ed17ecad2f954c44b6fea83c17388289",
            "8d2adaed69ed4891a989540975bf8edb",
            "210c87cef9d74d2797a8ebd1e7857073",
            "60e6a476b12a4ce78b8293bf4cd71523",
            "da2d9b5d0a8a4269908be5beb63fc530",
            "4d5c06003e264669b78e439f7aba39f9",
            "a5b9091f59314cd5a6970ae939fce48a",
            "11c253764f61418f86fc7f448889903e",
            "c74aff2e39dd4b8ab5beb2b928a4c729"
          ]
        },
        "id": "t4CbDQtlwI5M",
        "outputId": "ec2f28a6-46e6-4f29-a9a8-f9fe71e245c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/604 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "672c4cf8b2f442569edc517ef51fbfbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/5.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc4e3cc1df0b4375ac3db82af02f0e5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65748d8bb0c94b2ea2a58eef7eba0288"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt_neox.embed_in.weight\n",
            "gpt_neox.layers.0.input_layernorm.weight\n",
            "gpt_neox.layers.0.input_layernorm.bias\n",
            "gpt_neox.layers.0.post_attention_layernorm.weight\n",
            "gpt_neox.layers.0.post_attention_layernorm.bias\n",
            "gpt_neox.layers.0.attention.bias\n",
            "gpt_neox.layers.0.attention.masked_bias\n",
            "gpt_neox.layers.0.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.0.attention.query_key_value.weight\n",
            "gpt_neox.layers.0.attention.query_key_value.bias\n",
            "gpt_neox.layers.0.attention.dense.weight\n",
            "gpt_neox.layers.0.attention.dense.bias\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.1.input_layernorm.weight\n",
            "gpt_neox.layers.1.input_layernorm.bias\n",
            "gpt_neox.layers.1.post_attention_layernorm.weight\n",
            "gpt_neox.layers.1.post_attention_layernorm.bias\n",
            "gpt_neox.layers.1.attention.bias\n",
            "gpt_neox.layers.1.attention.masked_bias\n",
            "gpt_neox.layers.1.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.1.attention.query_key_value.weight\n",
            "gpt_neox.layers.1.attention.query_key_value.bias\n",
            "gpt_neox.layers.1.attention.dense.weight\n",
            "gpt_neox.layers.1.attention.dense.bias\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.2.input_layernorm.weight\n",
            "gpt_neox.layers.2.input_layernorm.bias\n",
            "gpt_neox.layers.2.post_attention_layernorm.weight\n",
            "gpt_neox.layers.2.post_attention_layernorm.bias\n",
            "gpt_neox.layers.2.attention.bias\n",
            "gpt_neox.layers.2.attention.masked_bias\n",
            "gpt_neox.layers.2.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.2.attention.query_key_value.weight\n",
            "gpt_neox.layers.2.attention.query_key_value.bias\n",
            "gpt_neox.layers.2.attention.dense.weight\n",
            "gpt_neox.layers.2.attention.dense.bias\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.3.input_layernorm.weight\n",
            "gpt_neox.layers.3.input_layernorm.bias\n",
            "gpt_neox.layers.3.post_attention_layernorm.weight\n",
            "gpt_neox.layers.3.post_attention_layernorm.bias\n",
            "gpt_neox.layers.3.attention.bias\n",
            "gpt_neox.layers.3.attention.masked_bias\n",
            "gpt_neox.layers.3.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.3.attention.query_key_value.weight\n",
            "gpt_neox.layers.3.attention.query_key_value.bias\n",
            "gpt_neox.layers.3.attention.dense.weight\n",
            "gpt_neox.layers.3.attention.dense.bias\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.4.input_layernorm.weight\n",
            "gpt_neox.layers.4.input_layernorm.bias\n",
            "gpt_neox.layers.4.post_attention_layernorm.weight\n",
            "gpt_neox.layers.4.post_attention_layernorm.bias\n",
            "gpt_neox.layers.4.attention.bias\n",
            "gpt_neox.layers.4.attention.masked_bias\n",
            "gpt_neox.layers.4.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.4.attention.query_key_value.weight\n",
            "gpt_neox.layers.4.attention.query_key_value.bias\n",
            "gpt_neox.layers.4.attention.dense.weight\n",
            "gpt_neox.layers.4.attention.dense.bias\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.5.input_layernorm.weight\n",
            "gpt_neox.layers.5.input_layernorm.bias\n",
            "gpt_neox.layers.5.post_attention_layernorm.weight\n",
            "gpt_neox.layers.5.post_attention_layernorm.bias\n",
            "gpt_neox.layers.5.attention.bias\n",
            "gpt_neox.layers.5.attention.masked_bias\n",
            "gpt_neox.layers.5.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.5.attention.query_key_value.weight\n",
            "gpt_neox.layers.5.attention.query_key_value.bias\n",
            "gpt_neox.layers.5.attention.dense.weight\n",
            "gpt_neox.layers.5.attention.dense.bias\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.6.input_layernorm.weight\n",
            "gpt_neox.layers.6.input_layernorm.bias\n",
            "gpt_neox.layers.6.post_attention_layernorm.weight\n",
            "gpt_neox.layers.6.post_attention_layernorm.bias\n",
            "gpt_neox.layers.6.attention.bias\n",
            "gpt_neox.layers.6.attention.masked_bias\n",
            "gpt_neox.layers.6.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.6.attention.query_key_value.weight\n",
            "gpt_neox.layers.6.attention.query_key_value.bias\n",
            "gpt_neox.layers.6.attention.dense.weight\n",
            "gpt_neox.layers.6.attention.dense.bias\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.7.input_layernorm.weight\n",
            "gpt_neox.layers.7.input_layernorm.bias\n",
            "gpt_neox.layers.7.post_attention_layernorm.weight\n",
            "gpt_neox.layers.7.post_attention_layernorm.bias\n",
            "gpt_neox.layers.7.attention.bias\n",
            "gpt_neox.layers.7.attention.masked_bias\n",
            "gpt_neox.layers.7.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.7.attention.query_key_value.weight\n",
            "gpt_neox.layers.7.attention.query_key_value.bias\n",
            "gpt_neox.layers.7.attention.dense.weight\n",
            "gpt_neox.layers.7.attention.dense.bias\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.8.input_layernorm.weight\n",
            "gpt_neox.layers.8.input_layernorm.bias\n",
            "gpt_neox.layers.8.post_attention_layernorm.weight\n",
            "gpt_neox.layers.8.post_attention_layernorm.bias\n",
            "gpt_neox.layers.8.attention.bias\n",
            "gpt_neox.layers.8.attention.masked_bias\n",
            "gpt_neox.layers.8.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.8.attention.query_key_value.weight\n",
            "gpt_neox.layers.8.attention.query_key_value.bias\n",
            "gpt_neox.layers.8.attention.dense.weight\n",
            "gpt_neox.layers.8.attention.dense.bias\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.9.input_layernorm.weight\n",
            "gpt_neox.layers.9.input_layernorm.bias\n",
            "gpt_neox.layers.9.post_attention_layernorm.weight\n",
            "gpt_neox.layers.9.post_attention_layernorm.bias\n",
            "gpt_neox.layers.9.attention.bias\n",
            "gpt_neox.layers.9.attention.masked_bias\n",
            "gpt_neox.layers.9.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.9.attention.query_key_value.weight\n",
            "gpt_neox.layers.9.attention.query_key_value.bias\n",
            "gpt_neox.layers.9.attention.dense.weight\n",
            "gpt_neox.layers.9.attention.dense.bias\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.10.input_layernorm.weight\n",
            "gpt_neox.layers.10.input_layernorm.bias\n",
            "gpt_neox.layers.10.post_attention_layernorm.weight\n",
            "gpt_neox.layers.10.post_attention_layernorm.bias\n",
            "gpt_neox.layers.10.attention.bias\n",
            "gpt_neox.layers.10.attention.masked_bias\n",
            "gpt_neox.layers.10.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.10.attention.query_key_value.weight\n",
            "gpt_neox.layers.10.attention.query_key_value.bias\n",
            "gpt_neox.layers.10.attention.dense.weight\n",
            "gpt_neox.layers.10.attention.dense.bias\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.11.input_layernorm.weight\n",
            "gpt_neox.layers.11.input_layernorm.bias\n",
            "gpt_neox.layers.11.post_attention_layernorm.weight\n",
            "gpt_neox.layers.11.post_attention_layernorm.bias\n",
            "gpt_neox.layers.11.attention.bias\n",
            "gpt_neox.layers.11.attention.masked_bias\n",
            "gpt_neox.layers.11.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.11.attention.query_key_value.weight\n",
            "gpt_neox.layers.11.attention.query_key_value.bias\n",
            "gpt_neox.layers.11.attention.dense.weight\n",
            "gpt_neox.layers.11.attention.dense.bias\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.12.input_layernorm.weight\n",
            "gpt_neox.layers.12.input_layernorm.bias\n",
            "gpt_neox.layers.12.post_attention_layernorm.weight\n",
            "gpt_neox.layers.12.post_attention_layernorm.bias\n",
            "gpt_neox.layers.12.attention.bias\n",
            "gpt_neox.layers.12.attention.masked_bias\n",
            "gpt_neox.layers.12.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.12.attention.query_key_value.weight\n",
            "gpt_neox.layers.12.attention.query_key_value.bias\n",
            "gpt_neox.layers.12.attention.dense.weight\n",
            "gpt_neox.layers.12.attention.dense.bias\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.13.input_layernorm.weight\n",
            "gpt_neox.layers.13.input_layernorm.bias\n",
            "gpt_neox.layers.13.post_attention_layernorm.weight\n",
            "gpt_neox.layers.13.post_attention_layernorm.bias\n",
            "gpt_neox.layers.13.attention.bias\n",
            "gpt_neox.layers.13.attention.masked_bias\n",
            "gpt_neox.layers.13.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.13.attention.query_key_value.weight\n",
            "gpt_neox.layers.13.attention.query_key_value.bias\n",
            "gpt_neox.layers.13.attention.dense.weight\n",
            "gpt_neox.layers.13.attention.dense.bias\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.14.input_layernorm.weight\n",
            "gpt_neox.layers.14.input_layernorm.bias\n",
            "gpt_neox.layers.14.post_attention_layernorm.weight\n",
            "gpt_neox.layers.14.post_attention_layernorm.bias\n",
            "gpt_neox.layers.14.attention.bias\n",
            "gpt_neox.layers.14.attention.masked_bias\n",
            "gpt_neox.layers.14.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.14.attention.query_key_value.weight\n",
            "gpt_neox.layers.14.attention.query_key_value.bias\n",
            "gpt_neox.layers.14.attention.dense.weight\n",
            "gpt_neox.layers.14.attention.dense.bias\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.15.input_layernorm.weight\n",
            "gpt_neox.layers.15.input_layernorm.bias\n",
            "gpt_neox.layers.15.post_attention_layernorm.weight\n",
            "gpt_neox.layers.15.post_attention_layernorm.bias\n",
            "gpt_neox.layers.15.attention.bias\n",
            "gpt_neox.layers.15.attention.masked_bias\n",
            "gpt_neox.layers.15.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.15.attention.query_key_value.weight\n",
            "gpt_neox.layers.15.attention.query_key_value.bias\n",
            "gpt_neox.layers.15.attention.dense.weight\n",
            "gpt_neox.layers.15.attention.dense.bias\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.16.input_layernorm.weight\n",
            "gpt_neox.layers.16.input_layernorm.bias\n",
            "gpt_neox.layers.16.post_attention_layernorm.weight\n",
            "gpt_neox.layers.16.post_attention_layernorm.bias\n",
            "gpt_neox.layers.16.attention.bias\n",
            "gpt_neox.layers.16.attention.masked_bias\n",
            "gpt_neox.layers.16.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.16.attention.query_key_value.weight\n",
            "gpt_neox.layers.16.attention.query_key_value.bias\n",
            "gpt_neox.layers.16.attention.dense.weight\n",
            "gpt_neox.layers.16.attention.dense.bias\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.17.input_layernorm.weight\n",
            "gpt_neox.layers.17.input_layernorm.bias\n",
            "gpt_neox.layers.17.post_attention_layernorm.weight\n",
            "gpt_neox.layers.17.post_attention_layernorm.bias\n",
            "gpt_neox.layers.17.attention.bias\n",
            "gpt_neox.layers.17.attention.masked_bias\n",
            "gpt_neox.layers.17.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.17.attention.query_key_value.weight\n",
            "gpt_neox.layers.17.attention.query_key_value.bias\n",
            "gpt_neox.layers.17.attention.dense.weight\n",
            "gpt_neox.layers.17.attention.dense.bias\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.18.input_layernorm.weight\n",
            "gpt_neox.layers.18.input_layernorm.bias\n",
            "gpt_neox.layers.18.post_attention_layernorm.weight\n",
            "gpt_neox.layers.18.post_attention_layernorm.bias\n",
            "gpt_neox.layers.18.attention.bias\n",
            "gpt_neox.layers.18.attention.masked_bias\n",
            "gpt_neox.layers.18.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.18.attention.query_key_value.weight\n",
            "gpt_neox.layers.18.attention.query_key_value.bias\n",
            "gpt_neox.layers.18.attention.dense.weight\n",
            "gpt_neox.layers.18.attention.dense.bias\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.19.input_layernorm.weight\n",
            "gpt_neox.layers.19.input_layernorm.bias\n",
            "gpt_neox.layers.19.post_attention_layernorm.weight\n",
            "gpt_neox.layers.19.post_attention_layernorm.bias\n",
            "gpt_neox.layers.19.attention.bias\n",
            "gpt_neox.layers.19.attention.masked_bias\n",
            "gpt_neox.layers.19.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.19.attention.query_key_value.weight\n",
            "gpt_neox.layers.19.attention.query_key_value.bias\n",
            "gpt_neox.layers.19.attention.dense.weight\n",
            "gpt_neox.layers.19.attention.dense.bias\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.20.input_layernorm.weight\n",
            "gpt_neox.layers.20.input_layernorm.bias\n",
            "gpt_neox.layers.20.post_attention_layernorm.weight\n",
            "gpt_neox.layers.20.post_attention_layernorm.bias\n",
            "gpt_neox.layers.20.attention.bias\n",
            "gpt_neox.layers.20.attention.masked_bias\n",
            "gpt_neox.layers.20.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.20.attention.query_key_value.weight\n",
            "gpt_neox.layers.20.attention.query_key_value.bias\n",
            "gpt_neox.layers.20.attention.dense.weight\n",
            "gpt_neox.layers.20.attention.dense.bias\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.21.input_layernorm.weight\n",
            "gpt_neox.layers.21.input_layernorm.bias\n",
            "gpt_neox.layers.21.post_attention_layernorm.weight\n",
            "gpt_neox.layers.21.post_attention_layernorm.bias\n",
            "gpt_neox.layers.21.attention.bias\n",
            "gpt_neox.layers.21.attention.masked_bias\n",
            "gpt_neox.layers.21.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.21.attention.query_key_value.weight\n",
            "gpt_neox.layers.21.attention.query_key_value.bias\n",
            "gpt_neox.layers.21.attention.dense.weight\n",
            "gpt_neox.layers.21.attention.dense.bias\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.22.input_layernorm.weight\n",
            "gpt_neox.layers.22.input_layernorm.bias\n",
            "gpt_neox.layers.22.post_attention_layernorm.weight\n",
            "gpt_neox.layers.22.post_attention_layernorm.bias\n",
            "gpt_neox.layers.22.attention.bias\n",
            "gpt_neox.layers.22.attention.masked_bias\n",
            "gpt_neox.layers.22.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.22.attention.query_key_value.weight\n",
            "gpt_neox.layers.22.attention.query_key_value.bias\n",
            "gpt_neox.layers.22.attention.dense.weight\n",
            "gpt_neox.layers.22.attention.dense.bias\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.23.input_layernorm.weight\n",
            "gpt_neox.layers.23.input_layernorm.bias\n",
            "gpt_neox.layers.23.post_attention_layernorm.weight\n",
            "gpt_neox.layers.23.post_attention_layernorm.bias\n",
            "gpt_neox.layers.23.attention.bias\n",
            "gpt_neox.layers.23.attention.masked_bias\n",
            "gpt_neox.layers.23.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.23.attention.query_key_value.weight\n",
            "gpt_neox.layers.23.attention.query_key_value.bias\n",
            "gpt_neox.layers.23.attention.dense.weight\n",
            "gpt_neox.layers.23.attention.dense.bias\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.24.input_layernorm.weight\n",
            "gpt_neox.layers.24.input_layernorm.bias\n",
            "gpt_neox.layers.24.post_attention_layernorm.weight\n",
            "gpt_neox.layers.24.post_attention_layernorm.bias\n",
            "gpt_neox.layers.24.attention.bias\n",
            "gpt_neox.layers.24.attention.masked_bias\n",
            "gpt_neox.layers.24.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.24.attention.query_key_value.weight\n",
            "gpt_neox.layers.24.attention.query_key_value.bias\n",
            "gpt_neox.layers.24.attention.dense.weight\n",
            "gpt_neox.layers.24.attention.dense.bias\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.25.input_layernorm.weight\n",
            "gpt_neox.layers.25.input_layernorm.bias\n",
            "gpt_neox.layers.25.post_attention_layernorm.weight\n",
            "gpt_neox.layers.25.post_attention_layernorm.bias\n",
            "gpt_neox.layers.25.attention.bias\n",
            "gpt_neox.layers.25.attention.masked_bias\n",
            "gpt_neox.layers.25.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.25.attention.query_key_value.weight\n",
            "gpt_neox.layers.25.attention.query_key_value.bias\n",
            "gpt_neox.layers.25.attention.dense.weight\n",
            "gpt_neox.layers.25.attention.dense.bias\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.26.input_layernorm.weight\n",
            "gpt_neox.layers.26.input_layernorm.bias\n",
            "gpt_neox.layers.26.post_attention_layernorm.weight\n",
            "gpt_neox.layers.26.post_attention_layernorm.bias\n",
            "gpt_neox.layers.26.attention.bias\n",
            "gpt_neox.layers.26.attention.masked_bias\n",
            "gpt_neox.layers.26.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.26.attention.query_key_value.weight\n",
            "gpt_neox.layers.26.attention.query_key_value.bias\n",
            "gpt_neox.layers.26.attention.dense.weight\n",
            "gpt_neox.layers.26.attention.dense.bias\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.27.input_layernorm.weight\n",
            "gpt_neox.layers.27.input_layernorm.bias\n",
            "gpt_neox.layers.27.post_attention_layernorm.weight\n",
            "gpt_neox.layers.27.post_attention_layernorm.bias\n",
            "gpt_neox.layers.27.attention.bias\n",
            "gpt_neox.layers.27.attention.masked_bias\n",
            "gpt_neox.layers.27.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.27.attention.query_key_value.weight\n",
            "gpt_neox.layers.27.attention.query_key_value.bias\n",
            "gpt_neox.layers.27.attention.dense.weight\n",
            "gpt_neox.layers.27.attention.dense.bias\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.28.input_layernorm.weight\n",
            "gpt_neox.layers.28.input_layernorm.bias\n",
            "gpt_neox.layers.28.post_attention_layernorm.weight\n",
            "gpt_neox.layers.28.post_attention_layernorm.bias\n",
            "gpt_neox.layers.28.attention.bias\n",
            "gpt_neox.layers.28.attention.masked_bias\n",
            "gpt_neox.layers.28.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.28.attention.query_key_value.weight\n",
            "gpt_neox.layers.28.attention.query_key_value.bias\n",
            "gpt_neox.layers.28.attention.dense.weight\n",
            "gpt_neox.layers.28.attention.dense.bias\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.29.input_layernorm.weight\n",
            "gpt_neox.layers.29.input_layernorm.bias\n",
            "gpt_neox.layers.29.post_attention_layernorm.weight\n",
            "gpt_neox.layers.29.post_attention_layernorm.bias\n",
            "gpt_neox.layers.29.attention.bias\n",
            "gpt_neox.layers.29.attention.masked_bias\n",
            "gpt_neox.layers.29.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.29.attention.query_key_value.weight\n",
            "gpt_neox.layers.29.attention.query_key_value.bias\n",
            "gpt_neox.layers.29.attention.dense.weight\n",
            "gpt_neox.layers.29.attention.dense.bias\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.30.input_layernorm.weight\n",
            "gpt_neox.layers.30.input_layernorm.bias\n",
            "gpt_neox.layers.30.post_attention_layernorm.weight\n",
            "gpt_neox.layers.30.post_attention_layernorm.bias\n",
            "gpt_neox.layers.30.attention.bias\n",
            "gpt_neox.layers.30.attention.masked_bias\n",
            "gpt_neox.layers.30.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.30.attention.query_key_value.weight\n",
            "gpt_neox.layers.30.attention.query_key_value.bias\n",
            "gpt_neox.layers.30.attention.dense.weight\n",
            "gpt_neox.layers.30.attention.dense.bias\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.31.input_layernorm.weight\n",
            "gpt_neox.layers.31.input_layernorm.bias\n",
            "gpt_neox.layers.31.post_attention_layernorm.weight\n",
            "gpt_neox.layers.31.post_attention_layernorm.bias\n",
            "gpt_neox.layers.31.attention.bias\n",
            "gpt_neox.layers.31.attention.masked_bias\n",
            "gpt_neox.layers.31.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.31.attention.query_key_value.weight\n",
            "gpt_neox.layers.31.attention.query_key_value.bias\n",
            "gpt_neox.layers.31.attention.dense.weight\n",
            "gpt_neox.layers.31.attention.dense.bias\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.final_layer_norm.weight\n",
            "gpt_neox.final_layer_norm.bias\n",
            "embed_out.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "base_model = 'rinna/japanese-gpt-neox-3.6b-instruction-sft'\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "for v in model.state_dict().items():\n",
        "  print(v[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e1fd6db4a0c245eaa56701c176487f56",
            "4bdadf248d7649cea4886965c1804a37",
            "b2a5032befc549339aec851bcf83ebce",
            "e99536e5f4754d2695c6c06241e7d833",
            "912cad6bb1fb46cd838abaf461a1bc0c",
            "43f76c91684e41c5839ff59ce45b1be9",
            "78270174984243efb67db5bb57d58cd6",
            "ebb7dcaf2ffb49f298525f944dafc127",
            "d2dd3c48788f4d708664d6ee615aa186",
            "5c01ae7dfaa54ace9dec47b4d2ccfdd7",
            "c4d0fcccfea84b7faf15f4e53ab91c37",
            "b1daa1075e08429b8864ea7e94ba94da",
            "a6573a0ec4d3496daa14623628ed37f1",
            "f688f8c26c1e45f89db98a7854ee2bf7",
            "cae0c6cb41294e5babf532805f06694c",
            "03b9683133b745baaa6ca65c4ba23f63",
            "734fde80218f45c59457c779f4592e17",
            "812a1cf090ba4285940cb3010ad6f838",
            "5368175d7b52453bae2e3ec3c47296dd",
            "504fea8dae19402182726b6b39d395ea",
            "6190fcdce0f6453bb399d3da7140bab4",
            "bb50053d3e374641829c22c2dabd8497"
          ]
        },
        "id": "AxWc0JFLGFpV",
        "outputId": "99c8e146-a94d-4f2c-9731-3828827df2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/534 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1fd6db4a0c245eaa56701c176487f56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/7.37G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1daa1075e08429b8864ea7e94ba94da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt_neox.embed_in.weight\n",
            "gpt_neox.layers.0.input_layernorm.weight\n",
            "gpt_neox.layers.0.input_layernorm.bias\n",
            "gpt_neox.layers.0.post_attention_layernorm.weight\n",
            "gpt_neox.layers.0.post_attention_layernorm.bias\n",
            "gpt_neox.layers.0.attention.bias\n",
            "gpt_neox.layers.0.attention.masked_bias\n",
            "gpt_neox.layers.0.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.0.attention.query_key_value.weight\n",
            "gpt_neox.layers.0.attention.query_key_value.bias\n",
            "gpt_neox.layers.0.attention.dense.weight\n",
            "gpt_neox.layers.0.attention.dense.bias\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.1.input_layernorm.weight\n",
            "gpt_neox.layers.1.input_layernorm.bias\n",
            "gpt_neox.layers.1.post_attention_layernorm.weight\n",
            "gpt_neox.layers.1.post_attention_layernorm.bias\n",
            "gpt_neox.layers.1.attention.bias\n",
            "gpt_neox.layers.1.attention.masked_bias\n",
            "gpt_neox.layers.1.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.1.attention.query_key_value.weight\n",
            "gpt_neox.layers.1.attention.query_key_value.bias\n",
            "gpt_neox.layers.1.attention.dense.weight\n",
            "gpt_neox.layers.1.attention.dense.bias\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.2.input_layernorm.weight\n",
            "gpt_neox.layers.2.input_layernorm.bias\n",
            "gpt_neox.layers.2.post_attention_layernorm.weight\n",
            "gpt_neox.layers.2.post_attention_layernorm.bias\n",
            "gpt_neox.layers.2.attention.bias\n",
            "gpt_neox.layers.2.attention.masked_bias\n",
            "gpt_neox.layers.2.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.2.attention.query_key_value.weight\n",
            "gpt_neox.layers.2.attention.query_key_value.bias\n",
            "gpt_neox.layers.2.attention.dense.weight\n",
            "gpt_neox.layers.2.attention.dense.bias\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.3.input_layernorm.weight\n",
            "gpt_neox.layers.3.input_layernorm.bias\n",
            "gpt_neox.layers.3.post_attention_layernorm.weight\n",
            "gpt_neox.layers.3.post_attention_layernorm.bias\n",
            "gpt_neox.layers.3.attention.bias\n",
            "gpt_neox.layers.3.attention.masked_bias\n",
            "gpt_neox.layers.3.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.3.attention.query_key_value.weight\n",
            "gpt_neox.layers.3.attention.query_key_value.bias\n",
            "gpt_neox.layers.3.attention.dense.weight\n",
            "gpt_neox.layers.3.attention.dense.bias\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.4.input_layernorm.weight\n",
            "gpt_neox.layers.4.input_layernorm.bias\n",
            "gpt_neox.layers.4.post_attention_layernorm.weight\n",
            "gpt_neox.layers.4.post_attention_layernorm.bias\n",
            "gpt_neox.layers.4.attention.bias\n",
            "gpt_neox.layers.4.attention.masked_bias\n",
            "gpt_neox.layers.4.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.4.attention.query_key_value.weight\n",
            "gpt_neox.layers.4.attention.query_key_value.bias\n",
            "gpt_neox.layers.4.attention.dense.weight\n",
            "gpt_neox.layers.4.attention.dense.bias\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.5.input_layernorm.weight\n",
            "gpt_neox.layers.5.input_layernorm.bias\n",
            "gpt_neox.layers.5.post_attention_layernorm.weight\n",
            "gpt_neox.layers.5.post_attention_layernorm.bias\n",
            "gpt_neox.layers.5.attention.bias\n",
            "gpt_neox.layers.5.attention.masked_bias\n",
            "gpt_neox.layers.5.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.5.attention.query_key_value.weight\n",
            "gpt_neox.layers.5.attention.query_key_value.bias\n",
            "gpt_neox.layers.5.attention.dense.weight\n",
            "gpt_neox.layers.5.attention.dense.bias\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.6.input_layernorm.weight\n",
            "gpt_neox.layers.6.input_layernorm.bias\n",
            "gpt_neox.layers.6.post_attention_layernorm.weight\n",
            "gpt_neox.layers.6.post_attention_layernorm.bias\n",
            "gpt_neox.layers.6.attention.bias\n",
            "gpt_neox.layers.6.attention.masked_bias\n",
            "gpt_neox.layers.6.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.6.attention.query_key_value.weight\n",
            "gpt_neox.layers.6.attention.query_key_value.bias\n",
            "gpt_neox.layers.6.attention.dense.weight\n",
            "gpt_neox.layers.6.attention.dense.bias\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.7.input_layernorm.weight\n",
            "gpt_neox.layers.7.input_layernorm.bias\n",
            "gpt_neox.layers.7.post_attention_layernorm.weight\n",
            "gpt_neox.layers.7.post_attention_layernorm.bias\n",
            "gpt_neox.layers.7.attention.bias\n",
            "gpt_neox.layers.7.attention.masked_bias\n",
            "gpt_neox.layers.7.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.7.attention.query_key_value.weight\n",
            "gpt_neox.layers.7.attention.query_key_value.bias\n",
            "gpt_neox.layers.7.attention.dense.weight\n",
            "gpt_neox.layers.7.attention.dense.bias\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.8.input_layernorm.weight\n",
            "gpt_neox.layers.8.input_layernorm.bias\n",
            "gpt_neox.layers.8.post_attention_layernorm.weight\n",
            "gpt_neox.layers.8.post_attention_layernorm.bias\n",
            "gpt_neox.layers.8.attention.bias\n",
            "gpt_neox.layers.8.attention.masked_bias\n",
            "gpt_neox.layers.8.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.8.attention.query_key_value.weight\n",
            "gpt_neox.layers.8.attention.query_key_value.bias\n",
            "gpt_neox.layers.8.attention.dense.weight\n",
            "gpt_neox.layers.8.attention.dense.bias\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.9.input_layernorm.weight\n",
            "gpt_neox.layers.9.input_layernorm.bias\n",
            "gpt_neox.layers.9.post_attention_layernorm.weight\n",
            "gpt_neox.layers.9.post_attention_layernorm.bias\n",
            "gpt_neox.layers.9.attention.bias\n",
            "gpt_neox.layers.9.attention.masked_bias\n",
            "gpt_neox.layers.9.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.9.attention.query_key_value.weight\n",
            "gpt_neox.layers.9.attention.query_key_value.bias\n",
            "gpt_neox.layers.9.attention.dense.weight\n",
            "gpt_neox.layers.9.attention.dense.bias\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.10.input_layernorm.weight\n",
            "gpt_neox.layers.10.input_layernorm.bias\n",
            "gpt_neox.layers.10.post_attention_layernorm.weight\n",
            "gpt_neox.layers.10.post_attention_layernorm.bias\n",
            "gpt_neox.layers.10.attention.bias\n",
            "gpt_neox.layers.10.attention.masked_bias\n",
            "gpt_neox.layers.10.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.10.attention.query_key_value.weight\n",
            "gpt_neox.layers.10.attention.query_key_value.bias\n",
            "gpt_neox.layers.10.attention.dense.weight\n",
            "gpt_neox.layers.10.attention.dense.bias\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.11.input_layernorm.weight\n",
            "gpt_neox.layers.11.input_layernorm.bias\n",
            "gpt_neox.layers.11.post_attention_layernorm.weight\n",
            "gpt_neox.layers.11.post_attention_layernorm.bias\n",
            "gpt_neox.layers.11.attention.bias\n",
            "gpt_neox.layers.11.attention.masked_bias\n",
            "gpt_neox.layers.11.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.11.attention.query_key_value.weight\n",
            "gpt_neox.layers.11.attention.query_key_value.bias\n",
            "gpt_neox.layers.11.attention.dense.weight\n",
            "gpt_neox.layers.11.attention.dense.bias\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.12.input_layernorm.weight\n",
            "gpt_neox.layers.12.input_layernorm.bias\n",
            "gpt_neox.layers.12.post_attention_layernorm.weight\n",
            "gpt_neox.layers.12.post_attention_layernorm.bias\n",
            "gpt_neox.layers.12.attention.bias\n",
            "gpt_neox.layers.12.attention.masked_bias\n",
            "gpt_neox.layers.12.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.12.attention.query_key_value.weight\n",
            "gpt_neox.layers.12.attention.query_key_value.bias\n",
            "gpt_neox.layers.12.attention.dense.weight\n",
            "gpt_neox.layers.12.attention.dense.bias\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.13.input_layernorm.weight\n",
            "gpt_neox.layers.13.input_layernorm.bias\n",
            "gpt_neox.layers.13.post_attention_layernorm.weight\n",
            "gpt_neox.layers.13.post_attention_layernorm.bias\n",
            "gpt_neox.layers.13.attention.bias\n",
            "gpt_neox.layers.13.attention.masked_bias\n",
            "gpt_neox.layers.13.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.13.attention.query_key_value.weight\n",
            "gpt_neox.layers.13.attention.query_key_value.bias\n",
            "gpt_neox.layers.13.attention.dense.weight\n",
            "gpt_neox.layers.13.attention.dense.bias\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.14.input_layernorm.weight\n",
            "gpt_neox.layers.14.input_layernorm.bias\n",
            "gpt_neox.layers.14.post_attention_layernorm.weight\n",
            "gpt_neox.layers.14.post_attention_layernorm.bias\n",
            "gpt_neox.layers.14.attention.bias\n",
            "gpt_neox.layers.14.attention.masked_bias\n",
            "gpt_neox.layers.14.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.14.attention.query_key_value.weight\n",
            "gpt_neox.layers.14.attention.query_key_value.bias\n",
            "gpt_neox.layers.14.attention.dense.weight\n",
            "gpt_neox.layers.14.attention.dense.bias\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.15.input_layernorm.weight\n",
            "gpt_neox.layers.15.input_layernorm.bias\n",
            "gpt_neox.layers.15.post_attention_layernorm.weight\n",
            "gpt_neox.layers.15.post_attention_layernorm.bias\n",
            "gpt_neox.layers.15.attention.bias\n",
            "gpt_neox.layers.15.attention.masked_bias\n",
            "gpt_neox.layers.15.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.15.attention.query_key_value.weight\n",
            "gpt_neox.layers.15.attention.query_key_value.bias\n",
            "gpt_neox.layers.15.attention.dense.weight\n",
            "gpt_neox.layers.15.attention.dense.bias\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.16.input_layernorm.weight\n",
            "gpt_neox.layers.16.input_layernorm.bias\n",
            "gpt_neox.layers.16.post_attention_layernorm.weight\n",
            "gpt_neox.layers.16.post_attention_layernorm.bias\n",
            "gpt_neox.layers.16.attention.bias\n",
            "gpt_neox.layers.16.attention.masked_bias\n",
            "gpt_neox.layers.16.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.16.attention.query_key_value.weight\n",
            "gpt_neox.layers.16.attention.query_key_value.bias\n",
            "gpt_neox.layers.16.attention.dense.weight\n",
            "gpt_neox.layers.16.attention.dense.bias\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.17.input_layernorm.weight\n",
            "gpt_neox.layers.17.input_layernorm.bias\n",
            "gpt_neox.layers.17.post_attention_layernorm.weight\n",
            "gpt_neox.layers.17.post_attention_layernorm.bias\n",
            "gpt_neox.layers.17.attention.bias\n",
            "gpt_neox.layers.17.attention.masked_bias\n",
            "gpt_neox.layers.17.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.17.attention.query_key_value.weight\n",
            "gpt_neox.layers.17.attention.query_key_value.bias\n",
            "gpt_neox.layers.17.attention.dense.weight\n",
            "gpt_neox.layers.17.attention.dense.bias\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.18.input_layernorm.weight\n",
            "gpt_neox.layers.18.input_layernorm.bias\n",
            "gpt_neox.layers.18.post_attention_layernorm.weight\n",
            "gpt_neox.layers.18.post_attention_layernorm.bias\n",
            "gpt_neox.layers.18.attention.bias\n",
            "gpt_neox.layers.18.attention.masked_bias\n",
            "gpt_neox.layers.18.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.18.attention.query_key_value.weight\n",
            "gpt_neox.layers.18.attention.query_key_value.bias\n",
            "gpt_neox.layers.18.attention.dense.weight\n",
            "gpt_neox.layers.18.attention.dense.bias\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.19.input_layernorm.weight\n",
            "gpt_neox.layers.19.input_layernorm.bias\n",
            "gpt_neox.layers.19.post_attention_layernorm.weight\n",
            "gpt_neox.layers.19.post_attention_layernorm.bias\n",
            "gpt_neox.layers.19.attention.bias\n",
            "gpt_neox.layers.19.attention.masked_bias\n",
            "gpt_neox.layers.19.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.19.attention.query_key_value.weight\n",
            "gpt_neox.layers.19.attention.query_key_value.bias\n",
            "gpt_neox.layers.19.attention.dense.weight\n",
            "gpt_neox.layers.19.attention.dense.bias\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.20.input_layernorm.weight\n",
            "gpt_neox.layers.20.input_layernorm.bias\n",
            "gpt_neox.layers.20.post_attention_layernorm.weight\n",
            "gpt_neox.layers.20.post_attention_layernorm.bias\n",
            "gpt_neox.layers.20.attention.bias\n",
            "gpt_neox.layers.20.attention.masked_bias\n",
            "gpt_neox.layers.20.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.20.attention.query_key_value.weight\n",
            "gpt_neox.layers.20.attention.query_key_value.bias\n",
            "gpt_neox.layers.20.attention.dense.weight\n",
            "gpt_neox.layers.20.attention.dense.bias\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.21.input_layernorm.weight\n",
            "gpt_neox.layers.21.input_layernorm.bias\n",
            "gpt_neox.layers.21.post_attention_layernorm.weight\n",
            "gpt_neox.layers.21.post_attention_layernorm.bias\n",
            "gpt_neox.layers.21.attention.bias\n",
            "gpt_neox.layers.21.attention.masked_bias\n",
            "gpt_neox.layers.21.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.21.attention.query_key_value.weight\n",
            "gpt_neox.layers.21.attention.query_key_value.bias\n",
            "gpt_neox.layers.21.attention.dense.weight\n",
            "gpt_neox.layers.21.attention.dense.bias\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.22.input_layernorm.weight\n",
            "gpt_neox.layers.22.input_layernorm.bias\n",
            "gpt_neox.layers.22.post_attention_layernorm.weight\n",
            "gpt_neox.layers.22.post_attention_layernorm.bias\n",
            "gpt_neox.layers.22.attention.bias\n",
            "gpt_neox.layers.22.attention.masked_bias\n",
            "gpt_neox.layers.22.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.22.attention.query_key_value.weight\n",
            "gpt_neox.layers.22.attention.query_key_value.bias\n",
            "gpt_neox.layers.22.attention.dense.weight\n",
            "gpt_neox.layers.22.attention.dense.bias\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.23.input_layernorm.weight\n",
            "gpt_neox.layers.23.input_layernorm.bias\n",
            "gpt_neox.layers.23.post_attention_layernorm.weight\n",
            "gpt_neox.layers.23.post_attention_layernorm.bias\n",
            "gpt_neox.layers.23.attention.bias\n",
            "gpt_neox.layers.23.attention.masked_bias\n",
            "gpt_neox.layers.23.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.23.attention.query_key_value.weight\n",
            "gpt_neox.layers.23.attention.query_key_value.bias\n",
            "gpt_neox.layers.23.attention.dense.weight\n",
            "gpt_neox.layers.23.attention.dense.bias\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.24.input_layernorm.weight\n",
            "gpt_neox.layers.24.input_layernorm.bias\n",
            "gpt_neox.layers.24.post_attention_layernorm.weight\n",
            "gpt_neox.layers.24.post_attention_layernorm.bias\n",
            "gpt_neox.layers.24.attention.bias\n",
            "gpt_neox.layers.24.attention.masked_bias\n",
            "gpt_neox.layers.24.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.24.attention.query_key_value.weight\n",
            "gpt_neox.layers.24.attention.query_key_value.bias\n",
            "gpt_neox.layers.24.attention.dense.weight\n",
            "gpt_neox.layers.24.attention.dense.bias\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.25.input_layernorm.weight\n",
            "gpt_neox.layers.25.input_layernorm.bias\n",
            "gpt_neox.layers.25.post_attention_layernorm.weight\n",
            "gpt_neox.layers.25.post_attention_layernorm.bias\n",
            "gpt_neox.layers.25.attention.bias\n",
            "gpt_neox.layers.25.attention.masked_bias\n",
            "gpt_neox.layers.25.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.25.attention.query_key_value.weight\n",
            "gpt_neox.layers.25.attention.query_key_value.bias\n",
            "gpt_neox.layers.25.attention.dense.weight\n",
            "gpt_neox.layers.25.attention.dense.bias\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.26.input_layernorm.weight\n",
            "gpt_neox.layers.26.input_layernorm.bias\n",
            "gpt_neox.layers.26.post_attention_layernorm.weight\n",
            "gpt_neox.layers.26.post_attention_layernorm.bias\n",
            "gpt_neox.layers.26.attention.bias\n",
            "gpt_neox.layers.26.attention.masked_bias\n",
            "gpt_neox.layers.26.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.26.attention.query_key_value.weight\n",
            "gpt_neox.layers.26.attention.query_key_value.bias\n",
            "gpt_neox.layers.26.attention.dense.weight\n",
            "gpt_neox.layers.26.attention.dense.bias\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.27.input_layernorm.weight\n",
            "gpt_neox.layers.27.input_layernorm.bias\n",
            "gpt_neox.layers.27.post_attention_layernorm.weight\n",
            "gpt_neox.layers.27.post_attention_layernorm.bias\n",
            "gpt_neox.layers.27.attention.bias\n",
            "gpt_neox.layers.27.attention.masked_bias\n",
            "gpt_neox.layers.27.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.27.attention.query_key_value.weight\n",
            "gpt_neox.layers.27.attention.query_key_value.bias\n",
            "gpt_neox.layers.27.attention.dense.weight\n",
            "gpt_neox.layers.27.attention.dense.bias\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.28.input_layernorm.weight\n",
            "gpt_neox.layers.28.input_layernorm.bias\n",
            "gpt_neox.layers.28.post_attention_layernorm.weight\n",
            "gpt_neox.layers.28.post_attention_layernorm.bias\n",
            "gpt_neox.layers.28.attention.bias\n",
            "gpt_neox.layers.28.attention.masked_bias\n",
            "gpt_neox.layers.28.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.28.attention.query_key_value.weight\n",
            "gpt_neox.layers.28.attention.query_key_value.bias\n",
            "gpt_neox.layers.28.attention.dense.weight\n",
            "gpt_neox.layers.28.attention.dense.bias\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.29.input_layernorm.weight\n",
            "gpt_neox.layers.29.input_layernorm.bias\n",
            "gpt_neox.layers.29.post_attention_layernorm.weight\n",
            "gpt_neox.layers.29.post_attention_layernorm.bias\n",
            "gpt_neox.layers.29.attention.bias\n",
            "gpt_neox.layers.29.attention.masked_bias\n",
            "gpt_neox.layers.29.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.29.attention.query_key_value.weight\n",
            "gpt_neox.layers.29.attention.query_key_value.bias\n",
            "gpt_neox.layers.29.attention.dense.weight\n",
            "gpt_neox.layers.29.attention.dense.bias\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.30.input_layernorm.weight\n",
            "gpt_neox.layers.30.input_layernorm.bias\n",
            "gpt_neox.layers.30.post_attention_layernorm.weight\n",
            "gpt_neox.layers.30.post_attention_layernorm.bias\n",
            "gpt_neox.layers.30.attention.bias\n",
            "gpt_neox.layers.30.attention.masked_bias\n",
            "gpt_neox.layers.30.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.30.attention.query_key_value.weight\n",
            "gpt_neox.layers.30.attention.query_key_value.bias\n",
            "gpt_neox.layers.30.attention.dense.weight\n",
            "gpt_neox.layers.30.attention.dense.bias\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.31.input_layernorm.weight\n",
            "gpt_neox.layers.31.input_layernorm.bias\n",
            "gpt_neox.layers.31.post_attention_layernorm.weight\n",
            "gpt_neox.layers.31.post_attention_layernorm.bias\n",
            "gpt_neox.layers.31.attention.bias\n",
            "gpt_neox.layers.31.attention.masked_bias\n",
            "gpt_neox.layers.31.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.31.attention.query_key_value.weight\n",
            "gpt_neox.layers.31.attention.query_key_value.bias\n",
            "gpt_neox.layers.31.attention.dense.weight\n",
            "gpt_neox.layers.31.attention.dense.bias\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.32.input_layernorm.weight\n",
            "gpt_neox.layers.32.input_layernorm.bias\n",
            "gpt_neox.layers.32.post_attention_layernorm.weight\n",
            "gpt_neox.layers.32.post_attention_layernorm.bias\n",
            "gpt_neox.layers.32.attention.bias\n",
            "gpt_neox.layers.32.attention.masked_bias\n",
            "gpt_neox.layers.32.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.32.attention.query_key_value.weight\n",
            "gpt_neox.layers.32.attention.query_key_value.bias\n",
            "gpt_neox.layers.32.attention.dense.weight\n",
            "gpt_neox.layers.32.attention.dense.bias\n",
            "gpt_neox.layers.32.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.32.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.32.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.32.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.33.input_layernorm.weight\n",
            "gpt_neox.layers.33.input_layernorm.bias\n",
            "gpt_neox.layers.33.post_attention_layernorm.weight\n",
            "gpt_neox.layers.33.post_attention_layernorm.bias\n",
            "gpt_neox.layers.33.attention.bias\n",
            "gpt_neox.layers.33.attention.masked_bias\n",
            "gpt_neox.layers.33.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.33.attention.query_key_value.weight\n",
            "gpt_neox.layers.33.attention.query_key_value.bias\n",
            "gpt_neox.layers.33.attention.dense.weight\n",
            "gpt_neox.layers.33.attention.dense.bias\n",
            "gpt_neox.layers.33.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.33.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.33.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.33.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.34.input_layernorm.weight\n",
            "gpt_neox.layers.34.input_layernorm.bias\n",
            "gpt_neox.layers.34.post_attention_layernorm.weight\n",
            "gpt_neox.layers.34.post_attention_layernorm.bias\n",
            "gpt_neox.layers.34.attention.bias\n",
            "gpt_neox.layers.34.attention.masked_bias\n",
            "gpt_neox.layers.34.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.34.attention.query_key_value.weight\n",
            "gpt_neox.layers.34.attention.query_key_value.bias\n",
            "gpt_neox.layers.34.attention.dense.weight\n",
            "gpt_neox.layers.34.attention.dense.bias\n",
            "gpt_neox.layers.34.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.34.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.34.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.34.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.35.input_layernorm.weight\n",
            "gpt_neox.layers.35.input_layernorm.bias\n",
            "gpt_neox.layers.35.post_attention_layernorm.weight\n",
            "gpt_neox.layers.35.post_attention_layernorm.bias\n",
            "gpt_neox.layers.35.attention.bias\n",
            "gpt_neox.layers.35.attention.masked_bias\n",
            "gpt_neox.layers.35.attention.rotary_emb.inv_freq\n",
            "gpt_neox.layers.35.attention.query_key_value.weight\n",
            "gpt_neox.layers.35.attention.query_key_value.bias\n",
            "gpt_neox.layers.35.attention.dense.weight\n",
            "gpt_neox.layers.35.attention.dense.bias\n",
            "gpt_neox.layers.35.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.35.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.35.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.35.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.final_layer_norm.weight\n",
            "gpt_neox.final_layer_norm.bias\n",
            "embed_out.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## for gpt-neox\n",
        "## https://github.com/ggerganov/ggml/blob/master/examples/gpt-neox/convert-h5-to-ggml.py\n",
        "\n",
        "%cd $cpp_repo\n",
        "\n",
        "import sys\n",
        "import struct\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, T5Tokenizer\n",
        "\n",
        "base_model = 'rinna/japanese-gpt-neox-3.6b-instruction-sft'\n",
        "fname_out = \"/content/ggml/models/gglm.bin\"\n",
        "model_config = \"/content/ggml/models/config.json\"\n",
        "\n",
        "ftype_str = [\"f32\", \"f16\"]\n",
        "ftype = 1\n",
        "\n",
        "with open(model_config, \"r\", encoding=\"utf-8\") as f:\n",
        "    hparams = json.load(f)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "\n",
        "\n",
        "#print (model)\n",
        "\n",
        "#print(tokenizer.encode('I believe the meaning of life is'))\n",
        "\n",
        "list_vars = model.state_dict()\n",
        "for name in list_vars.keys():\n",
        "    print(name, list_vars[name].shape, list_vars[name].dtype)\n",
        "\n",
        "fout = open(fname_out, \"wb\")\n",
        "\n",
        "print(hparams)\n",
        "\n",
        "fout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\n",
        "fout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\n",
        "fout.write(struct.pack(\"i\", hparams[\"max_position_embeddings\"]))\n",
        "fout.write(struct.pack(\"i\", hparams[\"hidden_size\"]))\n",
        "fout.write(struct.pack(\"i\", hparams[\"num_attention_heads\"]))\n",
        "fout.write(struct.pack(\"i\", hparams[\"num_hidden_layers\"]))\n",
        "fout.write(struct.pack(\"i\", int(hparams[\"rotary_pct\"]*(hparams[\"hidden_size\"]//hparams[\"num_attention_heads\"]))))\n",
        "fout.write(struct.pack(\"i\", hparams[\"use_parallel_residual\"]))\n",
        "fout.write(struct.pack(\"i\", ftype))\n",
        "\n",
        "# TODO: temporary hack to not deal with implementing the tokenizer\n",
        "dot_token = tokenizer.encode('.')[0]\n",
        "for i in range(hparams[\"vocab_size\"]):\n",
        "    text = tokenizer.decode([dot_token, i]).encode('utf-8')\n",
        "    # remove the first byte (it's always '.')\n",
        "    text = text[1:]\n",
        "    fout.write(struct.pack(\"i\", len(text)))\n",
        "    fout.write(text)\n",
        "\n",
        "for name in list_vars.keys():\n",
        "    data = list_vars[name].squeeze().numpy()\n",
        "    print(\"Processing variable: \" + name + \" with shape: \", data.shape)\n",
        "\n",
        "    # we don't need these\n",
        "    #if name.endswith(\".attention.masked_bias\") or     \\\n",
        "    #   name.endswith(\".attention.bias\") or \\\n",
        "    #   name.endswith(\".attention.rotary_emb.inv_freq\"):\n",
        "    #    print(\"  Skipping variable: \" + name)\n",
        "    #    continue\n",
        "\n",
        "    n_dims = len(data.shape);\n",
        "\n",
        "    # ftype == 0 -> float32, ftype == 1 -> float16\n",
        "    ftype_cur = 0;\n",
        "    if ftype != 0:\n",
        "        if name[-7:] == \".weight\" and n_dims == 2:\n",
        "            print(\"  Converting to float16\")\n",
        "            data = data.astype(np.float16)\n",
        "            ftype_cur = 1\n",
        "        else:\n",
        "            print(\"  Converting to float32\")\n",
        "            data = data.astype(np.float32)\n",
        "            ftype_cur = 0\n",
        "    else:\n",
        "        if data.dtype != np.float32:\n",
        "            print(\"  Converting to float32\")\n",
        "            data = data.astype(np.float32)\n",
        "            ftype_cur = 0\n",
        "\n",
        "    # header\n",
        "    str = name.encode('utf-8')\n",
        "    fout.write(struct.pack(\"iii\", n_dims, len(str), ftype_cur))\n",
        "    for i in range(n_dims):\n",
        "        fout.write(struct.pack(\"i\", data.shape[n_dims - 1 - i]))\n",
        "    fout.write(str);\n",
        "\n",
        "    # data\n",
        "    data.tofile(fout)\n",
        "\n",
        "fout.close()\n",
        "\n",
        "print(\"Done. Output file: \" + fname_out)\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "SxVBZMelA4qQ",
        "outputId": "ef4b04fe-2c63-4a4c-8c66-b0886520cb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ggml\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ReadTimeout",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             raise ReadTimeoutError(\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out. (read timeout=%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10.0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-bbee38768c0a>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloading %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Range\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bytes=%d-\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# 3. Exponential backoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     return http_backoff(\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_InvalidHeader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10.0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## redpajama\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WoRhI0zmhDFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/togethercomputer/redpajama.cpp.git\n",
        "%cd /content/redpajama.cpp\n",
        "cpp_repo='/content/redpajama.cpp'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOy2XLBghE7a",
        "outputId": "7d15b129-000e-4bd0-8d31-9ff2e8ce1a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'redpajama.cpp'...\n",
            "remote: Enumerating objects: 2476, done.\u001b[K\n",
            "remote: Counting objects: 100% (1298/1298), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 2476 (delta 1227), reused 1207 (delta 1207), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (2476/2476), 2.20 MiB | 7.54 MiB/s, done.\n",
            "Resolving deltas: 100% (1638/1638), done.\n",
            "/content/redpajama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p $cpp_repo/models\n",
        "%cd $cpp_repo/models\n",
        "\n",
        "!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/config.json\n",
        "#!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/tokenizer_config.json\n",
        "#!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "karA0k-9jqjc",
        "outputId": "a4a926f1-edfe-43b9-9e7e-d33c93abf828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/redpajama.cpp/models\n",
            "--2023-05-18 11:58:50--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/config.json\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.24.56, 13.35.24.127, 13.35.24.126, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.24.56|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 534 [text/plain]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "config.json         100%[===================>]     534  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-18 11:58:50 (104 MB/s) - ‘config.json’ saved [534/534]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p $cpp_repo/models\n",
        "%cd $cpp_repo/models\n",
        "\n",
        "!wget \"https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/pytorch_model.bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7C9Ptdfokh3",
        "outputId": "612f341a-2641-429b-a21a-9eae08aa9151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/redpajama.cpp/models\n",
            "--2023-05-18 11:33:53--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.24.18, 13.35.24.126, 13.35.24.56, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.24.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/0c6124c628f8ecc29be1b6ee0625670062340f5b99cfe543ccf049fa90e6207b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1684665826&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvMGM2MTI0YzYyOGY4ZWNjMjliZTFiNmVlMDYyNTY3MDA2MjM0MGY1Yjk5Y2ZlNTQzY2NmMDQ5ZmE5MGU2MjA3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ2NjU4MjZ9fX1dfQ__&Signature=I5UJFtpvQghcePCJCSAO6qiaynXhu-SN3%7ECUytQ8g-F-TyL1re8y6zmWP6nK3iXn3-%7ERfBD7s3JUvJ3n8ruXpjZ2fpO4Gz0olcmFu1nQhOQdSiQyMiW4azb6m7WmUIWuMaVjthdqUP5Q%7EtEY2wFueHbWCS4MRooukbcwRR9XP5poHYxvuKewdYYeuQdZ9ZyZnWjyd%7Exgrkr06L-O5xmzfjhSHwUVCQ%7EkOvVe2zlMy8LCFWH1SaY9wv8TaKED3w%7EqRlrQOmH%7EbhB4g-4Ukb8ujndS9h1Rl9zeuTf8Ka%7EmCkBV%7E5SLC9jkxCX3mclx8UBWo-2k8ODgxEBzV6%7EPlHlbyQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-18 11:33:53--  https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/0c6124c628f8ecc29be1b6ee0625670062340f5b99cfe543ccf049fa90e6207b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1684665826&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvMGM2MTI0YzYyOGY4ZWNjMjliZTFiNmVlMDYyNTY3MDA2MjM0MGY1Yjk5Y2ZlNTQzY2NmMDQ5ZmE5MGU2MjA3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ2NjU4MjZ9fX1dfQ__&Signature=I5UJFtpvQghcePCJCSAO6qiaynXhu-SN3%7ECUytQ8g-F-TyL1re8y6zmWP6nK3iXn3-%7ERfBD7s3JUvJ3n8ruXpjZ2fpO4Gz0olcmFu1nQhOQdSiQyMiW4azb6m7WmUIWuMaVjthdqUP5Q%7EtEY2wFueHbWCS4MRooukbcwRR9XP5poHYxvuKewdYYeuQdZ9ZyZnWjyd%7Exgrkr06L-O5xmzfjhSHwUVCQ%7EkOvVe2zlMy8LCFWH1SaY9wv8TaKED3w%7EqRlrQOmH%7EbhB4g-4Ukb8ujndS9h1Rl9zeuTf8Ka%7EmCkBV%7E5SLC9jkxCX3mclx8UBWo-2k8ODgxEBzV6%7EPlHlbyQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.7.14, 13.35.7.113, 13.35.7.93, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.7.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7365670537 (6.9G) [application/octet-stream]\n",
            "Saving to: ‘pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>]   6.86G  18.3MB/s    in 5m 44s  \n",
            "\n",
            "2023-05-18 11:39:39 (20.4 MB/s) - ‘pytorch_model.bin’ saved [7365670537/7365670537]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $cpp_repo\n",
        "\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrKHm-8tkAtD",
        "outputId": "b3c6717b-9dcf-4b00-cd2d-80d5c9d18a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/redpajama.cpp\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## https://github.com/togethercomputer/redpajama.cpp/blob/master/examples/redpajama/scripts/convert_gptneox_to_ggml.py\n",
        "\n",
        "#@title convert gptneox to ggml { display-mode: \"form\" }\n",
        "#model_name =  'rinna/japanese-gpt-neox-3.6b-instruction-sft' #@param {type:\"string\"}\n",
        "model_name =  f'{output_path}/hf_ckpt'\n",
        "dir_out = f'{output_path}/ggml'\n",
        "\n",
        "tokenizer_model = 'cyberagent/open-calm-7b'\n",
        "\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "import struct\n",
        "import json\n",
        "import code\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "os.makedirs(dir_out, exist_ok=True)\n",
        "\n",
        "ftype_str = [\"f32\", \"f16\"]\n",
        "ftype = 1\n",
        "\n",
        "model_cache_dir = dir_out + \"-cache\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
        "print(\"Loading model: \", model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16 if ftype == 1 else torch.float32, \n",
        "                                             cache_dir=model_cache_dir)\n",
        "model.eval()\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "hparams = model.config.to_dict()\n",
        "print(\"Model loaded: \", model_name)\n",
        "\n",
        "fn_bin = f\"/ggml-{model_name.split('/')[-1]}-{ftype_str[ftype]}.bin\"\n",
        "fn_out = dir_out + fn_bin\n",
        "fout = open(fn_out, \"wb\")\n",
        "\n",
        "ggml_file_magic = 0x67676d66 # 0x67676d6c is unversioned\n",
        "ggml_file_version = 0x00000001 # v1\n",
        "\n",
        "hparams[\"multiple_of\"] = 1\n",
        "fout.write(struct.pack(\"i\", ggml_file_magic)) # magic: ggmf in hex\n",
        "fout.write(struct.pack(\"i\", ggml_file_version))\n",
        "fout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\n",
        "fout.write(struct.pack(\"i\", hparams[\"max_position_embeddings\"]))\n",
        "fout.write(struct.pack(\"i\", hparams[\"hidden_size\"]))\n",
        "fout.write(struct.pack(\"i\", hparams[\"num_attention_heads\"]))\n",
        "fout.write(struct.pack(\"i\", hparams[\"num_hidden_layers\"]))\n",
        "fout.write(struct.pack(\"i\", int((hparams[\"hidden_size\"] / hparams[\"num_attention_heads\"]\n",
        "                             ) * hparams[\"rotary_pct\"]))) # rotary_dim\n",
        "fout.write(struct.pack(\"i\", int(hparams[\"use_parallel_residual\"])))\n",
        "fout.write(struct.pack(\"i\", ftype))\n",
        "\n",
        "# Is this correct??\n",
        "dot_token = tokenizer.encode(\".\")[0]\n",
        "for i in range(hparams[\"vocab_size\"]):\n",
        "    text = tokenizer.decode([i]).encode('utf-8')\n",
        "    fout.write(struct.pack(\"i\", len(text)))\n",
        "    fout.write(text)\n",
        "\n",
        "list_vars = model.state_dict()\n",
        "\n",
        "print(hparams)\n",
        "\n",
        "for name in list_vars.keys():\n",
        "    if name.startswith('gpt_neox.layers.'):\n",
        "        if 'attention.masked_bias' in name or \\\n",
        "            'attention.rotary_emb.inv_freq' in name or \\\n",
        "            'attention.bias' in name:\n",
        "            continue\n",
        "    # No gradients for these\n",
        "    list_vars[name].requires_grad = False\n",
        "    src = name\n",
        "    nn = name\n",
        "\n",
        "    print(src, ' -> ', name)\n",
        "    data = list_vars[src].squeeze().numpy()\n",
        "    data = data.astype(np.float32)\n",
        "\n",
        "    n_dims = len(data.shape)\n",
        "    print(name, n_dims, data.shape)\n",
        "\n",
        "    # default type is fp32\n",
        "    ftype_cur = 0\n",
        "    if ftype == 1 and n_dims > 1:\n",
        "        print(\"  Converting to float16\", data.shape, data[:3, :3].tolist())\n",
        "        data = data.astype(np.float16)\n",
        "        ftype_cur = 1\n",
        "    else:\n",
        "        print(\"  Converting to float32\", data.shape,\n",
        "              data[:3, :3].tolist() if n_dims > 1 else data[:3].tolist())\n",
        "        data = data.astype(np.float32)\n",
        "\n",
        "    # header\n",
        "    str = name.encode('utf-8')\n",
        "    fout.write(struct.pack(\"iii\", n_dims, len(str), ftype_cur))\n",
        "    for i in range(n_dims):\n",
        "        fout.write(struct.pack(\"i\", data.shape[n_dims - 1 - i]))\n",
        "    print(str)\n",
        "    fout.write(str)\n",
        "\n",
        "    # data\n",
        "    data.tofile(fout)\n",
        "\n",
        "fout.close()\n",
        "\n",
        "print(\"Done. Output file: \" + fn_out)\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "65eb5c3a92d849c1bd4815fcd5bfd44f",
            "cf2ef0b7773945a8a5f0b9c9c58b5088",
            "746eae67445c4fd093f9130e72eba2c2",
            "8280f2da3fbd4e71b27ea703682369e0",
            "490271b3ae674207b2066da00f244645",
            "258a6c67f6a443559b8f6295a1188e87",
            "bb4be8fef0f14433b07a1d8c8936b95f",
            "81b0daa69d14407a8b6640b54068762d",
            "58926460f03047e294352470981c9e9a",
            "7530f4e6b38e4ef6a1366e523cb671f8",
            "b1b7dbf43da1409786daedddb1f492f3"
          ]
        },
        "id": "tPw8VwcJykQz",
        "outputId": "18214059-47ef-4ac4-8789-14a70bf2e5e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model:  /content/drive/MyDrive/models/calm_dolly_7B/hf_ckpt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/45 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65eb5c3a92d849c1bd4815fcd5bfd44f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded:  /content/drive/MyDrive/models/calm_dolly_7B/hf_ckpt\n",
            "{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'float16', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['GPTNeoXForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': None, 'eos_token_id': 0, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '/content/drive/MyDrive/models/calm_dolly_7B/hf_ckpt', 'transformers_version': '4.29.2', 'model_type': 'gpt_neox', 'vocab_size': 52224, 'max_position_embeddings': 2048, 'hidden_size': 4096, 'num_hidden_layers': 32, 'num_attention_heads': 32, 'intermediate_size': 16384, 'hidden_act': 'gelu', 'rotary_pct': 1.0, 'rotary_emb_base': 10000, 'classifier_dropout': 0.1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'use_cache': True, 'use_parallel_residual': False, 'multiple_of': 1}\n",
            "gpt_neox.embed_in.weight  ->  gpt_neox.embed_in.weight\n",
            "gpt_neox.embed_in.weight 2 (52224, 4096)\n",
            "  Converting to float16 (52224, 4096) [[0.0206451416015625, -0.0255584716796875, 0.066162109375], [-0.00727081298828125, 0.004047393798828125, 0.0164031982421875], [-0.11614990234375, -0.10406494140625, 0.013641357421875]]\n",
            "b'gpt_neox.embed_in.weight'\n",
            "gpt_neox.layers.0.input_layernorm.weight  ->  gpt_neox.layers.0.input_layernorm.weight\n",
            "gpt_neox.layers.0.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.09405517578125, 0.07861328125, 0.085205078125]\n",
            "b'gpt_neox.layers.0.input_layernorm.weight'\n",
            "gpt_neox.layers.0.input_layernorm.bias  ->  gpt_neox.layers.0.input_layernorm.bias\n",
            "gpt_neox.layers.0.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.00353240966796875, -0.00035858154296875, 0.0099639892578125]\n",
            "b'gpt_neox.layers.0.input_layernorm.bias'\n",
            "gpt_neox.layers.0.post_attention_layernorm.weight  ->  gpt_neox.layers.0.post_attention_layernorm.weight\n",
            "gpt_neox.layers.0.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.6845703125, 0.76708984375, 0.712890625]\n",
            "b'gpt_neox.layers.0.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.0.post_attention_layernorm.bias  ->  gpt_neox.layers.0.post_attention_layernorm.bias\n",
            "gpt_neox.layers.0.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0167694091796875, 0.06072998046875, -0.030487060546875]\n",
            "b'gpt_neox.layers.0.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.0.attention.query_key_value.weight  ->  gpt_neox.layers.0.attention.query_key_value.weight\n",
            "gpt_neox.layers.0.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.043121337890625, -0.0021915435791015625, 0.040252685546875], [0.07452392578125, -0.058868408203125, -0.032196044921875], [0.02203369140625, -0.07086181640625, -0.06475830078125]]\n",
            "b'gpt_neox.layers.0.attention.query_key_value.weight'\n",
            "gpt_neox.layers.0.attention.query_key_value.bias  ->  gpt_neox.layers.0.attention.query_key_value.bias\n",
            "gpt_neox.layers.0.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.1763916015625, 0.0596923828125, -0.052581787109375]\n",
            "b'gpt_neox.layers.0.attention.query_key_value.bias'\n",
            "gpt_neox.layers.0.attention.dense.weight  ->  gpt_neox.layers.0.attention.dense.weight\n",
            "gpt_neox.layers.0.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.01251220703125, 0.0096893310546875, 0.0056610107421875], [-0.01410675048828125, 0.06640625, -0.02764892578125], [-0.03076171875, -0.037109375, 0.027130126953125]]\n",
            "b'gpt_neox.layers.0.attention.dense.weight'\n",
            "gpt_neox.layers.0.attention.dense.bias  ->  gpt_neox.layers.0.attention.dense.bias\n",
            "gpt_neox.layers.0.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.090087890625, 0.034515380859375, -0.1417236328125]\n",
            "b'gpt_neox.layers.0.attention.dense.bias'\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.0.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.001689910888671875, -0.059051513671875, 0.11932373046875], [0.00991058349609375, 0.06756591796875, 0.0450439453125], [-0.060272216796875, 0.03643798828125, 0.04730224609375]]\n",
            "b'gpt_neox.layers.0.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.0.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.0.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.03973388671875, -0.052978515625, -0.04107666015625]\n",
            "b'gpt_neox.layers.0.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.0.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.04681396484375, 0.039215087890625, -0.0021457672119140625], [-0.052276611328125, 0.0230712890625, -0.045928955078125], [-0.047088623046875, 0.0648193359375, -0.055694580078125]]\n",
            "b'gpt_neox.layers.0.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.0.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.0.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.005176544189453125, -0.04656982421875, -0.049530029296875]\n",
            "b'gpt_neox.layers.0.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.1.input_layernorm.weight  ->  gpt_neox.layers.1.input_layernorm.weight\n",
            "gpt_neox.layers.1.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.50927734375, 0.533203125, 0.5400390625]\n",
            "b'gpt_neox.layers.1.input_layernorm.weight'\n",
            "gpt_neox.layers.1.input_layernorm.bias  ->  gpt_neox.layers.1.input_layernorm.bias\n",
            "gpt_neox.layers.1.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0202178955078125, -0.0005669593811035156, 0.033905029296875]\n",
            "b'gpt_neox.layers.1.input_layernorm.bias'\n",
            "gpt_neox.layers.1.post_attention_layernorm.weight  ->  gpt_neox.layers.1.post_attention_layernorm.weight\n",
            "gpt_neox.layers.1.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.97802734375, 0.994140625, 0.8291015625]\n",
            "b'gpt_neox.layers.1.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.1.post_attention_layernorm.bias  ->  gpt_neox.layers.1.post_attention_layernorm.bias\n",
            "gpt_neox.layers.1.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0374755859375, 0.01242828369140625, -0.0203704833984375]\n",
            "b'gpt_neox.layers.1.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.1.attention.query_key_value.weight  ->  gpt_neox.layers.1.attention.query_key_value.weight\n",
            "gpt_neox.layers.1.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.043853759765625, -0.057037353515625, 0.0350341796875], [-0.034912109375, 0.0850830078125, 0.009246826171875], [0.0119171142578125, 0.01247406005859375, -0.0343017578125]]\n",
            "b'gpt_neox.layers.1.attention.query_key_value.weight'\n",
            "gpt_neox.layers.1.attention.query_key_value.bias  ->  gpt_neox.layers.1.attention.query_key_value.bias\n",
            "gpt_neox.layers.1.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.072998046875, -0.0523681640625, 0.01715087890625]\n",
            "b'gpt_neox.layers.1.attention.query_key_value.bias'\n",
            "gpt_neox.layers.1.attention.dense.weight  ->  gpt_neox.layers.1.attention.dense.weight\n",
            "gpt_neox.layers.1.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.04620361328125, -0.055877685546875, 0.09197998046875], [0.047393798828125, 0.054656982421875, -0.044586181640625], [0.049102783203125, -0.03277587890625, 0.01262664794921875]]\n",
            "b'gpt_neox.layers.1.attention.dense.weight'\n",
            "gpt_neox.layers.1.attention.dense.bias  ->  gpt_neox.layers.1.attention.dense.bias\n",
            "gpt_neox.layers.1.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0007090568542480469, -0.049041748046875, -0.0965576171875]\n",
            "b'gpt_neox.layers.1.attention.dense.bias'\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.1.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.07904052734375, 0.0675048828125, 0.007442474365234375], [0.04583740234375, 0.10577392578125, -0.0767822265625], [-0.048858642578125, -0.0013418197631835938, 0.00847625732421875]]\n",
            "b'gpt_neox.layers.1.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.1.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.1.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.03143310546875, -0.0196685791015625, -0.04730224609375]\n",
            "b'gpt_neox.layers.1.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.1.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.0022830963134765625, -0.0194854736328125, -0.049224853515625], [-0.006702423095703125, 0.044525146484375, 0.012786865234375], [0.0136566162109375, -0.12158203125, 0.0284576416015625]]\n",
            "b'gpt_neox.layers.1.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.1.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.1.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.061309814453125, -0.07733154296875, -0.058624267578125]\n",
            "b'gpt_neox.layers.1.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.2.input_layernorm.weight  ->  gpt_neox.layers.2.input_layernorm.weight\n",
            "gpt_neox.layers.2.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.67041015625, 0.6337890625, 0.59521484375]\n",
            "b'gpt_neox.layers.2.input_layernorm.weight'\n",
            "gpt_neox.layers.2.input_layernorm.bias  ->  gpt_neox.layers.2.input_layernorm.bias\n",
            "gpt_neox.layers.2.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01122283935546875, -0.0030956268310546875, 0.04132080078125]\n",
            "b'gpt_neox.layers.2.input_layernorm.bias'\n",
            "gpt_neox.layers.2.post_attention_layernorm.weight  ->  gpt_neox.layers.2.post_attention_layernorm.weight\n",
            "gpt_neox.layers.2.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [1.109375, 1.0185546875, 0.8935546875]\n",
            "b'gpt_neox.layers.2.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.2.post_attention_layernorm.bias  ->  gpt_neox.layers.2.post_attention_layernorm.bias\n",
            "gpt_neox.layers.2.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.046966552734375, -0.01461029052734375, -0.005039215087890625]\n",
            "b'gpt_neox.layers.2.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.2.attention.query_key_value.weight  ->  gpt_neox.layers.2.attention.query_key_value.weight\n",
            "gpt_neox.layers.2.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.01079559326171875, -0.030548095703125, 0.045379638671875], [0.01271820068359375, 0.0178680419921875, 0.044921875], [-0.0247344970703125, 0.0167694091796875, 0.04119873046875]]\n",
            "b'gpt_neox.layers.2.attention.query_key_value.weight'\n",
            "gpt_neox.layers.2.attention.query_key_value.bias  ->  gpt_neox.layers.2.attention.query_key_value.bias\n",
            "gpt_neox.layers.2.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.007511138916015625, -0.009765625, 0.020263671875]\n",
            "b'gpt_neox.layers.2.attention.query_key_value.bias'\n",
            "gpt_neox.layers.2.attention.dense.weight  ->  gpt_neox.layers.2.attention.dense.weight\n",
            "gpt_neox.layers.2.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.04071044921875, -0.027374267578125, -0.002712249755859375], [-0.06817626953125, -0.0310821533203125, -0.0211639404296875], [-0.0251007080078125, 0.11083984375, -0.00904083251953125]]\n",
            "b'gpt_neox.layers.2.attention.dense.weight'\n",
            "gpt_neox.layers.2.attention.dense.bias  ->  gpt_neox.layers.2.attention.dense.bias\n",
            "gpt_neox.layers.2.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.034271240234375, -0.07281494140625, -0.0791015625]\n",
            "b'gpt_neox.layers.2.attention.dense.bias'\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.2.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.01116180419921875, -0.0233917236328125, 0.1282958984375], [-0.0226593017578125, 0.04315185546875, 0.08026123046875], [-0.004039764404296875, 0.0036106109619140625, -0.0809326171875]]\n",
            "b'gpt_neox.layers.2.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.2.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.2.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.0452880859375, -0.0462646484375, -0.03179931640625]\n",
            "b'gpt_neox.layers.2.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.2.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.006011962890625, 0.0229644775390625, -0.01251983642578125], [-0.049224853515625, 0.0178375244140625, -0.0225677490234375], [-0.037506103515625, 0.10845947265625, -0.0235595703125]]\n",
            "b'gpt_neox.layers.2.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.2.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.2.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0285797119140625, -0.0675048828125, -0.09405517578125]\n",
            "b'gpt_neox.layers.2.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.3.input_layernorm.weight  ->  gpt_neox.layers.3.input_layernorm.weight\n",
            "gpt_neox.layers.3.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.63623046875, 0.57373046875, 0.54248046875]\n",
            "b'gpt_neox.layers.3.input_layernorm.weight'\n",
            "gpt_neox.layers.3.input_layernorm.bias  ->  gpt_neox.layers.3.input_layernorm.bias\n",
            "gpt_neox.layers.3.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01361846923828125, -0.0075225830078125, 0.01763916015625]\n",
            "b'gpt_neox.layers.3.input_layernorm.bias'\n",
            "gpt_neox.layers.3.post_attention_layernorm.weight  ->  gpt_neox.layers.3.post_attention_layernorm.weight\n",
            "gpt_neox.layers.3.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [1.0712890625, 1.0546875, 0.93798828125]\n",
            "b'gpt_neox.layers.3.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.3.post_attention_layernorm.bias  ->  gpt_neox.layers.3.post_attention_layernorm.bias\n",
            "gpt_neox.layers.3.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.020660400390625, -0.028656005859375, -0.033477783203125]\n",
            "b'gpt_neox.layers.3.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.3.attention.query_key_value.weight  ->  gpt_neox.layers.3.attention.query_key_value.weight\n",
            "gpt_neox.layers.3.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.030364990234375, -0.015899658203125, -0.10394287109375], [-0.0065460205078125, -0.2083740234375, 0.0679931640625], [-0.047637939453125, -0.068603515625, -0.02264404296875]]\n",
            "b'gpt_neox.layers.3.attention.query_key_value.weight'\n",
            "gpt_neox.layers.3.attention.query_key_value.bias  ->  gpt_neox.layers.3.attention.query_key_value.bias\n",
            "gpt_neox.layers.3.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.011199951171875, -0.01165008544921875, -0.004322052001953125]\n",
            "b'gpt_neox.layers.3.attention.query_key_value.bias'\n",
            "gpt_neox.layers.3.attention.dense.weight  ->  gpt_neox.layers.3.attention.dense.weight\n",
            "gpt_neox.layers.3.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.025146484375, 0.035919189453125, -0.0041961669921875], [-0.034759521484375, -0.01012420654296875, -0.041046142578125], [-0.001026153564453125, -0.043121337890625, 0.04193115234375]]\n",
            "b'gpt_neox.layers.3.attention.dense.weight'\n",
            "gpt_neox.layers.3.attention.dense.bias  ->  gpt_neox.layers.3.attention.dense.bias\n",
            "gpt_neox.layers.3.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0196990966796875, -0.062042236328125, -0.1083984375]\n",
            "b'gpt_neox.layers.3.attention.dense.bias'\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.3.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.00893402099609375, 0.0650634765625, 0.01151275634765625], [0.033599853515625, 0.046234130859375, 0.03759765625], [-0.052978515625, 0.03643798828125, 0.10076904296875]]\n",
            "b'gpt_neox.layers.3.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.3.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.3.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.0528564453125, -0.03826904296875, -0.042816162109375]\n",
            "b'gpt_neox.layers.3.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.3.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.0251617431640625, 0.0219268798828125, 0.012969970703125], [0.053955078125, -0.015533447265625, 0.023345947265625], [-0.1094970703125, 0.040679931640625, -0.0189666748046875]]\n",
            "b'gpt_neox.layers.3.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.3.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.3.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01131439208984375, -0.05023193359375, -0.098388671875]\n",
            "b'gpt_neox.layers.3.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.4.input_layernorm.weight  ->  gpt_neox.layers.4.input_layernorm.weight\n",
            "gpt_neox.layers.4.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.81298828125, 0.76220703125, 0.6787109375]\n",
            "b'gpt_neox.layers.4.input_layernorm.weight'\n",
            "gpt_neox.layers.4.input_layernorm.bias  ->  gpt_neox.layers.4.input_layernorm.bias\n",
            "gpt_neox.layers.4.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0196533203125, -0.0265960693359375, 0.0165557861328125]\n",
            "b'gpt_neox.layers.4.input_layernorm.bias'\n",
            "gpt_neox.layers.4.post_attention_layernorm.weight  ->  gpt_neox.layers.4.post_attention_layernorm.weight\n",
            "gpt_neox.layers.4.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [1.0048828125, 0.97265625, 0.90966796875]\n",
            "b'gpt_neox.layers.4.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.4.post_attention_layernorm.bias  ->  gpt_neox.layers.4.post_attention_layernorm.bias\n",
            "gpt_neox.layers.4.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01078033447265625, -0.04852294921875, -0.03753662109375]\n",
            "b'gpt_neox.layers.4.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.4.attention.query_key_value.weight  ->  gpt_neox.layers.4.attention.query_key_value.weight\n",
            "gpt_neox.layers.4.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.00750732421875, 0.10211181640625, -0.0175018310546875], [0.0272674560546875, 0.0589599609375, 0.048583984375], [-0.0675048828125, 0.0106048583984375, 0.0261993408203125]]\n",
            "b'gpt_neox.layers.4.attention.query_key_value.weight'\n",
            "gpt_neox.layers.4.attention.query_key_value.bias  ->  gpt_neox.layers.4.attention.query_key_value.bias\n",
            "gpt_neox.layers.4.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.0304107666015625, -0.0196685791015625, 0.0057525634765625]\n",
            "b'gpt_neox.layers.4.attention.query_key_value.bias'\n",
            "gpt_neox.layers.4.attention.dense.weight  ->  gpt_neox.layers.4.attention.dense.weight\n",
            "gpt_neox.layers.4.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.0016698837280273438, 0.0119476318359375, 0.0247802734375], [0.0105743408203125, -0.03887939453125, 0.01824951171875], [0.0004336833953857422, -0.0161590576171875, -0.0208892822265625]]\n",
            "b'gpt_neox.layers.4.attention.dense.weight'\n",
            "gpt_neox.layers.4.attention.dense.bias  ->  gpt_neox.layers.4.attention.dense.bias\n",
            "gpt_neox.layers.4.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0004892349243164062, -0.044036865234375, -0.093017578125]\n",
            "b'gpt_neox.layers.4.attention.dense.bias'\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.4.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.059967041015625, 0.09710693359375, 0.044403076171875], [-0.06878662109375, -0.01708984375, 0.031402587890625], [0.0108489990234375, 0.052459716796875, -0.0142059326171875]]\n",
            "b'gpt_neox.layers.4.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.4.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.4.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.0364990234375, -0.045806884765625, -0.035980224609375]\n",
            "b'gpt_neox.layers.4.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.4.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.0350341796875, -0.08154296875, -0.03521728515625], [0.0003986358642578125, -0.0316162109375, -0.003704071044921875], [0.103759765625, 0.058990478515625, 0.0643310546875]]\n",
            "b'gpt_neox.layers.4.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.4.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.4.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.01288604736328125, -0.028045654296875, -0.0965576171875]\n",
            "b'gpt_neox.layers.4.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.5.input_layernorm.weight  ->  gpt_neox.layers.5.input_layernorm.weight\n",
            "gpt_neox.layers.5.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.783203125, 0.73046875, 0.64111328125]\n",
            "b'gpt_neox.layers.5.input_layernorm.weight'\n",
            "gpt_neox.layers.5.input_layernorm.bias  ->  gpt_neox.layers.5.input_layernorm.bias\n",
            "gpt_neox.layers.5.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.028564453125, -0.016693115234375, 0.006755828857421875]\n",
            "b'gpt_neox.layers.5.input_layernorm.bias'\n",
            "gpt_neox.layers.5.post_attention_layernorm.weight  ->  gpt_neox.layers.5.post_attention_layernorm.weight\n",
            "gpt_neox.layers.5.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.9990234375, 0.9404296875, 0.8828125]\n",
            "b'gpt_neox.layers.5.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.5.post_attention_layernorm.bias  ->  gpt_neox.layers.5.post_attention_layernorm.bias\n",
            "gpt_neox.layers.5.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0384521484375, -0.011566162109375, -0.039215087890625]\n",
            "b'gpt_neox.layers.5.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.5.attention.query_key_value.weight  ->  gpt_neox.layers.5.attention.query_key_value.weight\n",
            "gpt_neox.layers.5.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.04498291015625, 0.0245361328125, 0.01006317138671875], [0.00887298583984375, 0.027435302734375, 0.058990478515625], [-0.04827880859375, 0.058441162109375, -0.01528167724609375]]\n",
            "b'gpt_neox.layers.5.attention.query_key_value.weight'\n",
            "gpt_neox.layers.5.attention.query_key_value.bias  ->  gpt_neox.layers.5.attention.query_key_value.bias\n",
            "gpt_neox.layers.5.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.0105133056640625, -0.023345947265625, -0.0215911865234375]\n",
            "b'gpt_neox.layers.5.attention.query_key_value.bias'\n",
            "gpt_neox.layers.5.attention.dense.weight  ->  gpt_neox.layers.5.attention.dense.weight\n",
            "gpt_neox.layers.5.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.022735595703125, 0.032318115234375, -0.041595458984375], [-0.0096588134765625, -0.012847900390625, 0.0258026123046875], [0.0154571533203125, -0.0009360313415527344, -0.02734375]]\n",
            "b'gpt_neox.layers.5.attention.dense.weight'\n",
            "gpt_neox.layers.5.attention.dense.bias  ->  gpt_neox.layers.5.attention.dense.bias\n",
            "gpt_neox.layers.5.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.010009765625, -0.0262298583984375, -0.0872802734375]\n",
            "b'gpt_neox.layers.5.attention.dense.bias'\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.5.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.0063629150390625, 0.03863525390625, 0.08514404296875], [0.028228759765625, 0.0069580078125, 0.041351318359375], [5.066394805908203e-05, -0.0294647216796875, -0.0592041015625]]\n",
            "b'gpt_neox.layers.5.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.5.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.5.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.03924560546875, -0.030303955078125, -0.042236328125]\n",
            "b'gpt_neox.layers.5.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.5.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.06451416015625, 0.0450439453125, 0.0254669189453125], [-0.01276397705078125, -0.0399169921875, 0.007350921630859375], [0.06170654296875, -0.00844573974609375, -0.01346588134765625]]\n",
            "b'gpt_neox.layers.5.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.5.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.5.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.027923583984375, -0.0179595947265625, -0.0811767578125]\n",
            "b'gpt_neox.layers.5.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.6.input_layernorm.weight  ->  gpt_neox.layers.6.input_layernorm.weight\n",
            "gpt_neox.layers.6.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.75048828125, 0.6787109375, 0.63671875]\n",
            "b'gpt_neox.layers.6.input_layernorm.weight'\n",
            "gpt_neox.layers.6.input_layernorm.bias  ->  gpt_neox.layers.6.input_layernorm.bias\n",
            "gpt_neox.layers.6.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01751708984375, -0.01042938232421875, 0.006557464599609375]\n",
            "b'gpt_neox.layers.6.input_layernorm.bias'\n",
            "gpt_neox.layers.6.post_attention_layernorm.weight  ->  gpt_neox.layers.6.post_attention_layernorm.weight\n",
            "gpt_neox.layers.6.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.970703125, 0.93701171875, 0.853515625]\n",
            "b'gpt_neox.layers.6.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.6.post_attention_layernorm.bias  ->  gpt_neox.layers.6.post_attention_layernorm.bias\n",
            "gpt_neox.layers.6.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.00909423828125, -0.00914764404296875, -0.03369140625]\n",
            "b'gpt_neox.layers.6.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.6.attention.query_key_value.weight  ->  gpt_neox.layers.6.attention.query_key_value.weight\n",
            "gpt_neox.layers.6.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.05169677734375, 0.10711669921875, 0.02642822265625], [-0.01397705078125, -0.0246124267578125, -0.0017690658569335938], [-0.06658935546875, 0.0450439453125, 0.058837890625]]\n",
            "b'gpt_neox.layers.6.attention.query_key_value.weight'\n",
            "gpt_neox.layers.6.attention.query_key_value.bias  ->  gpt_neox.layers.6.attention.query_key_value.bias\n",
            "gpt_neox.layers.6.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.0205078125, -0.0006952285766601562, 0.028961181640625]\n",
            "b'gpt_neox.layers.6.attention.query_key_value.bias'\n",
            "gpt_neox.layers.6.attention.dense.weight  ->  gpt_neox.layers.6.attention.dense.weight\n",
            "gpt_neox.layers.6.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.042449951171875, -0.04095458984375, -0.018768310546875], [-0.01432037353515625, 0.0751953125, 0.0293121337890625], [-0.0120697021484375, -0.005401611328125, -0.034637451171875]]\n",
            "b'gpt_neox.layers.6.attention.dense.weight'\n",
            "gpt_neox.layers.6.attention.dense.bias  ->  gpt_neox.layers.6.attention.dense.bias\n",
            "gpt_neox.layers.6.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0220184326171875, -0.02728271484375, -0.07501220703125]\n",
            "b'gpt_neox.layers.6.attention.dense.bias'\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.6.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.01169586181640625, 0.0181427001953125, 0.04583740234375], [-0.0467529296875, 0.05511474609375, 0.021942138671875], [-0.0335693359375, 0.0134735107421875, 0.0036296844482421875]]\n",
            "b'gpt_neox.layers.6.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.6.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.6.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.03717041015625, -0.038299560546875, -0.0290374755859375]\n",
            "b'gpt_neox.layers.6.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.6.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.0614013671875, -0.11224365234375, -0.07550048828125], [0.062042236328125, -0.044769287109375, -0.0941162109375], [-0.07537841796875, 0.06829833984375, 0.030609130859375]]\n",
            "b'gpt_neox.layers.6.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.6.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.6.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0230865478515625, -0.02740478515625, -0.08203125]\n",
            "b'gpt_neox.layers.6.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.7.input_layernorm.weight  ->  gpt_neox.layers.7.input_layernorm.weight\n",
            "gpt_neox.layers.7.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.7587890625, 0.755859375, 0.70703125]\n",
            "b'gpt_neox.layers.7.input_layernorm.weight'\n",
            "gpt_neox.layers.7.input_layernorm.bias  ->  gpt_neox.layers.7.input_layernorm.bias\n",
            "gpt_neox.layers.7.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0097808837890625, -0.0172576904296875, 0.0024967193603515625]\n",
            "b'gpt_neox.layers.7.input_layernorm.bias'\n",
            "gpt_neox.layers.7.post_attention_layernorm.weight  ->  gpt_neox.layers.7.post_attention_layernorm.weight\n",
            "gpt_neox.layers.7.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.92138671875, 0.91015625, 0.85107421875]\n",
            "b'gpt_neox.layers.7.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.7.post_attention_layernorm.bias  ->  gpt_neox.layers.7.post_attention_layernorm.bias\n",
            "gpt_neox.layers.7.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0126190185546875, -0.01568603515625, -0.05340576171875]\n",
            "b'gpt_neox.layers.7.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.7.attention.query_key_value.weight  ->  gpt_neox.layers.7.attention.query_key_value.weight\n",
            "gpt_neox.layers.7.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.060699462890625, -0.0038623809814453125, 0.054229736328125], [-0.028350830078125, -0.0074615478515625, -0.015869140625], [0.029937744140625, -0.041107177734375, -0.062744140625]]\n",
            "b'gpt_neox.layers.7.attention.query_key_value.weight'\n",
            "gpt_neox.layers.7.attention.query_key_value.bias  ->  gpt_neox.layers.7.attention.query_key_value.bias\n",
            "gpt_neox.layers.7.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.0037841796875, 0.01090240478515625, 0.0142669677734375]\n",
            "b'gpt_neox.layers.7.attention.query_key_value.bias'\n",
            "gpt_neox.layers.7.attention.dense.weight  ->  gpt_neox.layers.7.attention.dense.weight\n",
            "gpt_neox.layers.7.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.061920166015625, -0.0256500244140625, 0.059844970703125], [-0.026336669921875, 0.062744140625, 0.01702880859375], [0.03338623046875, 0.019012451171875, -0.095947265625]]\n",
            "b'gpt_neox.layers.7.attention.dense.weight'\n",
            "gpt_neox.layers.7.attention.dense.bias  ->  gpt_neox.layers.7.attention.dense.bias\n",
            "gpt_neox.layers.7.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.01206207275390625, -0.02069091796875, -0.07196044921875]\n",
            "b'gpt_neox.layers.7.attention.dense.bias'\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.7.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.02655029296875, -0.0625, 0.03485107421875], [-0.0131072998046875, -0.087646484375, 0.06427001953125], [-0.07012939453125, -0.049224853515625, 0.079833984375]]\n",
            "b'gpt_neox.layers.7.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.7.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.7.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.037322998046875, -0.042755126953125, -0.0570068359375]\n",
            "b'gpt_neox.layers.7.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.7.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.10150146484375, 0.057647705078125, 0.0543212890625], [0.05316162109375, 0.08416748046875, 0.0341796875], [-0.016845703125, -0.0197296142578125, 0.0341796875]]\n",
            "b'gpt_neox.layers.7.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.7.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.7.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.007442474365234375, -0.0173797607421875, -0.059234619140625]\n",
            "b'gpt_neox.layers.7.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.8.input_layernorm.weight  ->  gpt_neox.layers.8.input_layernorm.weight\n",
            "gpt_neox.layers.8.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.76025390625, 0.7255859375, 0.71923828125]\n",
            "b'gpt_neox.layers.8.input_layernorm.weight'\n",
            "gpt_neox.layers.8.input_layernorm.bias  ->  gpt_neox.layers.8.input_layernorm.bias\n",
            "gpt_neox.layers.8.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.00975799560546875, -0.01123046875, 0.005023956298828125]\n",
            "b'gpt_neox.layers.8.input_layernorm.bias'\n",
            "gpt_neox.layers.8.post_attention_layernorm.weight  ->  gpt_neox.layers.8.post_attention_layernorm.weight\n",
            "gpt_neox.layers.8.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.9375, 0.91162109375, 0.84619140625]\n",
            "b'gpt_neox.layers.8.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.8.post_attention_layernorm.bias  ->  gpt_neox.layers.8.post_attention_layernorm.bias\n",
            "gpt_neox.layers.8.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0007753372192382812, -0.006908416748046875, -0.03314208984375]\n",
            "b'gpt_neox.layers.8.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.8.attention.query_key_value.weight  ->  gpt_neox.layers.8.attention.query_key_value.weight\n",
            "gpt_neox.layers.8.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.08819580078125, -0.017547607421875, -0.017059326171875], [0.02252197265625, -0.0002942085266113281, -0.0831298828125], [0.05975341796875, -0.0294647216796875, 0.032135009765625]]\n",
            "b'gpt_neox.layers.8.attention.query_key_value.weight'\n",
            "gpt_neox.layers.8.attention.query_key_value.bias  ->  gpt_neox.layers.8.attention.query_key_value.bias\n",
            "gpt_neox.layers.8.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.0367431640625, 0.01910400390625, 0.03875732421875]\n",
            "b'gpt_neox.layers.8.attention.query_key_value.bias'\n",
            "gpt_neox.layers.8.attention.dense.weight  ->  gpt_neox.layers.8.attention.dense.weight\n",
            "gpt_neox.layers.8.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.01120758056640625, 0.07940673828125, 0.028228759765625], [0.0289459228515625, 0.03436279296875, 0.0027904510498046875], [-0.00804901123046875, -0.0269927978515625, 0.02862548828125]]\n",
            "b'gpt_neox.layers.8.attention.dense.weight'\n",
            "gpt_neox.layers.8.attention.dense.bias  ->  gpt_neox.layers.8.attention.dense.bias\n",
            "gpt_neox.layers.8.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.003093719482421875, -0.03021240234375, -0.054718017578125]\n",
            "b'gpt_neox.layers.8.attention.dense.bias'\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.8.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.016845703125, -0.015533447265625, 0.012664794921875], [-0.0767822265625, -0.10101318359375, -0.07232666015625], [0.0287628173828125, 0.045074462890625, -0.034210205078125]]\n",
            "b'gpt_neox.layers.8.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.8.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.8.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.046661376953125, -0.04547119140625, -0.04901123046875]\n",
            "b'gpt_neox.layers.8.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.8.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.032806396484375, 0.0167083740234375, 0.024688720703125], [-0.049224853515625, -0.01016998291015625, 0.038604736328125], [0.09619140625, 0.050994873046875, -0.03131103515625]]\n",
            "b'gpt_neox.layers.8.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.8.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.8.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0016450881958007812, -0.034149169921875, -0.0491943359375]\n",
            "b'gpt_neox.layers.8.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.9.input_layernorm.weight  ->  gpt_neox.layers.9.input_layernorm.weight\n",
            "gpt_neox.layers.9.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.73876953125, 0.7470703125, 0.67626953125]\n",
            "b'gpt_neox.layers.9.input_layernorm.weight'\n",
            "gpt_neox.layers.9.input_layernorm.bias  ->  gpt_neox.layers.9.input_layernorm.bias\n",
            "gpt_neox.layers.9.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0159149169921875, -0.015625, 0.019378662109375]\n",
            "b'gpt_neox.layers.9.input_layernorm.bias'\n",
            "gpt_neox.layers.9.post_attention_layernorm.weight  ->  gpt_neox.layers.9.post_attention_layernorm.weight\n",
            "gpt_neox.layers.9.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.93798828125, 0.9150390625, 0.80712890625]\n",
            "b'gpt_neox.layers.9.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.9.post_attention_layernorm.bias  ->  gpt_neox.layers.9.post_attention_layernorm.bias\n",
            "gpt_neox.layers.9.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.004810333251953125, -0.016387939453125, -0.00012004375457763672]\n",
            "b'gpt_neox.layers.9.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.9.attention.query_key_value.weight  ->  gpt_neox.layers.9.attention.query_key_value.weight\n",
            "gpt_neox.layers.9.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.114013671875, 0.0194091796875, 0.0011892318725585938], [0.0004608631134033203, 0.0010509490966796875, 0.025726318359375], [0.03375244140625, -0.044097900390625, 0.0338134765625]]\n",
            "b'gpt_neox.layers.9.attention.query_key_value.weight'\n",
            "gpt_neox.layers.9.attention.query_key_value.bias  ->  gpt_neox.layers.9.attention.query_key_value.bias\n",
            "gpt_neox.layers.9.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.00875091552734375, -0.0034999847412109375, -0.0175323486328125]\n",
            "b'gpt_neox.layers.9.attention.query_key_value.bias'\n",
            "gpt_neox.layers.9.attention.dense.weight  ->  gpt_neox.layers.9.attention.dense.weight\n",
            "gpt_neox.layers.9.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.045013427734375, 0.00453948974609375, -0.0023651123046875], [0.00032591819763183594, 0.0136566162109375, -0.06988525390625], [0.117919921875, 0.005504608154296875, 0.0799560546875]]\n",
            "b'gpt_neox.layers.9.attention.dense.weight'\n",
            "gpt_neox.layers.9.attention.dense.bias  ->  gpt_neox.layers.9.attention.dense.bias\n",
            "gpt_neox.layers.9.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0026607513427734375, -0.0286407470703125, -0.058624267578125]\n",
            "b'gpt_neox.layers.9.attention.dense.bias'\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.9.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.03985595703125, 0.0196533203125, 0.00653839111328125], [-0.09234619140625, -0.0325927734375, 0.038787841796875], [0.005924224853515625, 0.09210205078125, 0.059783935546875]]\n",
            "b'gpt_neox.layers.9.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.9.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.9.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.033355712890625, -0.034393310546875, -0.0343017578125]\n",
            "b'gpt_neox.layers.9.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.9.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.0014047622680664062, 0.0034008026123046875, -0.0164642333984375], [0.016448974609375, -0.0279388427734375, -0.0233306884765625], [0.047515869140625, -0.06536865234375, 0.020599365234375]]\n",
            "b'gpt_neox.layers.9.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.9.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.9.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0037021636962890625, -0.0259246826171875, -0.05816650390625]\n",
            "b'gpt_neox.layers.9.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.10.input_layernorm.weight  ->  gpt_neox.layers.10.input_layernorm.weight\n",
            "gpt_neox.layers.10.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.8076171875, 0.8212890625, 0.78564453125]\n",
            "b'gpt_neox.layers.10.input_layernorm.weight'\n",
            "gpt_neox.layers.10.input_layernorm.bias  ->  gpt_neox.layers.10.input_layernorm.bias\n",
            "gpt_neox.layers.10.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01383209228515625, -0.0187530517578125, 0.00702667236328125]\n",
            "b'gpt_neox.layers.10.input_layernorm.bias'\n",
            "gpt_neox.layers.10.post_attention_layernorm.weight  ->  gpt_neox.layers.10.post_attention_layernorm.weight\n",
            "gpt_neox.layers.10.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.900390625, 0.90771484375, 0.8251953125]\n",
            "b'gpt_neox.layers.10.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.10.post_attention_layernorm.bias  ->  gpt_neox.layers.10.post_attention_layernorm.bias\n",
            "gpt_neox.layers.10.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0083160400390625, -0.0150299072265625, -0.02093505859375]\n",
            "b'gpt_neox.layers.10.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.10.attention.query_key_value.weight  ->  gpt_neox.layers.10.attention.query_key_value.weight\n",
            "gpt_neox.layers.10.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.0064544677734375, -0.0149993896484375, -0.03759765625], [-0.03033447265625, -0.029754638671875, 0.059844970703125], [0.006252288818359375, 0.0186004638671875, 0.0080413818359375]]\n",
            "b'gpt_neox.layers.10.attention.query_key_value.weight'\n",
            "gpt_neox.layers.10.attention.query_key_value.bias  ->  gpt_neox.layers.10.attention.query_key_value.bias\n",
            "gpt_neox.layers.10.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.0075836181640625, -0.0098114013671875, -0.048858642578125]\n",
            "b'gpt_neox.layers.10.attention.query_key_value.bias'\n",
            "gpt_neox.layers.10.attention.dense.weight  ->  gpt_neox.layers.10.attention.dense.weight\n",
            "gpt_neox.layers.10.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.0008687973022460938, 0.01641845703125, -0.0252685546875], [-0.0011768341064453125, 0.040496826171875, -0.018585205078125], [0.03619384765625, -0.0295562744140625, -0.053466796875]]\n",
            "b'gpt_neox.layers.10.attention.dense.weight'\n",
            "gpt_neox.layers.10.attention.dense.bias  ->  gpt_neox.layers.10.attention.dense.bias\n",
            "gpt_neox.layers.10.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0013303756713867188, -0.01561737060546875, -0.058013916015625]\n",
            "b'gpt_neox.layers.10.attention.dense.bias'\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.10.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.03436279296875, 0.039703369140625, -0.066162109375], [0.01325225830078125, -0.00247955322265625, -0.03692626953125], [-0.039459228515625, 0.069091796875, -0.022979736328125]]\n",
            "b'gpt_neox.layers.10.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.10.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.10.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.03326416015625, -0.0430908203125, -0.0362548828125]\n",
            "b'gpt_neox.layers.10.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.10.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.01837158203125, 0.033294677734375, -0.0234375], [0.0004229545593261719, -0.00397491455078125, -0.05487060546875], [0.061614990234375, 0.033966064453125, 0.024810791015625]]\n",
            "b'gpt_neox.layers.10.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.10.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.10.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0017557144165039062, -0.01557159423828125, -0.054351806640625]\n",
            "b'gpt_neox.layers.10.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.11.input_layernorm.weight  ->  gpt_neox.layers.11.input_layernorm.weight\n",
            "gpt_neox.layers.11.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.806640625, 0.763671875, 0.75634765625]\n",
            "b'gpt_neox.layers.11.input_layernorm.weight'\n",
            "gpt_neox.layers.11.input_layernorm.bias  ->  gpt_neox.layers.11.input_layernorm.bias\n",
            "gpt_neox.layers.11.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0210723876953125, -0.0217132568359375, 0.02020263671875]\n",
            "b'gpt_neox.layers.11.input_layernorm.bias'\n",
            "gpt_neox.layers.11.post_attention_layernorm.weight  ->  gpt_neox.layers.11.post_attention_layernorm.weight\n",
            "gpt_neox.layers.11.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.8994140625, 0.90673828125, 0.8125]\n",
            "b'gpt_neox.layers.11.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.11.post_attention_layernorm.bias  ->  gpt_neox.layers.11.post_attention_layernorm.bias\n",
            "gpt_neox.layers.11.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.028472900390625, -0.0166473388671875, 0.00025534629821777344]\n",
            "b'gpt_neox.layers.11.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.11.attention.query_key_value.weight  ->  gpt_neox.layers.11.attention.query_key_value.weight\n",
            "gpt_neox.layers.11.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.003826141357421875, -0.0269622802734375, 0.04022216796875], [-0.043792724609375, -0.0220184326171875, -0.008087158203125], [-0.019317626953125, -0.0238037109375, -0.0185089111328125]]\n",
            "b'gpt_neox.layers.11.attention.query_key_value.weight'\n",
            "gpt_neox.layers.11.attention.query_key_value.bias  ->  gpt_neox.layers.11.attention.query_key_value.bias\n",
            "gpt_neox.layers.11.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.0214385986328125, -0.01471710205078125, 0.02001953125]\n",
            "b'gpt_neox.layers.11.attention.query_key_value.bias'\n",
            "gpt_neox.layers.11.attention.dense.weight  ->  gpt_neox.layers.11.attention.dense.weight\n",
            "gpt_neox.layers.11.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.010498046875, -0.0179290771484375, 0.0052032470703125], [-0.04925537109375, 0.00891876220703125, -0.046630859375], [-0.0081024169921875, 0.045074462890625, 0.0251007080078125]]\n",
            "b'gpt_neox.layers.11.attention.dense.weight'\n",
            "gpt_neox.layers.11.attention.dense.bias  ->  gpt_neox.layers.11.attention.dense.bias\n",
            "gpt_neox.layers.11.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.00884246826171875, -0.0097503662109375, -0.04803466796875]\n",
            "b'gpt_neox.layers.11.attention.dense.bias'\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.11.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.018310546875, 0.04052734375, -0.00043892860412597656], [-0.0169830322265625, 0.010162353515625, -0.0288238525390625], [0.0089874267578125, 0.0007028579711914062, -0.011077880859375]]\n",
            "b'gpt_neox.layers.11.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.11.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.11.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.0330810546875, -0.034149169921875, -0.0419921875]\n",
            "b'gpt_neox.layers.11.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.11.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.01416015625, 0.016510009765625, -0.0328369140625], [0.095947265625, 0.0309600830078125, 0.05633544921875], [0.0063018798828125, -0.02313232421875, 0.0011205673217773438]]\n",
            "b'gpt_neox.layers.11.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.11.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.11.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.003696441650390625, -0.01299285888671875, -0.05108642578125]\n",
            "b'gpt_neox.layers.11.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.12.input_layernorm.weight  ->  gpt_neox.layers.12.input_layernorm.weight\n",
            "gpt_neox.layers.12.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.79248046875, 0.7744140625, 0.736328125]\n",
            "b'gpt_neox.layers.12.input_layernorm.weight'\n",
            "gpt_neox.layers.12.input_layernorm.bias  ->  gpt_neox.layers.12.input_layernorm.bias\n",
            "gpt_neox.layers.12.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0231170654296875, -0.0161590576171875, 0.0168609619140625]\n",
            "b'gpt_neox.layers.12.input_layernorm.bias'\n",
            "gpt_neox.layers.12.post_attention_layernorm.weight  ->  gpt_neox.layers.12.post_attention_layernorm.weight\n",
            "gpt_neox.layers.12.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.90673828125, 0.89697265625, 0.8408203125]\n",
            "b'gpt_neox.layers.12.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.12.post_attention_layernorm.bias  ->  gpt_neox.layers.12.post_attention_layernorm.bias\n",
            "gpt_neox.layers.12.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0283660888671875, -0.0036411285400390625, 0.0015535354614257812]\n",
            "b'gpt_neox.layers.12.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.12.attention.query_key_value.weight  ->  gpt_neox.layers.12.attention.query_key_value.weight\n",
            "gpt_neox.layers.12.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.0487060546875, -0.00232696533203125, 0.040435791015625], [-0.009490966796875, -0.09307861328125, 0.06787109375], [0.038482666015625, 0.00395965576171875, -0.0306396484375]]\n",
            "b'gpt_neox.layers.12.attention.query_key_value.weight'\n",
            "gpt_neox.layers.12.attention.query_key_value.bias  ->  gpt_neox.layers.12.attention.query_key_value.bias\n",
            "gpt_neox.layers.12.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.02496337890625, 0.0005269050598144531, 0.0162353515625]\n",
            "b'gpt_neox.layers.12.attention.query_key_value.bias'\n",
            "gpt_neox.layers.12.attention.dense.weight  ->  gpt_neox.layers.12.attention.dense.weight\n",
            "gpt_neox.layers.12.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.03131103515625, -0.0260162353515625, -0.034332275390625], [0.006381988525390625, -0.135498046875, 0.11810302734375], [0.055938720703125, -0.007251739501953125, 0.0003142356872558594]]\n",
            "b'gpt_neox.layers.12.attention.dense.weight'\n",
            "gpt_neox.layers.12.attention.dense.bias  ->  gpt_neox.layers.12.attention.dense.bias\n",
            "gpt_neox.layers.12.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0002777576446533203, -0.0123138427734375, -0.038055419921875]\n",
            "b'gpt_neox.layers.12.attention.dense.bias'\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.12.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.00786590576171875, 0.02227783203125, 0.0202484130859375], [-0.0101776123046875, 0.0560302734375, 0.070556640625], [0.018524169921875, -0.0223846435546875, -0.030975341796875]]\n",
            "b'gpt_neox.layers.12.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.12.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.12.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.049102783203125, -0.0191802978515625, -0.0237274169921875]\n",
            "b'gpt_neox.layers.12.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.12.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.019500732421875, -0.034912109375, 0.0002598762512207031], [-0.005130767822265625, -0.0455322265625, 0.07550048828125], [-0.0643310546875, -0.046966552734375, 0.01751708984375]]\n",
            "b'gpt_neox.layers.12.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.12.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.12.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0048065185546875, -0.01099395751953125, -0.0279083251953125]\n",
            "b'gpt_neox.layers.12.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.13.input_layernorm.weight  ->  gpt_neox.layers.13.input_layernorm.weight\n",
            "gpt_neox.layers.13.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.80615234375, 0.80419921875, 0.75244140625]\n",
            "b'gpt_neox.layers.13.input_layernorm.weight'\n",
            "gpt_neox.layers.13.input_layernorm.bias  ->  gpt_neox.layers.13.input_layernorm.bias\n",
            "gpt_neox.layers.13.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0149078369140625, -0.016876220703125, 0.0201873779296875]\n",
            "b'gpt_neox.layers.13.input_layernorm.bias'\n",
            "gpt_neox.layers.13.post_attention_layernorm.weight  ->  gpt_neox.layers.13.post_attention_layernorm.weight\n",
            "gpt_neox.layers.13.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.9130859375, 0.92578125, 0.85107421875]\n",
            "b'gpt_neox.layers.13.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.13.post_attention_layernorm.bias  ->  gpt_neox.layers.13.post_attention_layernorm.bias\n",
            "gpt_neox.layers.13.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.006954193115234375, -0.0020351409912109375, -0.003330230712890625]\n",
            "b'gpt_neox.layers.13.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.13.attention.query_key_value.weight  ->  gpt_neox.layers.13.attention.query_key_value.weight\n",
            "gpt_neox.layers.13.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.0007948875427246094, -0.026641845703125, 0.042236328125], [0.031463623046875, -0.0196075439453125, 0.0906982421875], [-0.01107025146484375, -0.09173583984375, -0.041900634765625]]\n",
            "b'gpt_neox.layers.13.attention.query_key_value.weight'\n",
            "gpt_neox.layers.13.attention.query_key_value.bias  ->  gpt_neox.layers.13.attention.query_key_value.bias\n",
            "gpt_neox.layers.13.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.044830322265625, 0.020599365234375, -0.0100860595703125]\n",
            "b'gpt_neox.layers.13.attention.query_key_value.bias'\n",
            "gpt_neox.layers.13.attention.dense.weight  ->  gpt_neox.layers.13.attention.dense.weight\n",
            "gpt_neox.layers.13.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.007114410400390625, -0.01416778564453125, 0.0236968994140625], [-0.0071258544921875, -0.029449462890625, -0.0013332366943359375], [0.016571044921875, -0.04644775390625, -0.0209197998046875]]\n",
            "b'gpt_neox.layers.13.attention.dense.weight'\n",
            "gpt_neox.layers.13.attention.dense.bias  ->  gpt_neox.layers.13.attention.dense.bias\n",
            "gpt_neox.layers.13.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.004871368408203125, -0.00603485107421875, -0.0168304443359375]\n",
            "b'gpt_neox.layers.13.attention.dense.bias'\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.13.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.076416015625, 0.13916015625, -0.0501708984375], [-0.000888824462890625, -0.03802490234375, 0.03564453125], [0.12188720703125, -0.10174560546875, -0.0275115966796875]]\n",
            "b'gpt_neox.layers.13.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.13.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.13.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.05126953125, -0.04840087890625, -0.0477294921875]\n",
            "b'gpt_neox.layers.13.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.13.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.056060791015625, 0.021881103515625, -0.0186767578125], [-0.032989501953125, 0.069091796875, -0.02569580078125], [0.0614013671875, 0.00933074951171875, -0.004241943359375]]\n",
            "b'gpt_neox.layers.13.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.13.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.13.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0088043212890625, -0.0058746337890625, -0.01268768310546875]\n",
            "b'gpt_neox.layers.13.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.14.input_layernorm.weight  ->  gpt_neox.layers.14.input_layernorm.weight\n",
            "gpt_neox.layers.14.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.77978515625, 0.7587890625, 0.7568359375]\n",
            "b'gpt_neox.layers.14.input_layernorm.weight'\n",
            "gpt_neox.layers.14.input_layernorm.bias  ->  gpt_neox.layers.14.input_layernorm.bias\n",
            "gpt_neox.layers.14.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01514434814453125, -0.0114898681640625, 0.0216217041015625]\n",
            "b'gpt_neox.layers.14.input_layernorm.bias'\n",
            "gpt_neox.layers.14.post_attention_layernorm.weight  ->  gpt_neox.layers.14.post_attention_layernorm.weight\n",
            "gpt_neox.layers.14.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.92919921875, 0.9287109375, 0.8447265625]\n",
            "b'gpt_neox.layers.14.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.14.post_attention_layernorm.bias  ->  gpt_neox.layers.14.post_attention_layernorm.bias\n",
            "gpt_neox.layers.14.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.00861358642578125, 0.00545501708984375, 0.0068511962890625]\n",
            "b'gpt_neox.layers.14.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.14.attention.query_key_value.weight  ->  gpt_neox.layers.14.attention.query_key_value.weight\n",
            "gpt_neox.layers.14.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.110595703125, 0.0221710205078125, -0.016998291015625], [-0.058837890625, 0.01424407958984375, -0.037261962890625], [-0.0582275390625, -0.00885009765625, -0.073974609375]]\n",
            "b'gpt_neox.layers.14.attention.query_key_value.weight'\n",
            "gpt_neox.layers.14.attention.query_key_value.bias  ->  gpt_neox.layers.14.attention.query_key_value.bias\n",
            "gpt_neox.layers.14.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.003551483154296875, -0.00688934326171875, -0.002834320068359375]\n",
            "b'gpt_neox.layers.14.attention.query_key_value.bias'\n",
            "gpt_neox.layers.14.attention.dense.weight  ->  gpt_neox.layers.14.attention.dense.weight\n",
            "gpt_neox.layers.14.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.0419921875, 0.0579833984375, -0.050445556640625], [-0.0124664306640625, 0.03668212890625, -0.0150604248046875], [0.108154296875, -0.0360107421875, -0.07513427734375]]\n",
            "b'gpt_neox.layers.14.attention.dense.weight'\n",
            "gpt_neox.layers.14.attention.dense.bias  ->  gpt_neox.layers.14.attention.dense.bias\n",
            "gpt_neox.layers.14.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0024547576904296875, -0.0056915283203125, -0.0085296630859375]\n",
            "b'gpt_neox.layers.14.attention.dense.bias'\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.14.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.0491943359375, -0.04302978515625, 0.0850830078125], [-0.030303955078125, -0.0209808349609375, 0.03570556640625], [0.0015249252319335938, 0.052154541015625, -0.023712158203125]]\n",
            "b'gpt_neox.layers.14.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.14.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.14.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.0308380126953125, -0.031005859375, -0.039825439453125]\n",
            "b'gpt_neox.layers.14.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.14.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.047576904296875, 0.0736083984375, -0.06927490234375], [0.0033512115478515625, 0.01511383056640625, -0.031494140625], [-0.01219940185546875, -0.0195770263671875, 0.049835205078125]]\n",
            "b'gpt_neox.layers.14.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.14.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.14.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.009307861328125, -0.01039886474609375, -0.018402099609375]\n",
            "b'gpt_neox.layers.14.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.15.input_layernorm.weight  ->  gpt_neox.layers.15.input_layernorm.weight\n",
            "gpt_neox.layers.15.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.78173828125, 0.7744140625, 0.72998046875]\n",
            "b'gpt_neox.layers.15.input_layernorm.weight'\n",
            "gpt_neox.layers.15.input_layernorm.bias  ->  gpt_neox.layers.15.input_layernorm.bias\n",
            "gpt_neox.layers.15.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01617431640625, -0.00855255126953125, 0.0181884765625]\n",
            "b'gpt_neox.layers.15.input_layernorm.bias'\n",
            "gpt_neox.layers.15.post_attention_layernorm.weight  ->  gpt_neox.layers.15.post_attention_layernorm.weight\n",
            "gpt_neox.layers.15.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.9501953125, 0.94580078125, 0.8759765625]\n",
            "b'gpt_neox.layers.15.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.15.post_attention_layernorm.bias  ->  gpt_neox.layers.15.post_attention_layernorm.bias\n",
            "gpt_neox.layers.15.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0144805908203125, 0.0031948089599609375, 0.00478363037109375]\n",
            "b'gpt_neox.layers.15.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.15.attention.query_key_value.weight  ->  gpt_neox.layers.15.attention.query_key_value.weight\n",
            "gpt_neox.layers.15.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.05731201171875, 0.0262908935546875, 0.032196044921875], [0.0245361328125, 0.015869140625, -0.07220458984375], [-0.06658935546875, -0.050537109375, -0.0352783203125]]\n",
            "b'gpt_neox.layers.15.attention.query_key_value.weight'\n",
            "gpt_neox.layers.15.attention.query_key_value.bias  ->  gpt_neox.layers.15.attention.query_key_value.bias\n",
            "gpt_neox.layers.15.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.0213623046875, 0.01317596435546875, -0.014190673828125]\n",
            "b'gpt_neox.layers.15.attention.query_key_value.bias'\n",
            "gpt_neox.layers.15.attention.dense.weight  ->  gpt_neox.layers.15.attention.dense.weight\n",
            "gpt_neox.layers.15.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.038787841796875, -0.08135986328125, 0.00812530517578125], [0.020904541015625, -0.0323486328125, 0.00519561767578125], [0.00395965576171875, 0.0276031494140625, 0.020965576171875]]\n",
            "b'gpt_neox.layers.15.attention.dense.weight'\n",
            "gpt_neox.layers.15.attention.dense.bias  ->  gpt_neox.layers.15.attention.dense.bias\n",
            "gpt_neox.layers.15.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0010967254638671875, -0.0157012939453125, -0.0202484130859375]\n",
            "b'gpt_neox.layers.15.attention.dense.bias'\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.15.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.017608642578125, -0.099853515625, 0.1309814453125], [-0.036895751953125, -0.065185546875, 0.0251312255859375], [-0.005462646484375, -0.061676025390625, -0.089599609375]]\n",
            "b'gpt_neox.layers.15.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.15.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.15.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.033294677734375, -0.046783447265625, -0.052337646484375]\n",
            "b'gpt_neox.layers.15.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.15.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.014892578125, -0.0516357421875, -0.0209503173828125], [-0.00518798828125, 0.0161895751953125, 0.145751953125], [-0.10638427734375, -0.05975341796875, -0.0513916015625]]\n",
            "b'gpt_neox.layers.15.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.15.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.15.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0015382766723632812, -0.01236724853515625, -0.0174560546875]\n",
            "b'gpt_neox.layers.15.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.16.input_layernorm.weight  ->  gpt_neox.layers.16.input_layernorm.weight\n",
            "gpt_neox.layers.16.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.8544921875, 0.82568359375, 0.7548828125]\n",
            "b'gpt_neox.layers.16.input_layernorm.weight'\n",
            "gpt_neox.layers.16.input_layernorm.bias  ->  gpt_neox.layers.16.input_layernorm.bias\n",
            "gpt_neox.layers.16.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0163421630859375, -0.0170135498046875, 0.0216827392578125]\n",
            "b'gpt_neox.layers.16.input_layernorm.bias'\n",
            "gpt_neox.layers.16.post_attention_layernorm.weight  ->  gpt_neox.layers.16.post_attention_layernorm.weight\n",
            "gpt_neox.layers.16.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.97998046875, 1.0166015625, 0.9306640625]\n",
            "b'gpt_neox.layers.16.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.16.post_attention_layernorm.bias  ->  gpt_neox.layers.16.post_attention_layernorm.bias\n",
            "gpt_neox.layers.16.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01279449462890625, -0.01264190673828125, -0.0031585693359375]\n",
            "b'gpt_neox.layers.16.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.16.attention.query_key_value.weight  ->  gpt_neox.layers.16.attention.query_key_value.weight\n",
            "gpt_neox.layers.16.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.007549285888671875, -0.00283050537109375, -0.0036334991455078125], [0.00370025634765625, -0.0281982421875, -0.036529541015625], [-0.033905029296875, 0.0142059326171875, -0.08770751953125]]\n",
            "b'gpt_neox.layers.16.attention.query_key_value.weight'\n",
            "gpt_neox.layers.16.attention.query_key_value.bias  ->  gpt_neox.layers.16.attention.query_key_value.bias\n",
            "gpt_neox.layers.16.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.001972198486328125, -0.0019893646240234375, 0.002590179443359375]\n",
            "b'gpt_neox.layers.16.attention.query_key_value.bias'\n",
            "gpt_neox.layers.16.attention.dense.weight  ->  gpt_neox.layers.16.attention.dense.weight\n",
            "gpt_neox.layers.16.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.05218505859375, -0.08807373046875, -0.07000732421875], [0.01227569580078125, -0.06549072265625, 0.037841796875], [-0.0255126953125, 0.010406494140625, -0.034423828125]]\n",
            "b'gpt_neox.layers.16.attention.dense.weight'\n",
            "gpt_neox.layers.16.attention.dense.bias  ->  gpt_neox.layers.16.attention.dense.bias\n",
            "gpt_neox.layers.16.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.003528594970703125, -0.01238250732421875, -0.01464080810546875]\n",
            "b'gpt_neox.layers.16.attention.dense.bias'\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.16.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.014892578125, 0.1248779296875, 0.032379150390625], [-0.0303955078125, -0.0060577392578125, -0.01103973388671875], [-0.045318603515625, 0.061767578125, -0.00939178466796875]]\n",
            "b'gpt_neox.layers.16.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.16.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.16.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.056915283203125, -0.042144775390625, -0.05133056640625]\n",
            "b'gpt_neox.layers.16.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.16.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.004650115966796875, 0.0841064453125, -0.032073974609375], [0.0288543701171875, -0.0300750732421875, -0.0638427734375], [-0.0712890625, 0.01030731201171875, -0.00988006591796875]]\n",
            "b'gpt_neox.layers.16.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.16.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.16.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.003536224365234375, -0.00800323486328125, -0.013427734375]\n",
            "b'gpt_neox.layers.16.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.17.input_layernorm.weight  ->  gpt_neox.layers.17.input_layernorm.weight\n",
            "gpt_neox.layers.17.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.84130859375, 0.833984375, 0.787109375]\n",
            "b'gpt_neox.layers.17.input_layernorm.weight'\n",
            "gpt_neox.layers.17.input_layernorm.bias  ->  gpt_neox.layers.17.input_layernorm.bias\n",
            "gpt_neox.layers.17.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.022430419921875, -0.0197296142578125, 0.0180816650390625]\n",
            "b'gpt_neox.layers.17.input_layernorm.bias'\n",
            "gpt_neox.layers.17.post_attention_layernorm.weight  ->  gpt_neox.layers.17.post_attention_layernorm.weight\n",
            "gpt_neox.layers.17.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.9736328125, 1.0048828125, 0.94775390625]\n",
            "b'gpt_neox.layers.17.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.17.post_attention_layernorm.bias  ->  gpt_neox.layers.17.post_attention_layernorm.bias\n",
            "gpt_neox.layers.17.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.032135009765625, -0.0106964111328125, -0.0038547515869140625]\n",
            "b'gpt_neox.layers.17.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.17.attention.query_key_value.weight  ->  gpt_neox.layers.17.attention.query_key_value.weight\n",
            "gpt_neox.layers.17.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.0029659271240234375, -0.0765380859375, 0.04290771484375], [-0.042694091796875, -0.0292816162109375, -0.040008544921875], [0.024749755859375, -0.0220489501953125, -0.031829833984375]]\n",
            "b'gpt_neox.layers.17.attention.query_key_value.weight'\n",
            "gpt_neox.layers.17.attention.query_key_value.bias  ->  gpt_neox.layers.17.attention.query_key_value.bias\n",
            "gpt_neox.layers.17.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.040191650390625, 0.0161285400390625, 0.01061248779296875]\n",
            "b'gpt_neox.layers.17.attention.query_key_value.bias'\n",
            "gpt_neox.layers.17.attention.dense.weight  ->  gpt_neox.layers.17.attention.dense.weight\n",
            "gpt_neox.layers.17.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.07171630859375, -0.054107666015625, 0.0298004150390625], [-0.032196044921875, 0.00664520263671875, 0.059112548828125], [0.0080718994140625, -0.0665283203125, -0.00719451904296875]]\n",
            "b'gpt_neox.layers.17.attention.dense.weight'\n",
            "gpt_neox.layers.17.attention.dense.bias  ->  gpt_neox.layers.17.attention.dense.bias\n",
            "gpt_neox.layers.17.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.00659942626953125, -0.01137542724609375, -0.00574493408203125]\n",
            "b'gpt_neox.layers.17.attention.dense.bias'\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.17.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.06805419921875, 0.026123046875, 3.349781036376953e-05], [-0.0335693359375, 0.02239990234375, 0.006404876708984375], [-0.0006518363952636719, 0.014190673828125, -0.05120849609375]]\n",
            "b'gpt_neox.layers.17.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.17.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.17.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.01068878173828125, -0.03741455078125, -0.0411376953125]\n",
            "b'gpt_neox.layers.17.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.17.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.06219482421875, 0.016265869140625, 0.0650634765625], [-0.0010290145874023438, 0.0187530517578125, -0.042755126953125], [0.06805419921875, 0.01318359375, -0.0185089111328125]]\n",
            "b'gpt_neox.layers.17.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.17.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.17.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0038967132568359375, -0.0093231201171875, 0.0010919570922851562]\n",
            "b'gpt_neox.layers.17.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.18.input_layernorm.weight  ->  gpt_neox.layers.18.input_layernorm.weight\n",
            "gpt_neox.layers.18.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.8125, 0.8271484375, 0.7734375]\n",
            "b'gpt_neox.layers.18.input_layernorm.weight'\n",
            "gpt_neox.layers.18.input_layernorm.bias  ->  gpt_neox.layers.18.input_layernorm.bias\n",
            "gpt_neox.layers.18.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.02783203125, -0.014801025390625, 0.023590087890625]\n",
            "b'gpt_neox.layers.18.input_layernorm.bias'\n",
            "gpt_neox.layers.18.post_attention_layernorm.weight  ->  gpt_neox.layers.18.post_attention_layernorm.weight\n",
            "gpt_neox.layers.18.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.99609375, 1.025390625, 0.923828125]\n",
            "b'gpt_neox.layers.18.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.18.post_attention_layernorm.bias  ->  gpt_neox.layers.18.post_attention_layernorm.bias\n",
            "gpt_neox.layers.18.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.035064697265625, -0.006221771240234375, 0.00273895263671875]\n",
            "b'gpt_neox.layers.18.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.18.attention.query_key_value.weight  ->  gpt_neox.layers.18.attention.query_key_value.weight\n",
            "gpt_neox.layers.18.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.0294647216796875, 0.06195068359375, 0.041778564453125], [0.07403564453125, 0.0362548828125, -0.036102294921875], [-0.0215301513671875, 0.010040283203125, -0.0156402587890625]]\n",
            "b'gpt_neox.layers.18.attention.query_key_value.weight'\n",
            "gpt_neox.layers.18.attention.query_key_value.bias  ->  gpt_neox.layers.18.attention.query_key_value.bias\n",
            "gpt_neox.layers.18.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.0055999755859375, 0.0089874267578125, -0.006023406982421875]\n",
            "b'gpt_neox.layers.18.attention.query_key_value.bias'\n",
            "gpt_neox.layers.18.attention.dense.weight  ->  gpt_neox.layers.18.attention.dense.weight\n",
            "gpt_neox.layers.18.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.0184783935546875, 0.08056640625, 0.048980712890625], [0.0028629302978515625, -0.053192138671875, -0.01401519775390625], [-0.05938720703125, 0.038818359375, -0.0675048828125]]\n",
            "b'gpt_neox.layers.18.attention.dense.weight'\n",
            "gpt_neox.layers.18.attention.dense.bias  ->  gpt_neox.layers.18.attention.dense.bias\n",
            "gpt_neox.layers.18.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.002368927001953125, -0.00884246826171875, -0.0009136199951171875]\n",
            "b'gpt_neox.layers.18.attention.dense.bias'\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.18.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.05194091796875, 0.03436279296875, -0.035400390625], [-0.02154541015625, -0.005405426025390625, 0.010711669921875], [0.1195068359375, 0.01459503173828125, -0.06964111328125]]\n",
            "b'gpt_neox.layers.18.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.18.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.18.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.02911376953125, -0.04119873046875, -0.036041259765625]\n",
            "b'gpt_neox.layers.18.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.18.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.052032470703125, 0.036590576171875, -0.010345458984375], [-0.01197052001953125, -0.00867462158203125, -0.013336181640625], [0.0022373199462890625, -0.06591796875, -0.047760009765625]]\n",
            "b'gpt_neox.layers.18.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.18.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.18.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0108184814453125, -0.00994110107421875, -0.0017080307006835938]\n",
            "b'gpt_neox.layers.18.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.19.input_layernorm.weight  ->  gpt_neox.layers.19.input_layernorm.weight\n",
            "gpt_neox.layers.19.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.826171875, 0.849609375, 0.80615234375]\n",
            "b'gpt_neox.layers.19.input_layernorm.weight'\n",
            "gpt_neox.layers.19.input_layernorm.bias  ->  gpt_neox.layers.19.input_layernorm.bias\n",
            "gpt_neox.layers.19.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0274810791015625, -0.0188446044921875, 0.019195556640625]\n",
            "b'gpt_neox.layers.19.input_layernorm.bias'\n",
            "gpt_neox.layers.19.post_attention_layernorm.weight  ->  gpt_neox.layers.19.post_attention_layernorm.weight\n",
            "gpt_neox.layers.19.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.98681640625, 0.99462890625, 0.9169921875]\n",
            "b'gpt_neox.layers.19.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.19.post_attention_layernorm.bias  ->  gpt_neox.layers.19.post_attention_layernorm.bias\n",
            "gpt_neox.layers.19.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.037017822265625, 0.004177093505859375, -0.01165008544921875]\n",
            "b'gpt_neox.layers.19.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.19.attention.query_key_value.weight  ->  gpt_neox.layers.19.attention.query_key_value.weight\n",
            "gpt_neox.layers.19.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.07513427734375, 0.0311737060546875, -0.040435791015625], [-0.016998291015625, 0.09588623046875, -0.07708740234375], [0.02569580078125, -0.0784912109375, -0.015289306640625]]\n",
            "b'gpt_neox.layers.19.attention.query_key_value.weight'\n",
            "gpt_neox.layers.19.attention.query_key_value.bias  ->  gpt_neox.layers.19.attention.query_key_value.bias\n",
            "gpt_neox.layers.19.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.00907135009765625, 0.022064208984375, 0.0184326171875]\n",
            "b'gpt_neox.layers.19.attention.query_key_value.bias'\n",
            "gpt_neox.layers.19.attention.dense.weight  ->  gpt_neox.layers.19.attention.dense.weight\n",
            "gpt_neox.layers.19.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.00954437255859375, -0.04022216796875, 0.044769287109375], [-0.0101318359375, -0.00879669189453125, 0.02606201171875], [-0.05096435546875, -0.0811767578125, -0.06866455078125]]\n",
            "b'gpt_neox.layers.19.attention.dense.weight'\n",
            "gpt_neox.layers.19.attention.dense.bias  ->  gpt_neox.layers.19.attention.dense.bias\n",
            "gpt_neox.layers.19.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.01020050048828125, -0.01044464111328125, -0.0067901611328125]\n",
            "b'gpt_neox.layers.19.attention.dense.bias'\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.19.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.09722900390625, 0.055023193359375, -0.026702880859375], [-0.02301025390625, -0.00321197509765625, 0.02569580078125], [0.06414794921875, -0.048126220703125, -0.0178070068359375]]\n",
            "b'gpt_neox.layers.19.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.19.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.19.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.048065185546875, -0.0408935546875, -0.03363037109375]\n",
            "b'gpt_neox.layers.19.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.19.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.016754150390625, -0.0234527587890625, 0.024169921875], [-0.06884765625, -0.03167724609375, -0.053741455078125], [0.0115814208984375, -0.0025119781494140625, 0.0577392578125]]\n",
            "b'gpt_neox.layers.19.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.19.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.19.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.011505126953125, -0.00788116455078125, 0.00661468505859375]\n",
            "b'gpt_neox.layers.19.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.20.input_layernorm.weight  ->  gpt_neox.layers.20.input_layernorm.weight\n",
            "gpt_neox.layers.20.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.87939453125, 0.86279296875, 0.81884765625]\n",
            "b'gpt_neox.layers.20.input_layernorm.weight'\n",
            "gpt_neox.layers.20.input_layernorm.bias  ->  gpt_neox.layers.20.input_layernorm.bias\n",
            "gpt_neox.layers.20.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0309295654296875, -0.0157470703125, 0.0157623291015625]\n",
            "b'gpt_neox.layers.20.input_layernorm.bias'\n",
            "gpt_neox.layers.20.post_attention_layernorm.weight  ->  gpt_neox.layers.20.post_attention_layernorm.weight\n",
            "gpt_neox.layers.20.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [1.001953125, 1.01171875, 0.935546875]\n",
            "b'gpt_neox.layers.20.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.20.post_attention_layernorm.bias  ->  gpt_neox.layers.20.post_attention_layernorm.bias\n",
            "gpt_neox.layers.20.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.03411865234375, 0.00925445556640625, -0.0205841064453125]\n",
            "b'gpt_neox.layers.20.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.20.attention.query_key_value.weight  ->  gpt_neox.layers.20.attention.query_key_value.weight\n",
            "gpt_neox.layers.20.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.029205322265625, -0.0204620361328125, -0.03753662109375], [-0.059539794921875, 0.018951416015625, 0.01175689697265625], [-0.053070068359375, 0.0025920867919921875, -0.053619384765625]]\n",
            "b'gpt_neox.layers.20.attention.query_key_value.weight'\n",
            "gpt_neox.layers.20.attention.query_key_value.bias  ->  gpt_neox.layers.20.attention.query_key_value.bias\n",
            "gpt_neox.layers.20.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.01104736328125, 0.000667572021484375, -0.021331787109375]\n",
            "b'gpt_neox.layers.20.attention.query_key_value.bias'\n",
            "gpt_neox.layers.20.attention.dense.weight  ->  gpt_neox.layers.20.attention.dense.weight\n",
            "gpt_neox.layers.20.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.0166015625, 0.052825927734375, 0.0611572265625], [0.10186767578125, 0.02349853515625, -0.049774169921875], [-0.0105438232421875, -0.041107177734375, -0.0214691162109375]]\n",
            "b'gpt_neox.layers.20.attention.dense.weight'\n",
            "gpt_neox.layers.20.attention.dense.bias  ->  gpt_neox.layers.20.attention.dense.bias\n",
            "gpt_neox.layers.20.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0147247314453125, -0.0087890625, -0.0009045600891113281]\n",
            "b'gpt_neox.layers.20.attention.dense.bias'\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.20.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.02813720703125, 0.046783447265625, -0.03009033203125], [0.0718994140625, -0.0221710205078125, -0.034698486328125], [-0.06854248046875, -0.04052734375, -0.025604248046875]]\n",
            "b'gpt_neox.layers.20.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.20.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.20.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.04913330078125, -0.0440673828125, -0.04864501953125]\n",
            "b'gpt_neox.layers.20.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.20.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.03778076171875, -0.018463134765625, -0.0751953125], [0.000377655029296875, -0.075439453125, -0.018890380859375], [-0.051055908203125, 0.041168212890625, 0.06793212890625]]\n",
            "b'gpt_neox.layers.20.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.20.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.20.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0165863037109375, -0.008392333984375, 0.006450653076171875]\n",
            "b'gpt_neox.layers.20.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.21.input_layernorm.weight  ->  gpt_neox.layers.21.input_layernorm.weight\n",
            "gpt_neox.layers.21.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.86328125, 0.84912109375, 0.83740234375]\n",
            "b'gpt_neox.layers.21.input_layernorm.weight'\n",
            "gpt_neox.layers.21.input_layernorm.bias  ->  gpt_neox.layers.21.input_layernorm.bias\n",
            "gpt_neox.layers.21.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0268707275390625, -0.00925445556640625, 0.02252197265625]\n",
            "b'gpt_neox.layers.21.input_layernorm.bias'\n",
            "gpt_neox.layers.21.post_attention_layernorm.weight  ->  gpt_neox.layers.21.post_attention_layernorm.weight\n",
            "gpt_neox.layers.21.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.9892578125, 1.0498046875, 0.958984375]\n",
            "b'gpt_neox.layers.21.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.21.post_attention_layernorm.bias  ->  gpt_neox.layers.21.post_attention_layernorm.bias\n",
            "gpt_neox.layers.21.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0216827392578125, 0.00955963134765625, -0.007526397705078125]\n",
            "b'gpt_neox.layers.21.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.21.attention.query_key_value.weight  ->  gpt_neox.layers.21.attention.query_key_value.weight\n",
            "gpt_neox.layers.21.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.05126953125, -0.0019588470458984375, -0.0102996826171875], [0.0167999267578125, -0.050262451171875, -0.030670166015625], [0.053436279296875, 0.035614013671875, -0.0210113525390625]]\n",
            "b'gpt_neox.layers.21.attention.query_key_value.weight'\n",
            "gpt_neox.layers.21.attention.query_key_value.bias  ->  gpt_neox.layers.21.attention.query_key_value.bias\n",
            "gpt_neox.layers.21.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.0355224609375, 0.03765869140625, -0.0033855438232421875]\n",
            "b'gpt_neox.layers.21.attention.query_key_value.bias'\n",
            "gpt_neox.layers.21.attention.dense.weight  ->  gpt_neox.layers.21.attention.dense.weight\n",
            "gpt_neox.layers.21.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.031280517578125, 0.0572509765625, -0.049774169921875], [-0.007228851318359375, 0.0195159912109375, 0.0037326812744140625], [-0.0869140625, -0.008819580078125, 0.039764404296875]]\n",
            "b'gpt_neox.layers.21.attention.dense.weight'\n",
            "gpt_neox.layers.21.attention.dense.bias  ->  gpt_neox.layers.21.attention.dense.bias\n",
            "gpt_neox.layers.21.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.01480865478515625, -0.01099395751953125, 0.0013027191162109375]\n",
            "b'gpt_neox.layers.21.attention.dense.bias'\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.21.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.08734130859375, -0.034759521484375, 0.0662841796875], [-0.01236724853515625, 0.057525634765625, 0.158935546875], [-0.0640869140625, 0.034271240234375, -0.04400634765625]]\n",
            "b'gpt_neox.layers.21.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.21.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.21.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.0266265869140625, -0.0528564453125, -0.03009033203125]\n",
            "b'gpt_neox.layers.21.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.21.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.0216827392578125, -0.0625, 0.07867431640625], [0.0478515625, -0.055938720703125, -0.059051513671875], [0.021728515625, 0.1649169921875, -0.07037353515625]]\n",
            "b'gpt_neox.layers.21.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.21.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.21.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.00994110107421875, -0.01056671142578125, 0.01120758056640625]\n",
            "b'gpt_neox.layers.21.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.22.input_layernorm.weight  ->  gpt_neox.layers.22.input_layernorm.weight\n",
            "gpt_neox.layers.22.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.8359375, 0.8251953125, 0.77783203125]\n",
            "b'gpt_neox.layers.22.input_layernorm.weight'\n",
            "gpt_neox.layers.22.input_layernorm.bias  ->  gpt_neox.layers.22.input_layernorm.bias\n",
            "gpt_neox.layers.22.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0282745361328125, -0.011962890625, 0.016571044921875]\n",
            "b'gpt_neox.layers.22.input_layernorm.bias'\n",
            "gpt_neox.layers.22.post_attention_layernorm.weight  ->  gpt_neox.layers.22.post_attention_layernorm.weight\n",
            "gpt_neox.layers.22.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.99609375, 1.0390625, 0.97900390625]\n",
            "b'gpt_neox.layers.22.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.22.post_attention_layernorm.bias  ->  gpt_neox.layers.22.post_attention_layernorm.bias\n",
            "gpt_neox.layers.22.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.037109375, 0.019073486328125, -0.00797271728515625]\n",
            "b'gpt_neox.layers.22.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.22.attention.query_key_value.weight  ->  gpt_neox.layers.22.attention.query_key_value.weight\n",
            "gpt_neox.layers.22.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.001560211181640625, 0.04193115234375, -0.094970703125], [-0.016571044921875, -0.0703125, 0.00655364990234375], [-0.0267333984375, 0.039215087890625, 0.0272216796875]]\n",
            "b'gpt_neox.layers.22.attention.query_key_value.weight'\n",
            "gpt_neox.layers.22.attention.query_key_value.bias  ->  gpt_neox.layers.22.attention.query_key_value.bias\n",
            "gpt_neox.layers.22.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.07537841796875, 0.02642822265625, -0.007534027099609375]\n",
            "b'gpt_neox.layers.22.attention.query_key_value.bias'\n",
            "gpt_neox.layers.22.attention.dense.weight  ->  gpt_neox.layers.22.attention.dense.weight\n",
            "gpt_neox.layers.22.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.0014209747314453125, 0.01071929931640625, -0.041290283203125], [-0.01123809814453125, -0.005275726318359375, 0.00103759765625], [0.037078857421875, 0.06414794921875, -0.0164337158203125]]\n",
            "b'gpt_neox.layers.22.attention.dense.weight'\n",
            "gpt_neox.layers.22.attention.dense.bias  ->  gpt_neox.layers.22.attention.dense.bias\n",
            "gpt_neox.layers.22.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.007232666015625, -0.01438140869140625, 0.004840850830078125]\n",
            "b'gpt_neox.layers.22.attention.dense.bias'\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.22.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.06866455078125, -0.017578125, 0.061492919921875], [-0.12005615234375, -0.039398193359375, -0.00743865966796875], [-0.0274658203125, -0.004779815673828125, -0.05413818359375]]\n",
            "b'gpt_neox.layers.22.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.22.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.22.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.038787841796875, -0.031951904296875, -0.0570068359375]\n",
            "b'gpt_neox.layers.22.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.22.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.0780029296875, -0.03515625, -0.07891845703125], [-0.014251708984375, -0.027130126953125, 0.034515380859375], [0.048675537109375, -0.039276123046875, 0.12078857421875]]\n",
            "b'gpt_neox.layers.22.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.22.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.22.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.00974273681640625, -0.0143280029296875, 0.0127716064453125]\n",
            "b'gpt_neox.layers.22.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.23.input_layernorm.weight  ->  gpt_neox.layers.23.input_layernorm.weight\n",
            "gpt_neox.layers.23.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.8349609375, 0.83154296875, 0.80029296875]\n",
            "b'gpt_neox.layers.23.input_layernorm.weight'\n",
            "gpt_neox.layers.23.input_layernorm.bias  ->  gpt_neox.layers.23.input_layernorm.bias\n",
            "gpt_neox.layers.23.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0223541259765625, -0.021148681640625, 0.0188751220703125]\n",
            "b'gpt_neox.layers.23.input_layernorm.bias'\n",
            "gpt_neox.layers.23.post_attention_layernorm.weight  ->  gpt_neox.layers.23.post_attention_layernorm.weight\n",
            "gpt_neox.layers.23.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [1.0087890625, 1.0146484375, 0.94775390625]\n",
            "b'gpt_neox.layers.23.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.23.post_attention_layernorm.bias  ->  gpt_neox.layers.23.post_attention_layernorm.bias\n",
            "gpt_neox.layers.23.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0303802490234375, 0.00586700439453125, -0.0016632080078125]\n",
            "b'gpt_neox.layers.23.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.23.attention.query_key_value.weight  ->  gpt_neox.layers.23.attention.query_key_value.weight\n",
            "gpt_neox.layers.23.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.05810546875, 0.002105712890625, 0.0711669921875], [0.04608154296875, 0.025848388671875, 0.044952392578125], [-0.05084228515625, 0.0026874542236328125, 0.027008056640625]]\n",
            "b'gpt_neox.layers.23.attention.query_key_value.weight'\n",
            "gpt_neox.layers.23.attention.query_key_value.bias  ->  gpt_neox.layers.23.attention.query_key_value.bias\n",
            "gpt_neox.layers.23.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.0260467529296875, 0.0124359130859375, -0.0107574462890625]\n",
            "b'gpt_neox.layers.23.attention.query_key_value.bias'\n",
            "gpt_neox.layers.23.attention.dense.weight  ->  gpt_neox.layers.23.attention.dense.weight\n",
            "gpt_neox.layers.23.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.08050537109375, 0.044342041015625, -0.0223846435546875], [-0.054595947265625, -0.0313720703125, 0.159423828125], [0.0009937286376953125, 0.0151214599609375, -0.0982666015625]]\n",
            "b'gpt_neox.layers.23.attention.dense.weight'\n",
            "gpt_neox.layers.23.attention.dense.bias  ->  gpt_neox.layers.23.attention.dense.bias\n",
            "gpt_neox.layers.23.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0084075927734375, -0.01259613037109375, 0.006748199462890625]\n",
            "b'gpt_neox.layers.23.attention.dense.bias'\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.23.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.0225830078125, 0.0218353271484375, 0.021575927734375], [-0.06158447265625, -0.04803466796875, 0.057403564453125], [0.08001708984375, 0.0352783203125, 0.04766845703125]]\n",
            "b'gpt_neox.layers.23.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.23.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.23.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.03399658203125, -0.040863037109375, -0.016204833984375]\n",
            "b'gpt_neox.layers.23.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.23.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.00936126708984375, 0.144775390625, -0.0154266357421875], [0.06787109375, -0.08355712890625, 0.04364013671875], [0.01267242431640625, -0.05194091796875, -0.064697265625]]\n",
            "b'gpt_neox.layers.23.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.23.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.23.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.00942230224609375, -0.01285552978515625, 0.0107421875]\n",
            "b'gpt_neox.layers.23.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.24.input_layernorm.weight  ->  gpt_neox.layers.24.input_layernorm.weight\n",
            "gpt_neox.layers.24.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.84619140625, 0.84912109375, 0.80859375]\n",
            "b'gpt_neox.layers.24.input_layernorm.weight'\n",
            "gpt_neox.layers.24.input_layernorm.bias  ->  gpt_neox.layers.24.input_layernorm.bias\n",
            "gpt_neox.layers.24.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0218505859375, -0.0222625732421875, 0.020172119140625]\n",
            "b'gpt_neox.layers.24.input_layernorm.bias'\n",
            "gpt_neox.layers.24.post_attention_layernorm.weight  ->  gpt_neox.layers.24.post_attention_layernorm.weight\n",
            "gpt_neox.layers.24.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.99658203125, 1.01953125, 0.95947265625]\n",
            "b'gpt_neox.layers.24.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.24.post_attention_layernorm.bias  ->  gpt_neox.layers.24.post_attention_layernorm.bias\n",
            "gpt_neox.layers.24.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0154266357421875, 0.007617950439453125, -0.004360198974609375]\n",
            "b'gpt_neox.layers.24.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.24.attention.query_key_value.weight  ->  gpt_neox.layers.24.attention.query_key_value.weight\n",
            "gpt_neox.layers.24.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.0184783935546875, 0.057098388671875, 0.10443115234375], [-0.0303192138671875, 0.05596923828125, 0.00795745849609375], [0.00894927978515625, -0.0458984375, -0.0031528472900390625]]\n",
            "b'gpt_neox.layers.24.attention.query_key_value.weight'\n",
            "gpt_neox.layers.24.attention.query_key_value.bias  ->  gpt_neox.layers.24.attention.query_key_value.bias\n",
            "gpt_neox.layers.24.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.01146697998046875, 0.003467559814453125, -0.00811004638671875]\n",
            "b'gpt_neox.layers.24.attention.query_key_value.bias'\n",
            "gpt_neox.layers.24.attention.dense.weight  ->  gpt_neox.layers.24.attention.dense.weight\n",
            "gpt_neox.layers.24.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.0389404296875, 0.0367431640625, 0.005207061767578125], [-0.036285400390625, 0.044158935546875, -0.0350341796875], [0.04296875, 0.103271484375, -0.0121612548828125]]\n",
            "b'gpt_neox.layers.24.attention.dense.weight'\n",
            "gpt_neox.layers.24.attention.dense.bias  ->  gpt_neox.layers.24.attention.dense.bias\n",
            "gpt_neox.layers.24.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.01617431640625, -0.01348876953125, 0.00323486328125]\n",
            "b'gpt_neox.layers.24.attention.dense.bias'\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.24.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.01251983642578125, 0.0054473876953125, 0.02630615234375], [-0.056915283203125, -0.0217742919921875, 0.1162109375], [-0.00423431396484375, -0.0577392578125, 0.0296783447265625]]\n",
            "b'gpt_neox.layers.24.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.24.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.24.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.0116424560546875, -0.04205322265625, -0.046661376953125]\n",
            "b'gpt_neox.layers.24.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.24.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.005886077880859375, 0.03314208984375, -0.0174407958984375], [0.07305908203125, 0.089111328125, 0.08526611328125], [0.036376953125, -0.0011968612670898438, -0.07659912109375]]\n",
            "b'gpt_neox.layers.24.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.24.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.24.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.01201629638671875, -0.01207733154296875, 0.0141448974609375]\n",
            "b'gpt_neox.layers.24.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.25.input_layernorm.weight  ->  gpt_neox.layers.25.input_layernorm.weight\n",
            "gpt_neox.layers.25.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.955078125, 0.9267578125, 0.9287109375]\n",
            "b'gpt_neox.layers.25.input_layernorm.weight'\n",
            "gpt_neox.layers.25.input_layernorm.bias  ->  gpt_neox.layers.25.input_layernorm.bias\n",
            "gpt_neox.layers.25.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0204620361328125, -0.0222930908203125, 0.0240631103515625]\n",
            "b'gpt_neox.layers.25.input_layernorm.bias'\n",
            "gpt_neox.layers.25.post_attention_layernorm.weight  ->  gpt_neox.layers.25.post_attention_layernorm.weight\n",
            "gpt_neox.layers.25.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.99267578125, 1.025390625, 0.97705078125]\n",
            "b'gpt_neox.layers.25.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.25.post_attention_layernorm.bias  ->  gpt_neox.layers.25.post_attention_layernorm.bias\n",
            "gpt_neox.layers.25.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.024017333984375, 0.01113128662109375, -0.0007777214050292969]\n",
            "b'gpt_neox.layers.25.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.25.attention.query_key_value.weight  ->  gpt_neox.layers.25.attention.query_key_value.weight\n",
            "gpt_neox.layers.25.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.005954742431640625, -0.043121337890625, 0.0218353271484375], [-0.005565643310546875, 0.05780029296875, -0.045745849609375], [0.003246307373046875, 0.009368896484375, -0.00525665283203125]]\n",
            "b'gpt_neox.layers.25.attention.query_key_value.weight'\n",
            "gpt_neox.layers.25.attention.query_key_value.bias  ->  gpt_neox.layers.25.attention.query_key_value.bias\n",
            "gpt_neox.layers.25.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.004886627197265625, -0.0024776458740234375, -0.01389312744140625]\n",
            "b'gpt_neox.layers.25.attention.query_key_value.bias'\n",
            "gpt_neox.layers.25.attention.dense.weight  ->  gpt_neox.layers.25.attention.dense.weight\n",
            "gpt_neox.layers.25.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.02947998046875, -0.017669677734375, -0.00479888916015625], [-0.05975341796875, -0.00775909423828125, 0.06341552734375], [-0.1435546875, -0.03179931640625, 0.011474609375]]\n",
            "b'gpt_neox.layers.25.attention.dense.weight'\n",
            "gpt_neox.layers.25.attention.dense.bias  ->  gpt_neox.layers.25.attention.dense.bias\n",
            "gpt_neox.layers.25.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0160980224609375, -0.0154571533203125, 0.006977081298828125]\n",
            "b'gpt_neox.layers.25.attention.dense.bias'\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.25.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.004283905029296875, 0.0220489501953125, 0.04443359375], [0.01983642578125, -0.0011491775512695312, -0.076904296875], [-0.040252685546875, 0.026702880859375, 0.0294952392578125]]\n",
            "b'gpt_neox.layers.25.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.25.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.25.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.03173828125, -0.05963134765625, -0.0257568359375]\n",
            "b'gpt_neox.layers.25.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.25.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.03485107421875, 0.058380126953125, -0.11553955078125], [-0.08929443359375, 0.00366973876953125, -0.10418701171875], [-0.056182861328125, -0.017120361328125, -0.034423828125]]\n",
            "b'gpt_neox.layers.25.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.25.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.25.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0139007568359375, -0.012420654296875, 0.0162506103515625]\n",
            "b'gpt_neox.layers.25.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.26.input_layernorm.weight  ->  gpt_neox.layers.26.input_layernorm.weight\n",
            "gpt_neox.layers.26.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.82177734375, 0.8408203125, 0.8125]\n",
            "b'gpt_neox.layers.26.input_layernorm.weight'\n",
            "gpt_neox.layers.26.input_layernorm.bias  ->  gpt_neox.layers.26.input_layernorm.bias\n",
            "gpt_neox.layers.26.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01538848876953125, -0.01995849609375, 0.02484130859375]\n",
            "b'gpt_neox.layers.26.input_layernorm.bias'\n",
            "gpt_neox.layers.26.post_attention_layernorm.weight  ->  gpt_neox.layers.26.post_attention_layernorm.weight\n",
            "gpt_neox.layers.26.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.98486328125, 1.0283203125, 0.978515625]\n",
            "b'gpt_neox.layers.26.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.26.post_attention_layernorm.bias  ->  gpt_neox.layers.26.post_attention_layernorm.bias\n",
            "gpt_neox.layers.26.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0227813720703125, -0.00457000732421875, 0.0008716583251953125]\n",
            "b'gpt_neox.layers.26.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.26.attention.query_key_value.weight  ->  gpt_neox.layers.26.attention.query_key_value.weight\n",
            "gpt_neox.layers.26.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[-0.0469970703125, 0.08154296875, 0.006927490234375], [0.07403564453125, 0.07012939453125, 0.0227203369140625], [0.0159149169921875, 0.0216064453125, 0.058685302734375]]\n",
            "b'gpt_neox.layers.26.attention.query_key_value.weight'\n",
            "gpt_neox.layers.26.attention.query_key_value.bias  ->  gpt_neox.layers.26.attention.query_key_value.bias\n",
            "gpt_neox.layers.26.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.015777587890625, -0.0002130270004272461, -0.01128387451171875]\n",
            "b'gpt_neox.layers.26.attention.query_key_value.bias'\n",
            "gpt_neox.layers.26.attention.dense.weight  ->  gpt_neox.layers.26.attention.dense.weight\n",
            "gpt_neox.layers.26.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.0110015869140625, -0.061279296875, 0.0270843505859375], [-0.021453857421875, 0.0200958251953125, -0.084228515625], [-0.005496978759765625, 0.01384735107421875, 0.0036945343017578125]]\n",
            "b'gpt_neox.layers.26.attention.dense.weight'\n",
            "gpt_neox.layers.26.attention.dense.bias  ->  gpt_neox.layers.26.attention.dense.bias\n",
            "gpt_neox.layers.26.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.018035888671875, -0.01617431640625, 0.00930023193359375]\n",
            "b'gpt_neox.layers.26.attention.dense.bias'\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.26.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[0.049896240234375, 0.02032470703125, -0.01457977294921875], [0.026123046875, 0.01611328125, -0.0953369140625], [-0.037841796875, 0.055816650390625, -0.0467529296875]]\n",
            "b'gpt_neox.layers.26.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.26.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.26.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.04327392578125, -0.0191192626953125, -0.03424072265625]\n",
            "b'gpt_neox.layers.26.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.26.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.032440185546875, 0.0914306640625, -0.006885528564453125], [0.0246429443359375, -0.08172607421875, 0.05242919921875], [-0.021881103515625, 0.0128936767578125, 0.031280517578125]]\n",
            "b'gpt_neox.layers.26.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.26.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.26.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.01486968994140625, -0.01092529296875, 0.0166015625]\n",
            "b'gpt_neox.layers.26.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.27.input_layernorm.weight  ->  gpt_neox.layers.27.input_layernorm.weight\n",
            "gpt_neox.layers.27.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.85693359375, 0.83251953125, 0.83740234375]\n",
            "b'gpt_neox.layers.27.input_layernorm.weight'\n",
            "gpt_neox.layers.27.input_layernorm.bias  ->  gpt_neox.layers.27.input_layernorm.bias\n",
            "gpt_neox.layers.27.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0110321044921875, -0.028961181640625, 0.00484466552734375]\n",
            "b'gpt_neox.layers.27.input_layernorm.bias'\n",
            "gpt_neox.layers.27.post_attention_layernorm.weight  ->  gpt_neox.layers.27.post_attention_layernorm.weight\n",
            "gpt_neox.layers.27.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.9931640625, 1.009765625, 0.9794921875]\n",
            "b'gpt_neox.layers.27.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.27.post_attention_layernorm.bias  ->  gpt_neox.layers.27.post_attention_layernorm.bias\n",
            "gpt_neox.layers.27.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0173492431640625, 0.006011962890625, -0.01094818115234375]\n",
            "b'gpt_neox.layers.27.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.27.attention.query_key_value.weight  ->  gpt_neox.layers.27.attention.query_key_value.weight\n",
            "gpt_neox.layers.27.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.05487060546875, 0.01055908203125, -0.059051513671875], [0.03515625, 0.027374267578125, 0.054962158203125], [-0.006015777587890625, -0.044097900390625, -0.0743408203125]]\n",
            "b'gpt_neox.layers.27.attention.query_key_value.weight'\n",
            "gpt_neox.layers.27.attention.query_key_value.bias  ->  gpt_neox.layers.27.attention.query_key_value.bias\n",
            "gpt_neox.layers.27.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.0089111328125, 0.0113372802734375, -0.0012903213500976562]\n",
            "b'gpt_neox.layers.27.attention.query_key_value.bias'\n",
            "gpt_neox.layers.27.attention.dense.weight  ->  gpt_neox.layers.27.attention.dense.weight\n",
            "gpt_neox.layers.27.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.026519775390625, 0.009429931640625, 0.0120697021484375], [-0.00998687744140625, -0.006267547607421875, -0.10400390625], [0.01244354248046875, -0.0012311935424804688, -0.0574951171875]]\n",
            "b'gpt_neox.layers.27.attention.dense.weight'\n",
            "gpt_neox.layers.27.attention.dense.bias  ->  gpt_neox.layers.27.attention.dense.bias\n",
            "gpt_neox.layers.27.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0196533203125, -0.0118560791015625, 0.0125732421875]\n",
            "b'gpt_neox.layers.27.attention.dense.bias'\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.27.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.040130615234375, -0.04901123046875, -0.0179595947265625], [0.0013980865478515625, 0.02166748046875, -0.025360107421875], [0.0191192626953125, 0.0272979736328125, -0.03497314453125]]\n",
            "b'gpt_neox.layers.27.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.27.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.27.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.032684326171875, -0.0301513671875, -0.03753662109375]\n",
            "b'gpt_neox.layers.27.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.27.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.059173583984375, -0.04742431640625, -0.04730224609375], [0.01381683349609375, -0.00934600830078125, 0.054168701171875], [-0.006465911865234375, -0.0274505615234375, 0.057098388671875]]\n",
            "b'gpt_neox.layers.27.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.27.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.27.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.007904052734375, -0.0039520263671875, 0.022613525390625]\n",
            "b'gpt_neox.layers.27.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.28.input_layernorm.weight  ->  gpt_neox.layers.28.input_layernorm.weight\n",
            "gpt_neox.layers.28.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.82666015625, 0.86376953125, 0.82861328125]\n",
            "b'gpt_neox.layers.28.input_layernorm.weight'\n",
            "gpt_neox.layers.28.input_layernorm.bias  ->  gpt_neox.layers.28.input_layernorm.bias\n",
            "gpt_neox.layers.28.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.01071929931640625, -0.0276031494140625, 0.004878997802734375]\n",
            "b'gpt_neox.layers.28.input_layernorm.bias'\n",
            "gpt_neox.layers.28.post_attention_layernorm.weight  ->  gpt_neox.layers.28.post_attention_layernorm.weight\n",
            "gpt_neox.layers.28.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [1.021484375, 1.052734375, 1.0107421875]\n",
            "b'gpt_neox.layers.28.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.28.post_attention_layernorm.bias  ->  gpt_neox.layers.28.post_attention_layernorm.bias\n",
            "gpt_neox.layers.28.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.003078460693359375, 0.0014019012451171875, -0.0202789306640625]\n",
            "b'gpt_neox.layers.28.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.28.attention.query_key_value.weight  ->  gpt_neox.layers.28.attention.query_key_value.weight\n",
            "gpt_neox.layers.28.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.087158203125, 0.01128387451171875, 0.1346435546875], [-0.106201171875, -0.050933837890625, 0.03997802734375], [0.024200439453125, 0.08447265625, 0.00531005859375]]\n",
            "b'gpt_neox.layers.28.attention.query_key_value.weight'\n",
            "gpt_neox.layers.28.attention.query_key_value.bias  ->  gpt_neox.layers.28.attention.query_key_value.bias\n",
            "gpt_neox.layers.28.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.05859375, -0.00945281982421875, -0.0010290145874023438]\n",
            "b'gpt_neox.layers.28.attention.query_key_value.bias'\n",
            "gpt_neox.layers.28.attention.dense.weight  ->  gpt_neox.layers.28.attention.dense.weight\n",
            "gpt_neox.layers.28.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[-0.045928955078125, 0.054840087890625, -0.0552978515625], [0.0457763671875, 0.12359619140625, 0.09246826171875], [0.0227508544921875, -0.009490966796875, 0.04229736328125]]\n",
            "b'gpt_neox.layers.28.attention.dense.weight'\n",
            "gpt_neox.layers.28.attention.dense.bias  ->  gpt_neox.layers.28.attention.dense.bias\n",
            "gpt_neox.layers.28.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.01561737060546875, -0.00794219970703125, 0.0163726806640625]\n",
            "b'gpt_neox.layers.28.attention.dense.bias'\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.28.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.025421142578125, -0.0947265625, -0.050628662109375], [0.0120697021484375, 0.006740570068359375, 0.06610107421875], [-0.054656982421875, -0.0267486572265625, -0.048248291015625]]\n",
            "b'gpt_neox.layers.28.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.28.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.28.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.04132080078125, -0.0237274169921875, -0.040863037109375]\n",
            "b'gpt_neox.layers.28.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.28.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.005359649658203125, -0.0655517578125, -0.0169830322265625], [-0.0191192626953125, -0.00763702392578125, -0.0555419921875], [-0.026123046875, -0.00022935867309570312, 0.08380126953125]]\n",
            "b'gpt_neox.layers.28.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.28.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.28.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0031986236572265625, -0.001201629638671875, 0.030303955078125]\n",
            "b'gpt_neox.layers.28.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.29.input_layernorm.weight  ->  gpt_neox.layers.29.input_layernorm.weight\n",
            "gpt_neox.layers.29.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.869140625, 0.869140625, 0.900390625]\n",
            "b'gpt_neox.layers.29.input_layernorm.weight'\n",
            "gpt_neox.layers.29.input_layernorm.bias  ->  gpt_neox.layers.29.input_layernorm.bias\n",
            "gpt_neox.layers.29.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0182647705078125, -0.0205078125, 0.00447845458984375]\n",
            "b'gpt_neox.layers.29.input_layernorm.bias'\n",
            "gpt_neox.layers.29.post_attention_layernorm.weight  ->  gpt_neox.layers.29.post_attention_layernorm.weight\n",
            "gpt_neox.layers.29.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [1.0634765625, 1.0595703125, 1.08203125]\n",
            "b'gpt_neox.layers.29.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.29.post_attention_layernorm.bias  ->  gpt_neox.layers.29.post_attention_layernorm.bias\n",
            "gpt_neox.layers.29.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0171661376953125, -0.006195068359375, -0.02362060546875]\n",
            "b'gpt_neox.layers.29.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.29.attention.query_key_value.weight  ->  gpt_neox.layers.29.attention.query_key_value.weight\n",
            "gpt_neox.layers.29.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.0628662109375, -0.00603485107421875, 0.038421630859375], [0.0260467529296875, -0.09197998046875, 0.021514892578125], [0.050018310546875, -0.1038818359375, 0.0216827392578125]]\n",
            "b'gpt_neox.layers.29.attention.query_key_value.weight'\n",
            "gpt_neox.layers.29.attention.query_key_value.bias  ->  gpt_neox.layers.29.attention.query_key_value.bias\n",
            "gpt_neox.layers.29.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.018646240234375, -0.039031982421875, -0.0012369155883789062]\n",
            "b'gpt_neox.layers.29.attention.query_key_value.bias'\n",
            "gpt_neox.layers.29.attention.dense.weight  ->  gpt_neox.layers.29.attention.dense.weight\n",
            "gpt_neox.layers.29.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.0184783935546875, 0.025634765625, 0.009613037109375], [0.0222930908203125, 0.06512451171875, -0.0621337890625], [0.1231689453125, 0.00977325439453125, -0.0023822784423828125]]\n",
            "b'gpt_neox.layers.29.attention.dense.weight'\n",
            "gpt_neox.layers.29.attention.dense.bias  ->  gpt_neox.layers.29.attention.dense.bias\n",
            "gpt_neox.layers.29.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0159149169921875, -0.007205963134765625, 0.0246734619140625]\n",
            "b'gpt_neox.layers.29.attention.dense.bias'\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.29.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.039794921875, 0.056976318359375, -0.0821533203125], [0.007354736328125, -0.08636474609375, -0.030914306640625], [0.041351318359375, 0.01959228515625, 0.0079193115234375]]\n",
            "b'gpt_neox.layers.29.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.29.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.29.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.04296875, -0.036895751953125, -0.050994873046875]\n",
            "b'gpt_neox.layers.29.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.29.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[0.03497314453125, 0.08087158203125, 0.09332275390625], [-0.035003662109375, -0.1229248046875, 0.0247955322265625], [0.145751953125, 0.01363372802734375, 0.022674560546875]]\n",
            "b'gpt_neox.layers.29.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.29.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.29.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.003185272216796875, -0.0081634521484375, 0.03558349609375]\n",
            "b'gpt_neox.layers.29.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.30.input_layernorm.weight  ->  gpt_neox.layers.30.input_layernorm.weight\n",
            "gpt_neox.layers.30.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.90478515625, 0.92138671875, 0.9365234375]\n",
            "b'gpt_neox.layers.30.input_layernorm.weight'\n",
            "gpt_neox.layers.30.input_layernorm.bias  ->  gpt_neox.layers.30.input_layernorm.bias\n",
            "gpt_neox.layers.30.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.03125, -0.00957489013671875, 0.0164031982421875]\n",
            "b'gpt_neox.layers.30.input_layernorm.bias'\n",
            "gpt_neox.layers.30.post_attention_layernorm.weight  ->  gpt_neox.layers.30.post_attention_layernorm.weight\n",
            "gpt_neox.layers.30.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [1.0615234375, 1.09765625, 1.0927734375]\n",
            "b'gpt_neox.layers.30.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.30.post_attention_layernorm.bias  ->  gpt_neox.layers.30.post_attention_layernorm.bias\n",
            "gpt_neox.layers.30.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.0003898143768310547, -0.01003265380859375, -0.019073486328125]\n",
            "b'gpt_neox.layers.30.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.30.attention.query_key_value.weight  ->  gpt_neox.layers.30.attention.query_key_value.weight\n",
            "gpt_neox.layers.30.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.0233917236328125, 0.0097198486328125, 0.047821044921875], [0.06597900390625, -0.04058837890625, -0.03631591796875], [0.040191650390625, -0.0015697479248046875, -0.06427001953125]]\n",
            "b'gpt_neox.layers.30.attention.query_key_value.weight'\n",
            "gpt_neox.layers.30.attention.query_key_value.bias  ->  gpt_neox.layers.30.attention.query_key_value.bias\n",
            "gpt_neox.layers.30.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [-0.002231597900390625, -0.0052947998046875, -0.01447296142578125]\n",
            "b'gpt_neox.layers.30.attention.query_key_value.bias'\n",
            "gpt_neox.layers.30.attention.dense.weight  ->  gpt_neox.layers.30.attention.dense.weight\n",
            "gpt_neox.layers.30.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.027557373046875, -0.006256103515625, -0.0005946159362792969], [-0.06414794921875, 0.06414794921875, -0.051971435546875], [0.0164947509765625, -0.050079345703125, -0.09344482421875]]\n",
            "b'gpt_neox.layers.30.attention.dense.weight'\n",
            "gpt_neox.layers.30.attention.dense.bias  ->  gpt_neox.layers.30.attention.dense.bias\n",
            "gpt_neox.layers.30.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.00923919677734375, -0.0175018310546875, 0.0262603759765625]\n",
            "b'gpt_neox.layers.30.attention.dense.bias'\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.30.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.026885986328125, 0.055084228515625, -0.00498199462890625], [-0.022918701171875, 0.051055908203125, -0.01102447509765625], [-0.0330810546875, -0.0158538818359375, 0.062408447265625]]\n",
            "b'gpt_neox.layers.30.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.30.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.30.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.0426025390625, -0.029022216796875, -0.03070068359375]\n",
            "b'gpt_neox.layers.30.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.30.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.019927978515625, -0.059783935546875, 0.0286865234375], [0.10186767578125, -0.00702667236328125, -0.0887451171875], [-0.1025390625, 0.0200042724609375, 0.016693115234375]]\n",
            "b'gpt_neox.layers.30.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.30.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.30.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0030651092529296875, -0.00811004638671875, 0.0260162353515625]\n",
            "b'gpt_neox.layers.30.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.layers.31.input_layernorm.weight  ->  gpt_neox.layers.31.input_layernorm.weight\n",
            "gpt_neox.layers.31.input_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.94580078125, 0.98486328125, 0.9404296875]\n",
            "b'gpt_neox.layers.31.input_layernorm.weight'\n",
            "gpt_neox.layers.31.input_layernorm.bias  ->  gpt_neox.layers.31.input_layernorm.bias\n",
            "gpt_neox.layers.31.input_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.044525146484375, 0.0167388916015625, 0.0458984375]\n",
            "b'gpt_neox.layers.31.input_layernorm.bias'\n",
            "gpt_neox.layers.31.post_attention_layernorm.weight  ->  gpt_neox.layers.31.post_attention_layernorm.weight\n",
            "gpt_neox.layers.31.post_attention_layernorm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.9658203125, 0.98828125, 0.99658203125]\n",
            "b'gpt_neox.layers.31.post_attention_layernorm.weight'\n",
            "gpt_neox.layers.31.post_attention_layernorm.bias  ->  gpt_neox.layers.31.post_attention_layernorm.bias\n",
            "gpt_neox.layers.31.post_attention_layernorm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.00711822509765625, 0.00946044921875, -0.00716400146484375]\n",
            "b'gpt_neox.layers.31.post_attention_layernorm.bias'\n",
            "gpt_neox.layers.31.attention.query_key_value.weight  ->  gpt_neox.layers.31.attention.query_key_value.weight\n",
            "gpt_neox.layers.31.attention.query_key_value.weight 2 (12288, 4096)\n",
            "  Converting to float16 (12288, 4096) [[0.01116180419921875, 0.00824737548828125, -0.03729248046875], [0.01018524169921875, -0.0223388671875, 0.0240020751953125], [-0.003200531005859375, 0.0156402587890625, -0.0276031494140625]]\n",
            "b'gpt_neox.layers.31.attention.query_key_value.weight'\n",
            "gpt_neox.layers.31.attention.query_key_value.bias  ->  gpt_neox.layers.31.attention.query_key_value.bias\n",
            "gpt_neox.layers.31.attention.query_key_value.bias 1 (12288,)\n",
            "  Converting to float32 (12288,) [0.00014448165893554688, -0.01096343994140625, -0.00222015380859375]\n",
            "b'gpt_neox.layers.31.attention.query_key_value.bias'\n",
            "gpt_neox.layers.31.attention.dense.weight  ->  gpt_neox.layers.31.attention.dense.weight\n",
            "gpt_neox.layers.31.attention.dense.weight 2 (4096, 4096)\n",
            "  Converting to float16 (4096, 4096) [[0.040557861328125, -0.06207275390625, -0.0181427001953125], [0.009918212890625, -0.056304931640625, 0.0038814544677734375], [-0.020263671875, -0.00864410400390625, -0.05059814453125]]\n",
            "b'gpt_neox.layers.31.attention.dense.weight'\n",
            "gpt_neox.layers.31.attention.dense.bias  ->  gpt_neox.layers.31.attention.dense.bias\n",
            "gpt_neox.layers.31.attention.dense.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.0063018798828125, -0.006710052490234375, 0.0200347900390625]\n",
            "b'gpt_neox.layers.31.attention.dense.bias'\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.weight  ->  gpt_neox.layers.31.mlp.dense_h_to_4h.weight\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.weight 2 (16384, 4096)\n",
            "  Converting to float16 (16384, 4096) [[-0.0321044921875, 0.03753662109375, -0.045654296875], [0.1158447265625, -0.01413726806640625, -0.05303955078125], [0.031829833984375, 0.00846099853515625, -0.08038330078125]]\n",
            "b'gpt_neox.layers.31.mlp.dense_h_to_4h.weight'\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.bias  ->  gpt_neox.layers.31.mlp.dense_h_to_4h.bias\n",
            "gpt_neox.layers.31.mlp.dense_h_to_4h.bias 1 (16384,)\n",
            "  Converting to float32 (16384,) [-0.02587890625, -0.0247650146484375, -0.0237274169921875]\n",
            "b'gpt_neox.layers.31.mlp.dense_h_to_4h.bias'\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.weight  ->  gpt_neox.layers.31.mlp.dense_4h_to_h.weight\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.weight 2 (4096, 16384)\n",
            "  Converting to float16 (4096, 16384) [[-0.0253143310546875, 0.0926513671875, 0.0014543533325195312], [0.03717041015625, 0.11004638671875, -0.028961181640625], [0.0850830078125, -0.021453857421875, -0.00959014892578125]]\n",
            "b'gpt_neox.layers.31.mlp.dense_4h_to_h.weight'\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.bias  ->  gpt_neox.layers.31.mlp.dense_4h_to_h.bias\n",
            "gpt_neox.layers.31.mlp.dense_4h_to_h.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [-0.051727294921875, 0.044708251953125, 0.04705810546875]\n",
            "b'gpt_neox.layers.31.mlp.dense_4h_to_h.bias'\n",
            "gpt_neox.final_layer_norm.weight  ->  gpt_neox.final_layer_norm.weight\n",
            "gpt_neox.final_layer_norm.weight 1 (4096,)\n",
            "  Converting to float32 (4096,) [2.587890625, 2.529296875, 2.548828125]\n",
            "b'gpt_neox.final_layer_norm.weight'\n",
            "gpt_neox.final_layer_norm.bias  ->  gpt_neox.final_layer_norm.bias\n",
            "gpt_neox.final_layer_norm.bias 1 (4096,)\n",
            "  Converting to float32 (4096,) [0.10784912109375, -0.08349609375, -0.06634521484375]\n",
            "b'gpt_neox.final_layer_norm.bias'\n",
            "embed_out.weight  ->  embed_out.weight\n",
            "embed_out.weight 2 (52224, 4096)\n",
            "  Converting to float16 (52224, 4096) [[0.0004208087921142578, -0.06683349609375, -0.08740234375], [-0.0218505859375, 0.0469970703125, 0.0482177734375], [-0.022979736328125, -0.0328369140625, 0.006622314453125]]\n",
            "b'embed_out.weight'\n",
            "Done. Output file: /content/drive/MyDrive/models/calm_dolly_7B/ggml/ggml-hf_ckpt-f16.bin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $cpp_repo\n",
        "!make quantize-gptneox redpajama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4ahkWqpPTUw",
        "outputId": "c943fea6-bcdc-4f1e-ab78-6263e884d296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/redpajama.cpp\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "make: 'quantize-gptneox' is up to date.\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -c examples/redpajama/common-gptneox.cpp -o common-gptneox.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native examples/redpajama/main-redpajama.cpp ggml.o gptneox.o common-gptneox.o -o redpajama \n",
            "\u001b[01m\u001b[Kexamples/redpajama/main-redpajama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/redpajama/main-redpajama.cpp:291:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kis_antiprompt\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            "  291 |     bool \u001b[01;35m\u001b[Kis_antiprompt\u001b[m\u001b[K = false;\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\n",
            "====  Run ./redpajama -h for help.  ====\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title quantize { display-mode: \"form\" }\n",
        "# model_name =  '/content/redpajama.cpp/models/ggml-japanese-gpt-neox-3.6b-instruction-sft-f16.bin' #@param {type:\"string\"}\n",
        "model_name =  f'{output_path}/ggml/ggml-hf_ckpt-f16.bin'\n",
        "\n",
        "%cd $cpp_repo\n",
        "\n",
        "!python ./examples/redpajama/scripts/quantize-gptneox.py $model_name --quantize-output-type q8_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrdmbDTkOsXQ",
        "outputId": "411ddbbf-e148-4468-da21-2182bff323eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/redpajama.cpp\n",
            "gptneox.cpp: loading model from /content/drive/MyDrive/models/calm_dolly_7B/ggml/ggml-hf_ckpt-f16.bin\n",
            "gptneox.cpp: saving model to /content/drive/MyDrive/models/calm_dolly_7B/ggml/ggml-hf_ckpt-q8_0.bin\n",
            "[   1/ 388]             gpt_neox.embed_in.weight -     4096 x 52224, type =    f16, quantizing .. size =   408.00 MiB ->   229.50 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[   2/ 388] gpt_neox.layers.0.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[   3/ 388] gpt_neox.layers.0.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[   4/ 388] gpt_neox.layers.0.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[   5/ 388] gpt_neox.layers.0.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[   6/ 388] gpt_neox.layers.0.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[   7/ 388] gpt_neox.layers.0.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[   8/ 388] gpt_neox.layers.0.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.019 0.027 \n",
            "[   9/ 388] gpt_neox.layers.0.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  10/ 388] gpt_neox.layers.0.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.029 0.021 0.033 0.049 0.069 0.089 0.106 0.223 0.105 0.087 0.066 0.047 0.031 0.019 0.026 \n",
            "[  11/ 388] gpt_neox.layers.0.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[  12/ 388] gpt_neox.layers.0.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  13/ 388] gpt_neox.layers.0.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  14/ 388] gpt_neox.layers.1.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  15/ 388] gpt_neox.layers.1.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  16/ 388] gpt_neox.layers.1.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  17/ 388] gpt_neox.layers.1.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  18/ 388] gpt_neox.layers.1.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  19/ 388] gpt_neox.layers.1.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[  20/ 388] gpt_neox.layers.1.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  21/ 388] gpt_neox.layers.1.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  22/ 388] gpt_neox.layers.1.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.021 0.033 0.049 0.068 0.089 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[  23/ 388] gpt_neox.layers.1.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[  24/ 388] gpt_neox.layers.1.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  25/ 388] gpt_neox.layers.1.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  26/ 388] gpt_neox.layers.2.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  27/ 388] gpt_neox.layers.2.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  28/ 388] gpt_neox.layers.2.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  29/ 388] gpt_neox.layers.2.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  30/ 388] gpt_neox.layers.2.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  31/ 388] gpt_neox.layers.2.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[  32/ 388] gpt_neox.layers.2.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  33/ 388] gpt_neox.layers.2.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  34/ 388] gpt_neox.layers.2.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.089 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[  35/ 388] gpt_neox.layers.2.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[  36/ 388] gpt_neox.layers.2.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  37/ 388] gpt_neox.layers.2.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  38/ 388] gpt_neox.layers.3.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  39/ 388] gpt_neox.layers.3.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  40/ 388] gpt_neox.layers.3.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  41/ 388] gpt_neox.layers.3.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  42/ 388] gpt_neox.layers.3.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  43/ 388] gpt_neox.layers.3.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[  44/ 388] gpt_neox.layers.3.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  45/ 388] gpt_neox.layers.3.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  46/ 388] gpt_neox.layers.3.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.089 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[  47/ 388] gpt_neox.layers.3.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[  48/ 388] gpt_neox.layers.3.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  49/ 388] gpt_neox.layers.3.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  50/ 388] gpt_neox.layers.4.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  51/ 388] gpt_neox.layers.4.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  52/ 388] gpt_neox.layers.4.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  53/ 388] gpt_neox.layers.4.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  54/ 388] gpt_neox.layers.4.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  55/ 388] gpt_neox.layers.4.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[  56/ 388] gpt_neox.layers.4.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  57/ 388] gpt_neox.layers.4.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  58/ 388] gpt_neox.layers.4.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[  59/ 388] gpt_neox.layers.4.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[  60/ 388] gpt_neox.layers.4.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  61/ 388] gpt_neox.layers.4.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  62/ 388] gpt_neox.layers.5.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  63/ 388] gpt_neox.layers.5.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  64/ 388] gpt_neox.layers.5.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  65/ 388] gpt_neox.layers.5.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  66/ 388] gpt_neox.layers.5.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  67/ 388] gpt_neox.layers.5.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[  68/ 388] gpt_neox.layers.5.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  69/ 388] gpt_neox.layers.5.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  70/ 388] gpt_neox.layers.5.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[  71/ 388] gpt_neox.layers.5.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[  72/ 388] gpt_neox.layers.5.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  73/ 388] gpt_neox.layers.5.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  74/ 388] gpt_neox.layers.6.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  75/ 388] gpt_neox.layers.6.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  76/ 388] gpt_neox.layers.6.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  77/ 388] gpt_neox.layers.6.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  78/ 388] gpt_neox.layers.6.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  79/ 388] gpt_neox.layers.6.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[  80/ 388] gpt_neox.layers.6.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  81/ 388] gpt_neox.layers.6.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  82/ 388] gpt_neox.layers.6.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[  83/ 388] gpt_neox.layers.6.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[  84/ 388] gpt_neox.layers.6.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[  85/ 388] gpt_neox.layers.6.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  86/ 388] gpt_neox.layers.7.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  87/ 388] gpt_neox.layers.7.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  88/ 388] gpt_neox.layers.7.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  89/ 388] gpt_neox.layers.7.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  90/ 388] gpt_neox.layers.7.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  91/ 388] gpt_neox.layers.7.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[  92/ 388] gpt_neox.layers.7.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[  93/ 388] gpt_neox.layers.7.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  94/ 388] gpt_neox.layers.7.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[  95/ 388] gpt_neox.layers.7.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[  96/ 388] gpt_neox.layers.7.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.019 0.027 \n",
            "[  97/ 388] gpt_neox.layers.7.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[  98/ 388] gpt_neox.layers.8.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[  99/ 388] gpt_neox.layers.8.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 100/ 388] gpt_neox.layers.8.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 101/ 388] gpt_neox.layers.8.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 102/ 388] gpt_neox.layers.8.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 103/ 388] gpt_neox.layers.8.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 104/ 388] gpt_neox.layers.8.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 105/ 388] gpt_neox.layers.8.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 106/ 388] gpt_neox.layers.8.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 107/ 388] gpt_neox.layers.8.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 108/ 388] gpt_neox.layers.8.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.019 0.027 \n",
            "[ 109/ 388] gpt_neox.layers.8.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 110/ 388] gpt_neox.layers.9.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 111/ 388] gpt_neox.layers.9.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 112/ 388] gpt_neox.layers.9.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 113/ 388] gpt_neox.layers.9.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 114/ 388] gpt_neox.layers.9.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 115/ 388] gpt_neox.layers.9.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 116/ 388] gpt_neox.layers.9.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.223 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 117/ 388] gpt_neox.layers.9.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 118/ 388] gpt_neox.layers.9.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.087 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 119/ 388] gpt_neox.layers.9.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 120/ 388] gpt_neox.layers.9.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.019 0.027 \n",
            "[ 121/ 388] gpt_neox.layers.9.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 122/ 388] gpt_neox.layers.10.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 123/ 388] gpt_neox.layers.10.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 124/ 388] gpt_neox.layers.10.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 125/ 388] gpt_neox.layers.10.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 126/ 388] gpt_neox.layers.10.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 127/ 388] gpt_neox.layers.10.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 128/ 388] gpt_neox.layers.10.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 129/ 388] gpt_neox.layers.10.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 130/ 388] gpt_neox.layers.10.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 131/ 388] gpt_neox.layers.10.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 132/ 388] gpt_neox.layers.10.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 133/ 388] gpt_neox.layers.10.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 134/ 388] gpt_neox.layers.11.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 135/ 388] gpt_neox.layers.11.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 136/ 388] gpt_neox.layers.11.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 137/ 388] gpt_neox.layers.11.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 138/ 388] gpt_neox.layers.11.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 139/ 388] gpt_neox.layers.11.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 140/ 388] gpt_neox.layers.11.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 141/ 388] gpt_neox.layers.11.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 142/ 388] gpt_neox.layers.11.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 143/ 388] gpt_neox.layers.11.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 144/ 388] gpt_neox.layers.11.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.067 0.088 0.107 0.228 0.107 0.088 0.067 0.047 0.031 0.019 0.027 \n",
            "[ 145/ 388] gpt_neox.layers.11.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 146/ 388] gpt_neox.layers.12.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 147/ 388] gpt_neox.layers.12.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 148/ 388] gpt_neox.layers.12.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 149/ 388] gpt_neox.layers.12.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 150/ 388] gpt_neox.layers.12.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 151/ 388] gpt_neox.layers.12.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 152/ 388] gpt_neox.layers.12.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 153/ 388] gpt_neox.layers.12.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 154/ 388] gpt_neox.layers.12.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 155/ 388] gpt_neox.layers.12.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 156/ 388] gpt_neox.layers.12.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.227 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 157/ 388] gpt_neox.layers.12.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 158/ 388] gpt_neox.layers.13.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 159/ 388] gpt_neox.layers.13.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 160/ 388] gpt_neox.layers.13.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 161/ 388] gpt_neox.layers.13.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 162/ 388] gpt_neox.layers.13.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 163/ 388] gpt_neox.layers.13.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 164/ 388] gpt_neox.layers.13.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 165/ 388] gpt_neox.layers.13.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 166/ 388] gpt_neox.layers.13.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 167/ 388] gpt_neox.layers.13.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 168/ 388] gpt_neox.layers.13.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.227 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 169/ 388] gpt_neox.layers.13.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 170/ 388] gpt_neox.layers.14.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 171/ 388] gpt_neox.layers.14.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 172/ 388] gpt_neox.layers.14.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 173/ 388] gpt_neox.layers.14.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 174/ 388] gpt_neox.layers.14.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 175/ 388] gpt_neox.layers.14.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 176/ 388] gpt_neox.layers.14.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 177/ 388] gpt_neox.layers.14.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 178/ 388] gpt_neox.layers.14.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.047 0.032 0.020 0.027 \n",
            "[ 179/ 388] gpt_neox.layers.14.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 180/ 388] gpt_neox.layers.14.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.227 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 181/ 388] gpt_neox.layers.14.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 182/ 388] gpt_neox.layers.15.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 183/ 388] gpt_neox.layers.15.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 184/ 388] gpt_neox.layers.15.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 185/ 388] gpt_neox.layers.15.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 186/ 388] gpt_neox.layers.15.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 187/ 388] gpt_neox.layers.15.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 188/ 388] gpt_neox.layers.15.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 189/ 388] gpt_neox.layers.15.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 190/ 388] gpt_neox.layers.15.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 191/ 388] gpt_neox.layers.15.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 192/ 388] gpt_neox.layers.15.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.227 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 193/ 388] gpt_neox.layers.15.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 194/ 388] gpt_neox.layers.16.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 195/ 388] gpt_neox.layers.16.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 196/ 388] gpt_neox.layers.16.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 197/ 388] gpt_neox.layers.16.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 198/ 388] gpt_neox.layers.16.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 199/ 388] gpt_neox.layers.16.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 200/ 388] gpt_neox.layers.16.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 201/ 388] gpt_neox.layers.16.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 202/ 388] gpt_neox.layers.16.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 203/ 388] gpt_neox.layers.16.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 204/ 388] gpt_neox.layers.16.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 205/ 388] gpt_neox.layers.16.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 206/ 388] gpt_neox.layers.17.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 207/ 388] gpt_neox.layers.17.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 208/ 388] gpt_neox.layers.17.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 209/ 388] gpt_neox.layers.17.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 210/ 388] gpt_neox.layers.17.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 211/ 388] gpt_neox.layers.17.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 212/ 388] gpt_neox.layers.17.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 213/ 388] gpt_neox.layers.17.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 214/ 388] gpt_neox.layers.17.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 215/ 388] gpt_neox.layers.17.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 216/ 388] gpt_neox.layers.17.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.227 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 217/ 388] gpt_neox.layers.17.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 218/ 388] gpt_neox.layers.18.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 219/ 388] gpt_neox.layers.18.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 220/ 388] gpt_neox.layers.18.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 221/ 388] gpt_neox.layers.18.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 222/ 388] gpt_neox.layers.18.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 223/ 388] gpt_neox.layers.18.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 224/ 388] gpt_neox.layers.18.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 225/ 388] gpt_neox.layers.18.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 226/ 388] gpt_neox.layers.18.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 227/ 388] gpt_neox.layers.18.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 228/ 388] gpt_neox.layers.18.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 229/ 388] gpt_neox.layers.18.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 230/ 388] gpt_neox.layers.19.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 231/ 388] gpt_neox.layers.19.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 232/ 388] gpt_neox.layers.19.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 233/ 388] gpt_neox.layers.19.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 234/ 388] gpt_neox.layers.19.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 235/ 388] gpt_neox.layers.19.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 236/ 388] gpt_neox.layers.19.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 237/ 388] gpt_neox.layers.19.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 238/ 388] gpt_neox.layers.19.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 239/ 388] gpt_neox.layers.19.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 240/ 388] gpt_neox.layers.19.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 241/ 388] gpt_neox.layers.19.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 242/ 388] gpt_neox.layers.20.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 243/ 388] gpt_neox.layers.20.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 244/ 388] gpt_neox.layers.20.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 245/ 388] gpt_neox.layers.20.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 246/ 388] gpt_neox.layers.20.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 247/ 388] gpt_neox.layers.20.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 248/ 388] gpt_neox.layers.20.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 249/ 388] gpt_neox.layers.20.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 250/ 388] gpt_neox.layers.20.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 251/ 388] gpt_neox.layers.20.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 252/ 388] gpt_neox.layers.20.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.031 0.020 0.027 \n",
            "[ 253/ 388] gpt_neox.layers.20.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 254/ 388] gpt_neox.layers.21.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 255/ 388] gpt_neox.layers.21.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 256/ 388] gpt_neox.layers.21.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 257/ 388] gpt_neox.layers.21.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 258/ 388] gpt_neox.layers.21.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 259/ 388] gpt_neox.layers.21.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 260/ 388] gpt_neox.layers.21.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 261/ 388] gpt_neox.layers.21.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 262/ 388] gpt_neox.layers.21.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 263/ 388] gpt_neox.layers.21.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 264/ 388] gpt_neox.layers.21.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.048 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 265/ 388] gpt_neox.layers.21.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 266/ 388] gpt_neox.layers.22.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 267/ 388] gpt_neox.layers.22.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 268/ 388] gpt_neox.layers.22.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 269/ 388] gpt_neox.layers.22.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 270/ 388] gpt_neox.layers.22.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 271/ 388] gpt_neox.layers.22.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 272/ 388] gpt_neox.layers.22.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 273/ 388] gpt_neox.layers.22.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 274/ 388] gpt_neox.layers.22.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 275/ 388] gpt_neox.layers.22.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 276/ 388] gpt_neox.layers.22.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 277/ 388] gpt_neox.layers.22.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 278/ 388] gpt_neox.layers.23.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 279/ 388] gpt_neox.layers.23.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 280/ 388] gpt_neox.layers.23.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 281/ 388] gpt_neox.layers.23.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 282/ 388] gpt_neox.layers.23.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 283/ 388] gpt_neox.layers.23.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 284/ 388] gpt_neox.layers.23.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 285/ 388] gpt_neox.layers.23.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 286/ 388] gpt_neox.layers.23.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 287/ 388] gpt_neox.layers.23.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 288/ 388] gpt_neox.layers.23.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 289/ 388] gpt_neox.layers.23.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 290/ 388] gpt_neox.layers.24.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 291/ 388] gpt_neox.layers.24.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 292/ 388] gpt_neox.layers.24.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 293/ 388] gpt_neox.layers.24.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 294/ 388] gpt_neox.layers.24.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 295/ 388] gpt_neox.layers.24.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 296/ 388] gpt_neox.layers.24.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.028 \n",
            "[ 297/ 388] gpt_neox.layers.24.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 298/ 388] gpt_neox.layers.24.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 299/ 388] gpt_neox.layers.24.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 300/ 388] gpt_neox.layers.24.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 301/ 388] gpt_neox.layers.24.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 302/ 388] gpt_neox.layers.25.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 303/ 388] gpt_neox.layers.25.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 304/ 388] gpt_neox.layers.25.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 305/ 388] gpt_neox.layers.25.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 306/ 388] gpt_neox.layers.25.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 307/ 388] gpt_neox.layers.25.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 308/ 388] gpt_neox.layers.25.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 309/ 388] gpt_neox.layers.25.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 310/ 388] gpt_neox.layers.25.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 311/ 388] gpt_neox.layers.25.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 312/ 388] gpt_neox.layers.25.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 313/ 388] gpt_neox.layers.25.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 314/ 388] gpt_neox.layers.26.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 315/ 388] gpt_neox.layers.26.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 316/ 388] gpt_neox.layers.26.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 317/ 388] gpt_neox.layers.26.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 318/ 388] gpt_neox.layers.26.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 319/ 388] gpt_neox.layers.26.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 320/ 388] gpt_neox.layers.26.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 321/ 388] gpt_neox.layers.26.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 322/ 388] gpt_neox.layers.26.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 323/ 388] gpt_neox.layers.26.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 324/ 388] gpt_neox.layers.26.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 325/ 388] gpt_neox.layers.26.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 326/ 388] gpt_neox.layers.27.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 327/ 388] gpt_neox.layers.27.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 328/ 388] gpt_neox.layers.27.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 329/ 388] gpt_neox.layers.27.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 330/ 388] gpt_neox.layers.27.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 331/ 388] gpt_neox.layers.27.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 332/ 388] gpt_neox.layers.27.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.105 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 333/ 388] gpt_neox.layers.27.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 334/ 388] gpt_neox.layers.27.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 335/ 388] gpt_neox.layers.27.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 336/ 388] gpt_neox.layers.27.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 337/ 388] gpt_neox.layers.27.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 338/ 388] gpt_neox.layers.28.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 339/ 388] gpt_neox.layers.28.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 340/ 388] gpt_neox.layers.28.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 341/ 388] gpt_neox.layers.28.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 342/ 388] gpt_neox.layers.28.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 343/ 388] gpt_neox.layers.28.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 344/ 388] gpt_neox.layers.28.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 345/ 388] gpt_neox.layers.28.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 346/ 388] gpt_neox.layers.28.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 347/ 388] gpt_neox.layers.28.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 348/ 388] gpt_neox.layers.28.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.106 0.226 0.106 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 349/ 388] gpt_neox.layers.28.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 350/ 388] gpt_neox.layers.29.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 351/ 388] gpt_neox.layers.29.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 352/ 388] gpt_neox.layers.29.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 353/ 388] gpt_neox.layers.29.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 354/ 388] gpt_neox.layers.29.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 355/ 388] gpt_neox.layers.29.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 356/ 388] gpt_neox.layers.29.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 357/ 388] gpt_neox.layers.29.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 358/ 388] gpt_neox.layers.29.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 359/ 388] gpt_neox.layers.29.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 360/ 388] gpt_neox.layers.29.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.020 0.031 0.047 0.067 0.088 0.107 0.227 0.107 0.088 0.067 0.047 0.031 0.020 0.027 \n",
            "[ 361/ 388] gpt_neox.layers.29.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 362/ 388] gpt_neox.layers.30.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 363/ 388] gpt_neox.layers.30.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 364/ 388] gpt_neox.layers.30.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 365/ 388] gpt_neox.layers.30.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 366/ 388] gpt_neox.layers.30.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 367/ 388] gpt_neox.layers.30.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 368/ 388] gpt_neox.layers.30.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 369/ 388] gpt_neox.layers.30.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 370/ 388] gpt_neox.layers.30.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 371/ 388] gpt_neox.layers.30.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 372/ 388] gpt_neox.layers.30.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.019 0.031 0.047 0.066 0.088 0.107 0.229 0.107 0.088 0.066 0.047 0.031 0.019 0.027 \n",
            "[ 373/ 388] gpt_neox.layers.30.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 374/ 388] gpt_neox.layers.31.input_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 375/ 388] gpt_neox.layers.31.input_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 376/ 388] gpt_neox.layers.31.post_attention_layernorm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 377/ 388] gpt_neox.layers.31.post_attention_layernorm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 378/ 388] gpt_neox.layers.31.attention.query_key_value.weight -     4096 x 12288, type =    f16, quantizing .. size =    96.00 MiB ->    54.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 379/ 388] gpt_neox.layers.31.attention.query_key_value.bias -            12288, type =    f32, size =    0.047 MiB\n",
            "[ 380/ 388] gpt_neox.layers.31.attention.dense.weight -     4096 x  4096, type =    f16, quantizing .. size =    32.00 MiB ->    18.00 MiB | hist: 0.000 0.027 0.020 0.032 0.048 0.067 0.088 0.106 0.225 0.106 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 381/ 388] gpt_neox.layers.31.attention.dense.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 382/ 388] gpt_neox.layers.31.mlp.dense_h_to_4h.weight -     4096 x 16384, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.028 0.020 0.032 0.048 0.067 0.088 0.106 0.224 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "[ 383/ 388] gpt_neox.layers.31.mlp.dense_h_to_4h.bias -            16384, type =    f32, size =    0.062 MiB\n",
            "[ 384/ 388] gpt_neox.layers.31.mlp.dense_4h_to_h.weight -    16384 x  4096, type =    f16, quantizing .. size =   128.00 MiB ->    72.00 MiB | hist: 0.000 0.027 0.018 0.029 0.045 0.065 0.088 0.109 0.237 0.109 0.088 0.065 0.045 0.029 0.018 0.026 \n",
            "[ 385/ 388] gpt_neox.layers.31.mlp.dense_4h_to_h.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 386/ 388]     gpt_neox.final_layer_norm.weight -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 387/ 388]       gpt_neox.final_layer_norm.bias -             4096, type =    f32, size =    0.016 MiB\n",
            "[ 388/ 388]                     embed_out.weight -     4096 x 52224, type =    f16, quantizing .. size =   408.00 MiB ->   229.50 MiB | hist: 0.000 0.031 0.028 0.045 0.064 0.081 0.090 0.090 0.166 0.085 0.084 0.076 0.061 0.043 0.027 0.031 \n",
            "gptneox_model_quantize_internal: model size  = 13110.53 MiB\n",
            "gptneox_model_quantize_internal: quant size  =  7377.53 MiB\n",
            "gptneox_model_quantize_internal: hist: 0.000 0.028 0.020 0.032 0.048 0.068 0.088 0.105 0.223 0.105 0.088 0.067 0.048 0.032 0.020 0.027 \n",
            "\n",
            "main: quantize time = 124966.26 ms\n",
            "main:    total time = 124966.26 ms\n",
            "\n",
            "Succesfully quantized all models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src=\"/content/redpajama.cpp/models/ggml-japanese-gpt-neox-3.6b-instruction-sft-q8_0.bin\"\n",
        "target=\"/content/drive/MyDrive/models/rinna_3B_cpp/ggml-rinna-3B-q8_0.bin\"\n",
        "!cp $src $target"
      ],
      "metadata": {
        "id": "YqjVlK1FQmx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title infarence ggml { display-mode: \"form\" }\n",
        "#model =  'rinna/japanese-gpt-neox-3.6b-instruction-sft' #@param {type:\"string\"}\n",
        "model = f'{output_path}/ggml/ggml-hf_ckpt-q8_0.bin'\n",
        "\n",
        "%cd $cpp_repo\n",
        "\n",
        "prompt = [\n",
        "    {\n",
        "        \"speaker\": \"ユーザー\",\n",
        "        \"text\": \"日本のおすすめの観光地を教えてください。\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"システム\",\n",
        "        \"text\": \"どの地域の観光地が知りたいですか？\"\n",
        "    },\n",
        "    {\n",
        "        \"speaker\": \"ユーザー\",\n",
        "        \"text\": \"渋谷の観光地を教えてください。\"\n",
        "    }\n",
        "]\n",
        "prompt = [\n",
        "    f\"{uttr['speaker']}: {uttr['text']}\"\n",
        "    for uttr in prompt\n",
        "]\n",
        "prompt = \"<NL>\".join(prompt)\n",
        "prompt = (\n",
        "    prompt\n",
        "    + \"<NL>\"\n",
        "    + \"システム: \"\n",
        ")\n",
        "\n",
        "instruction = \"日本で1番高い山を教えてください。\"\n",
        "prompt=f\"\"\"\n",
        "以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\n",
        "\n",
        "### 指示:\n",
        "{instruction}\n",
        "\n",
        "### 応答:\n",
        "\"\"\"\n",
        "\n",
        "!./redpajama -m \"$model\" \\\n",
        "  -c 1024 \\\n",
        "  -b 128 \\\n",
        "  -n 1 \\\n",
        "  -t 8 \\\n",
        "  --color \\\n",
        "  --top_k 30 \\\n",
        "  --top_p 0.95 \\\n",
        "  --temp 0.2 \\\n",
        "  --repeat_last_n 3 \\\n",
        "  --repeat_penalty 1.1 \\\n",
        "  --seed 0 \\\n",
        "  --n_predict 256 \\\n",
        "  --verbose-prompt \\\n",
        "  -p \"$prompt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lal3aLRdSQU3",
        "outputId": "7e9efe20-5cbd-4ef9-8e49-5f2c606f925a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/redpajama.cpp\n",
            "main: seed = 0\n",
            "gptneox.cpp: loading model from /content/drive/MyDrive/models/calm_dolly_7B/ggml/ggml-hf_ckpt-q8_0.bin\n",
            "gptneox_model_load_internal: format     = ggjt v1 (latest)\n",
            "gptneox_model_load_internal: n_vocab    = 52224\n",
            "gptneox_model_load_internal: n_ctx      = 1024\n",
            "gptneox_model_load_internal: n_embd     = 4096\n",
            "gptneox_model_load_internal: n_head     = 32\n",
            "gptneox_model_load_internal: n_layer    = 32\n",
            "gptneox_model_load_internal: n_rot      = 128\n",
            "gptneox_model_load_internal: use_parallel_residual = 0\n",
            "gptneox_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
            "gptneox_model_load_internal: n_parts    = 1\n",
            "gptneox_model_load_internal: model size = 7B\n",
            "gptneox_model_load_internal: ggml ctx size =  90.94 KiB\n",
            "gptneox_model_load_internal: mem required  = 9169.62 MiB (+ 1026.00 MiB per state)\n",
            "gptneox_init_from_file: kv self size  =  512.00 MiB\n",
            "\n",
            "system_info: n_threads = 8 / 4 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "\n",
            "main: prompt: '\n",
            "以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\n",
            "\n",
            "### 指示:\n",
            "日本で１番高い山を教えてください。\n",
            "\n",
            "### 応答:\n",
            "'\n",
            "main: number of tokens in prompt = 41\n",
            "   186 -> '\n",
            "'\n",
            " 24284 -> '以下は'\n",
            "   245 -> '、'\n",
            "   345 -> 'ある'\n",
            " 20874 -> '作業を'\n",
            "  4845 -> '記述'\n",
            "   295 -> 'した'\n",
            " 10806 -> '指示'\n",
            "   358 -> 'です'\n",
            "   247 -> '。'\n",
            "  4125 -> '依頼'\n",
            "   255 -> 'を'\n",
            " 21261 -> '適切に'\n",
            "  4792 -> '完了'\n",
            "  1983 -> 'させる'\n",
            " 17945 -> '応答'\n",
            " 10383 -> 'を書き'\n",
            " 11854 -> 'なさい'\n",
            " 32988 -> '\n",
            "\n",
            "'\n",
            " 39843 -> '##'\n",
            "     4 -> '#'\n",
            "   204 -> ' '\n",
            " 10806 -> '指示'\n",
            "    27 -> ':'\n",
            "   186 -> '\n",
            "'\n",
            "  5619 -> '日本で'\n",
            "   242 -> '�'\n",
            "   191 -> '\u0011'\n",
            "   148 -> '�'\n",
            "   854 -> '番'\n",
            "  1737 -> '高い'\n",
            " 37705 -> '山を'\n",
            " 22554 -> '教えてください'\n",
            "   247 -> '。'\n",
            " 32988 -> '\n",
            "\n",
            "'\n",
            " 39843 -> '##'\n",
            "     4 -> '#'\n",
            "   204 -> ' '\n",
            " 17945 -> '応答'\n",
            "    27 -> ':'\n",
            "   186 -> '\n",
            "'\n",
            "\n",
            "sampling: repeat_last_n = 3, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 30, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 1024, n_batch = 128, n_predict = 256, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m\n",
            "以下は、ある作業を記述した指示です。依頼を適切に完了させる応答を書きなさい\n",
            "\n",
            "### 指示:\n",
            "日本で�\u0011�番高い山を教えてください。\n",
            "\n",
            "### 応答:\n",
            "\u001b[0m山は、富士山と北アルプスです。富士山と北アルプスの標高は、3300m、北アルプスの標高は、3180mです。富士山は、3867m、北アルプスは、3180mです。\n",
            "\n",
            "#^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cpp 2 (fail)"
      ],
      "metadata": {
        "id": "VS1L-J9nSNPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/if001/llama.cpp.git\n",
        "cpp_repo=\"/content/llama.cpp\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmwl2xGXSOzc",
        "outputId": "fb1a2e29-5a11-4048-934e-fe7bc963ffd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 2579, done.\u001b[K\n",
            "remote: Counting objects: 100% (1121/1121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (183/183), done.\u001b[K\n",
            "remote: Total 2579 (delta 1024), reused 954 (delta 938), pack-reused 1458\u001b[K\n",
            "Receiving objects: 100% (2579/2579), 2.44 MiB | 8.06 MiB/s, done.\n",
            "Resolving deltas: 100% (1685/1685), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $cpp_repo\n",
        "!mkdir -p build\n",
        "%cd $cpp_repo/build\n",
        "!cmake ..\n",
        "!cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOBOquWbSdTy",
        "outputId": "702455a2-2ec5-4a81-dc28-0204967db739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n",
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.25.1\") \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found Threads: TRUE  \n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  3%] Built target BUILD_INFO\n",
            "[  6%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "[  6%] Built target ggml\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/llama.cpp/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ksize_t llama_set_state_data(llama_context*, const uint8_t*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/llama.cpp:2686:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst uint8_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst unsigned char*\u001b[m\u001b[K’} to type ‘\u001b[01m\u001b[Kvoid*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K-Wcast-qual\u001b[m\u001b[K]\n",
            " 2686 |             kin3d->data = (void *) \u001b[01;35m\u001b[Kinp\u001b[m\u001b[K;\n",
            "      |                                    \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/llama.cpp/llama.cpp:2690:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst uint8_t*\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kconst unsigned char*\u001b[m\u001b[K’} to type ‘\u001b[01m\u001b[Kvoid*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K-Wcast-qual\u001b[m\u001b[K]\n",
            " 2690 |             vin3d->data = (void *) \u001b[01;35m\u001b[Kinp\u001b[m\u001b[K;\n",
            "      |                                    \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "[ 12%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
            "[ 12%] Built target llama\n",
            "[ 15%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 18%] Built target test-quantize-fns\n",
            "[ 21%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 24%] Built target test-quantize-perf\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid test_top_k(const std::vector<float>&, const std::vector<float>&, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:22:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kexpected_probs\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
            "   22 |                 \u001b[01;35m\u001b[Kconst std::vector<float> & expected_probs\u001b[m\u001b[K,\n",
            "      |                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid test_top_p(const std::vector<float>&, const std::vector<float>&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:46:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kexpected_probs\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
            "   46 |                 \u001b[01;35m\u001b[Kconst std::vector<float> & expected_probs\u001b[m\u001b[K,\n",
            "      |                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid test_tfs(const std::vector<float>&, const std::vector<float>&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:71:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kexpected_probs\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
            "   71 |                 \u001b[01;35m\u001b[Kconst std::vector<float> & expected_probs\u001b[m\u001b[K,\n",
            "      |                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid test_typical(const std::vector<float>&, const std::vector<float>&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:94:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kexpected_probs\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
            "   94 |                 \u001b[01;35m\u001b[Kconst std::vector<float> & expected_probs\u001b[m\u001b[K,\n",
            "      |                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid test_repetition_penalty(const std::vector<float>&, const std::vector<int>&, const std::vector<float>&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:119:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kexpected_probs\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
            "  119 |                 \u001b[01;35m\u001b[Kconst std::vector<float> & expected_probs\u001b[m\u001b[K,\n",
            "      |                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid test_frequency_presence_penalty(const std::vector<float>&, const std::vector<int>&, const std::vector<float>&, float, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-sampling.cpp:148:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kexpected_probs\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n",
            "  148 |                 \u001b[01;35m\u001b[Kconst std::vector<float> & expected_probs\u001b[m\u001b[K,\n",
            "      |                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 30%] Built target test-sampling\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-tokenizer-0.cpp:19:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kextra ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wpedantic\u001b[m\u001b[K]\n",
            "   19 | }\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "      |  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 36%] Built target test-tokenizer-0\n",
            "[ 39%] \u001b[32mBuilding CXX object examples/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 39%] Built target common\n",
            "[ 42%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
            "[ 45%] Built target main\n",
            "[ 48%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
            "[ 51%] Built target quantize\n",
            "[ 54%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
            "[ 57%] Built target quantize-stats\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
            "[ 63%] Built target perplexity\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
            "[ 69%] Built target embedding\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
            "[ 75%] Built target save-load-state\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
            "[ 81%] Built target benchmark\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/llama.cpp/examples/baby-llama/baby-llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main(int, char**)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/llama.cpp/examples/baby-llama/baby-llama.cpp:1593:32:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kopt_params_adam\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n",
            " 1593 |         struct ggml_opt_params \u001b[01;35m\u001b[Kopt_params_adam\u001b[m\u001b[K = ggml_opt_default_params(GGML_OPT_ADAM);\n",
            "      |                                \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
            "[ 87%] Built target baby-llama\n",
            "[ 90%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
            "[ 93%] Built target vdot\n",
            "[ 96%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
            "[100%] Built target q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p $cpp_repo/models/3B\n",
        "%cd $cpp_repo/models/3B\n",
        "\n",
        "!wget \"https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/pytorch_model.bin\"\n",
        "\n",
        "%cd $cpp_repo/models\n",
        "\n",
        "!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.model\n",
        "!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.vocab\n",
        "!wget https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_lzIBtRSpqN",
        "outputId": "b77d2b0c-461f-463c-9abc-96957c96245c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/models/3B\n",
            "--2023-05-17 15:23:32--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.24.56, 13.35.24.126, 13.35.24.18, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.24.56|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/0c6124c628f8ecc29be1b6ee0625670062340f5b99cfe543ccf049fa90e6207b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1684596222&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvMGM2MTI0YzYyOGY4ZWNjMjliZTFiNmVlMDYyNTY3MDA2MjM0MGY1Yjk5Y2ZlNTQzY2NmMDQ5ZmE5MGU2MjA3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ1OTYyMjJ9fX1dfQ__&Signature=iiG-JGdHwvk02cqwqDdWzzi%7ENdLdSFZ5J0TDtoAFTiD7OZmDbgHRK90FLa1Y7HRun0LPDCefuyqqXpJpvLQrD3SHUTGDRpNutlnHeb72PpqOMcjRvn33KcYx35sUSzocGve5KfJuLsFKNPp6YfcEHClvN1RanxcZWaN9dw9fMTa9JQvA1NwZjKSZ7wnnZIzzI%7EcyI6A8nT1ELdW6GrZb0NiEgG5t977yt89lEqnTL%7Eib5tqkpA6oaC32v8h6fSpx27pII%7EWOy%7Er9Q-2NJRz68RqlU%7Ehj9I5%7EyzpaX1LBDwUmNmrsElP1ienEGkUhv1qyEbNurZq5xYZEijSKMnkX4Q__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-17 15:23:42--  https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/0c6124c628f8ecc29be1b6ee0625670062340f5b99cfe543ccf049fa90e6207b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1684596222&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvMGM2MTI0YzYyOGY4ZWNjMjliZTFiNmVlMDYyNTY3MDA2MjM0MGY1Yjk5Y2ZlNTQzY2NmMDQ5ZmE5MGU2MjA3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ1OTYyMjJ9fX1dfQ__&Signature=iiG-JGdHwvk02cqwqDdWzzi%7ENdLdSFZ5J0TDtoAFTiD7OZmDbgHRK90FLa1Y7HRun0LPDCefuyqqXpJpvLQrD3SHUTGDRpNutlnHeb72PpqOMcjRvn33KcYx35sUSzocGve5KfJuLsFKNPp6YfcEHClvN1RanxcZWaN9dw9fMTa9JQvA1NwZjKSZ7wnnZIzzI%7EcyI6A8nT1ELdW6GrZb0NiEgG5t977yt89lEqnTL%7Eib5tqkpA6oaC32v8h6fSpx27pII%7EWOy%7Er9Q-2NJRz68RqlU%7Ehj9I5%7EyzpaX1LBDwUmNmrsElP1ienEGkUhv1qyEbNurZq5xYZEijSKMnkX4Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.24.76, 13.35.24.87, 13.35.24.128, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.24.76|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7365670537 (6.9G) [application/octet-stream]\n",
            "Saving to: ‘pytorch_model.bin’\n",
            "\n",
            "pytorch_model.bin   100%[===================>]   6.86G  22.2MB/s    in 5m 28s  \n",
            "\n",
            "2023-05-17 15:29:11 (21.4 MB/s) - ‘pytorch_model.bin’ saved [7365670537/7365670537]\n",
            "\n",
            "/content/llama.cpp/models\n",
            "--2023-05-17 15:29:12--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.model\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.7.88, 13.35.7.102, 13.35.7.28, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.7.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/7d78ab344146700112cd41628ac7ce54b79c0868fe0c7c201750d8237b54dbb4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27spiece.model%3B+filename%3D%22spiece.model%22%3B&Expires=1684591098&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvN2Q3OGFiMzQ0MTQ2NzAwMTEyY2Q0MTYyOGFjN2NlNTRiNzljMDg2OGZlMGM3YzIwMTc1MGQ4MjM3YjU0ZGJiND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ1OTEwOTh9fX1dfQ__&Signature=L-QwXWypKrr1F7NBnqrD2eg7wiJfelmy4VOkzFw38bH1gvAV4zF4C64zgqyOnT3N4VuYPz9AGTGboX7TOVn7y%7ExBnPC1dAvnYrYpfYDGoKCNPYnk-9UnhBJu9z-4GupIA3utigmzW851sxYFkPT-Xk6n8tuvAmygJTzaMOvptcVJ3Xi4ZVgP-3Ek8FlNV-oLfLISviObhrOkLEpO8peBKau5alJ7Y1ycLyvqFQ9KfKIlh-5XDcxKZm2ZqX58Z9SkfiQPZ9i9wgI1HW%7EF3tHkwlQpxcyuBk5yiXqeMX6ASY3m802c-GXWA0KVy24olQ2byDoGUoT50exzcR9teR2hhw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-17 15:29:21--  https://cdn-lfs.huggingface.co/repos/58/e9/58e9ff5c59adafdd9012d69c25c4272a64be26c511f2c3f69fe8a17881ac2120/7d78ab344146700112cd41628ac7ce54b79c0868fe0c7c201750d8237b54dbb4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27spiece.model%3B+filename%3D%22spiece.model%22%3B&Expires=1684591098&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4L2U5LzU4ZTlmZjVjNTlhZGFmZGQ5MDEyZDY5YzI1YzQyNzJhNjRiZTI2YzUxMWYyYzNmNjlmZThhMTc4ODFhYzIxMjAvN2Q3OGFiMzQ0MTQ2NzAwMTEyY2Q0MTYyOGFjN2NlNTRiNzljMDg2OGZlMGM3YzIwMTc1MGQ4MjM3YjU0ZGJiND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODQ1OTEwOTh9fX1dfQ__&Signature=L-QwXWypKrr1F7NBnqrD2eg7wiJfelmy4VOkzFw38bH1gvAV4zF4C64zgqyOnT3N4VuYPz9AGTGboX7TOVn7y%7ExBnPC1dAvnYrYpfYDGoKCNPYnk-9UnhBJu9z-4GupIA3utigmzW851sxYFkPT-Xk6n8tuvAmygJTzaMOvptcVJ3Xi4ZVgP-3Ek8FlNV-oLfLISviObhrOkLEpO8peBKau5alJ7Y1ycLyvqFQ9KfKIlh-5XDcxKZm2ZqX58Z9SkfiQPZ9i9wgI1HW%7EF3tHkwlQpxcyuBk5yiXqeMX6ASY3m802c-GXWA0KVy24olQ2byDoGUoT50exzcR9teR2hhw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.24.87, 13.35.24.128, 13.35.24.38, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.24.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 786216 (768K) [binary/octet-stream]\n",
            "Saving to: ‘spiece.model’\n",
            "\n",
            "spiece.model        100%[===================>] 767.79K   882KB/s    in 0.9s    \n",
            "\n",
            "2023-05-17 15:29:22 (882 KB/s) - ‘spiece.model’ saved [786216/786216]\n",
            "\n",
            "--2023-05-17 15:29:23--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/spiece.vocab\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.7.88, 13.35.7.102, 13.35.7.28, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.7.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 574430 (561K) [text/plain]\n",
            "Saving to: ‘spiece.vocab’\n",
            "\n",
            "spiece.vocab        100%[===================>] 560.97K   808KB/s    in 0.7s    \n",
            "\n",
            "2023-05-17 15:29:33 (808 KB/s) - ‘spiece.vocab’ saved [574430/574430]\n",
            "\n",
            "--2023-05-17 15:29:33--  https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft/resolve/main/config.json\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.7.88, 13.35.7.102, 13.35.7.28, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.7.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 534 [text/plain]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "config.json         100%[===================>]     534  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-17 15:29:43 (130 MB/s) - ‘config.json’ saved [534/534]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkBoge-sVRIy",
        "outputId": "8f82923b-b828-4203-9724-ccfdc0af9645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $cpp_repo/models\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_vhxUQTVYqa",
        "outputId": "2fd31071-feff-4e3b-c75f-a24e5a27b924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/models\n",
            "3B  config.json  ggml-vocab.bin  spiece.model  spiece.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $cpp_repo\n",
        "\n",
        "!python3 convert.py models/3B/ --vocab-dir=\"/content/llama.cpp/models/spiece.model\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmZjWuf_Sf6N",
        "outputId": "ae20807c-3e89-4507-c855-65a71050e5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n",
            "Loading model file models/3B/pytorch_model.bin\n",
            "Loading vocab file /content/llama.cpp/models/spiece.model\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert.py\", line 1171, in <module>\n",
            "    main()\n",
            "  File \"/content/llama.cpp/convert.py\", line 1162, in main\n",
            "    output_type = pick_output_type(model, args.outtype)\n",
            "  File \"/content/llama.cpp/convert.py\", line 981, in pick_output_type\n",
            "    wq_type = model[\"layers.0.attention.wq.weight\"].data_type\n",
            "KeyError: 'layers.0.attention.wq.weight'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lora merge"
      ],
      "metadata": {
        "id": "y35nYsrwKIOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/peft.git transformers sentencepiece torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFwyb_TuKPOW",
        "outputId": "0f477bfb-a9ee-4a6f-c433-6811d6df5f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/peft.git\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-mvkw28jf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-mvkw28jf\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 3714aa2fff158fdfa637b2b65952580801d890b2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (6.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (0.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer  # noqa: E402\n",
        "\n",
        "model_name = 'cyberagent/open-calm-7b'\n",
        "lora_weights = output_path\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=False,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": \"cpu\"},\n",
        ")\n",
        "\n",
        "#first_weight = base_model.model.gpt_neox.layers[0].attention.query_key_value.weight\n",
        "#first_weight_old = first_weight.clone()\n",
        "\n",
        "lora_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    lora_weights,\n",
        "    device_map={\"\": \"cpu\"},\n",
        "    torch_dtype=torch.float16,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "69a4c67d8c484de080d7ac62dfe7d72c",
            "a046d5ce7e6a45f990958d013e83f84c",
            "593957576f004b7d8f99c1fae6e7d1f6",
            "16528f2753cf498eaf00da0258fb427e",
            "013972997f6c407d99983fffe7329938",
            "cf9a8f672eb24a03b5c644376ae36ce0",
            "ddb3f448451e4d028bd00818dfc3b0da",
            "0fac3844a2dd488bb377aebd0252847e",
            "5526bd223c5245a4b82247783c749ea6",
            "3551c1b30926408599d4c73fce71fd53",
            "381650067cbf400289234a1a5d64aa40"
          ]
        },
        "id": "xidfq8mZLMY5",
        "outputId": "1adb4285-06f7-4291-b812-4de908ef11e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69a4c67d8c484de080d7ac62dfe7d72c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merge weights\n",
        "# print(base_model.config)\n",
        "\n",
        "#for layer in lora_model.base_model.model.gpt_neox.layers:\n",
        "#    layer.attention.query_key_value.merge_weights = True\n",
        "  \n",
        "lora_model_sd = lora_model.state_dict()\n",
        "for k, v in lora_model_sd.items():\n",
        "  print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T03WAol-NOEw",
        "outputId": "4e0aa285-7df3-4039-f709-1afe8607f7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model.model.gpt_neox.embed_in.weight\n",
            "base_model.model.gpt_neox.layers.0.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.0.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.0.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.0.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.0.attention.bias\n",
            "base_model.model.gpt_neox.layers.0.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.0.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.0.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.0.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.0.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.0.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.0.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.0.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.0.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.0.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.0.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.1.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.1.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.1.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.1.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.1.attention.bias\n",
            "base_model.model.gpt_neox.layers.1.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.1.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.1.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.1.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.1.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.1.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.1.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.1.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.1.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.1.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.1.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.2.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.2.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.2.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.2.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.2.attention.bias\n",
            "base_model.model.gpt_neox.layers.2.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.2.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.2.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.2.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.2.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.2.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.2.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.2.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.2.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.2.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.2.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.3.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.3.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.3.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.3.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.3.attention.bias\n",
            "base_model.model.gpt_neox.layers.3.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.3.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.3.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.3.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.3.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.3.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.3.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.3.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.3.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.3.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.3.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.4.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.4.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.4.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.4.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.4.attention.bias\n",
            "base_model.model.gpt_neox.layers.4.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.4.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.4.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.4.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.4.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.4.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.4.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.4.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.4.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.4.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.4.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.5.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.5.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.5.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.5.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.5.attention.bias\n",
            "base_model.model.gpt_neox.layers.5.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.5.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.5.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.5.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.5.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.5.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.5.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.5.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.5.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.5.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.5.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.6.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.6.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.6.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.6.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.6.attention.bias\n",
            "base_model.model.gpt_neox.layers.6.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.6.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.6.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.6.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.6.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.6.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.6.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.6.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.6.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.6.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.6.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.7.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.7.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.7.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.7.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.7.attention.bias\n",
            "base_model.model.gpt_neox.layers.7.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.7.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.7.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.7.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.7.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.7.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.7.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.7.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.7.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.7.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.7.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.8.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.8.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.8.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.8.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.8.attention.bias\n",
            "base_model.model.gpt_neox.layers.8.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.8.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.8.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.8.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.8.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.8.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.8.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.8.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.8.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.8.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.8.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.9.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.9.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.9.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.9.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.9.attention.bias\n",
            "base_model.model.gpt_neox.layers.9.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.9.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.9.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.9.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.9.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.9.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.9.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.9.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.9.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.9.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.9.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.10.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.10.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.10.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.10.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.10.attention.bias\n",
            "base_model.model.gpt_neox.layers.10.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.10.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.10.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.10.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.10.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.10.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.10.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.10.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.10.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.10.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.10.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.11.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.11.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.11.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.11.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.11.attention.bias\n",
            "base_model.model.gpt_neox.layers.11.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.11.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.11.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.11.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.11.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.11.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.11.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.11.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.11.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.11.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.11.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.12.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.12.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.12.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.12.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.12.attention.bias\n",
            "base_model.model.gpt_neox.layers.12.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.12.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.12.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.12.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.12.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.12.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.12.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.12.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.12.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.12.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.12.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.13.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.13.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.13.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.13.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.13.attention.bias\n",
            "base_model.model.gpt_neox.layers.13.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.13.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.13.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.13.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.13.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.13.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.13.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.13.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.13.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.13.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.13.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.14.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.14.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.14.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.14.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.14.attention.bias\n",
            "base_model.model.gpt_neox.layers.14.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.14.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.14.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.14.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.14.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.14.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.14.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.14.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.14.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.14.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.14.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.15.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.15.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.15.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.15.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.15.attention.bias\n",
            "base_model.model.gpt_neox.layers.15.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.15.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.15.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.15.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.15.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.15.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.15.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.15.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.15.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.15.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.15.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.16.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.16.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.16.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.16.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.16.attention.bias\n",
            "base_model.model.gpt_neox.layers.16.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.16.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.16.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.16.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.16.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.16.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.16.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.16.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.16.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.16.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.16.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.17.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.17.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.17.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.17.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.17.attention.bias\n",
            "base_model.model.gpt_neox.layers.17.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.17.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.17.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.17.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.17.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.17.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.17.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.17.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.17.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.17.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.17.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.18.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.18.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.18.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.18.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.18.attention.bias\n",
            "base_model.model.gpt_neox.layers.18.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.18.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.18.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.18.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.18.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.18.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.18.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.18.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.18.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.18.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.18.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.19.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.19.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.19.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.19.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.19.attention.bias\n",
            "base_model.model.gpt_neox.layers.19.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.19.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.19.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.19.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.19.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.19.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.19.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.19.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.19.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.19.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.19.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.20.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.20.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.20.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.20.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.20.attention.bias\n",
            "base_model.model.gpt_neox.layers.20.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.20.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.20.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.20.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.20.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.20.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.20.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.20.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.20.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.20.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.20.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.21.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.21.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.21.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.21.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.21.attention.bias\n",
            "base_model.model.gpt_neox.layers.21.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.21.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.21.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.21.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.21.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.21.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.21.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.21.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.21.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.21.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.21.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.22.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.22.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.22.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.22.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.22.attention.bias\n",
            "base_model.model.gpt_neox.layers.22.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.22.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.22.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.22.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.22.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.22.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.22.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.22.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.22.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.22.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.22.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.23.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.23.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.23.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.23.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.23.attention.bias\n",
            "base_model.model.gpt_neox.layers.23.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.23.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.23.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.23.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.23.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.23.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.23.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.23.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.23.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.23.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.23.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.24.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.24.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.24.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.24.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.24.attention.bias\n",
            "base_model.model.gpt_neox.layers.24.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.24.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.24.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.24.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.24.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.24.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.24.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.24.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.24.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.24.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.24.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.24.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.25.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.25.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.25.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.25.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.25.attention.bias\n",
            "base_model.model.gpt_neox.layers.25.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.25.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.25.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.25.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.25.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.25.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.25.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.25.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.25.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.25.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.25.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.25.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.26.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.26.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.26.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.26.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.26.attention.bias\n",
            "base_model.model.gpt_neox.layers.26.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.26.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.26.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.26.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.26.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.26.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.26.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.26.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.26.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.26.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.26.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.26.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.27.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.27.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.27.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.27.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.27.attention.bias\n",
            "base_model.model.gpt_neox.layers.27.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.27.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.27.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.27.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.27.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.27.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.27.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.27.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.27.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.27.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.27.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.27.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.28.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.28.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.28.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.28.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.28.attention.bias\n",
            "base_model.model.gpt_neox.layers.28.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.28.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.28.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.28.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.28.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.28.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.28.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.28.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.28.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.28.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.28.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.28.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.29.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.29.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.29.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.29.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.29.attention.bias\n",
            "base_model.model.gpt_neox.layers.29.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.29.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.29.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.29.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.29.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.29.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.29.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.29.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.29.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.29.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.29.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.29.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.30.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.30.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.30.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.30.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.30.attention.bias\n",
            "base_model.model.gpt_neox.layers.30.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.30.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.30.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.30.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.30.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.30.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.30.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.30.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.30.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.30.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.30.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.30.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.layers.31.input_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.31.input_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.31.post_attention_layernorm.weight\n",
            "base_model.model.gpt_neox.layers.31.post_attention_layernorm.bias\n",
            "base_model.model.gpt_neox.layers.31.attention.bias\n",
            "base_model.model.gpt_neox.layers.31.attention.masked_bias\n",
            "base_model.model.gpt_neox.layers.31.attention.rotary_emb.inv_freq\n",
            "base_model.model.gpt_neox.layers.31.attention.query_key_value.weight\n",
            "base_model.model.gpt_neox.layers.31.attention.query_key_value.bias\n",
            "base_model.model.gpt_neox.layers.31.attention.query_key_value.lora_A.default.weight\n",
            "base_model.model.gpt_neox.layers.31.attention.query_key_value.lora_B.default.weight\n",
            "base_model.model.gpt_neox.layers.31.attention.dense.weight\n",
            "base_model.model.gpt_neox.layers.31.attention.dense.bias\n",
            "base_model.model.gpt_neox.layers.31.mlp.dense_h_to_4h.weight\n",
            "base_model.model.gpt_neox.layers.31.mlp.dense_h_to_4h.bias\n",
            "base_model.model.gpt_neox.layers.31.mlp.dense_4h_to_h.weight\n",
            "base_model.model.gpt_neox.layers.31.mlp.dense_4h_to_h.bias\n",
            "base_model.model.gpt_neox.final_layer_norm.weight\n",
            "base_model.model.gpt_neox.final_layer_norm.bias\n",
            "base_model.model.embed_out.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $output_path\n",
        "\n",
        "import json\n",
        "import os\n",
        "from transformers import GPTNeoXForCausalLM\n",
        "\n",
        "#for layer in lora_model.base_model.model.gpt_neox.layers:\n",
        "#    layer.attention.query_key_value.merge_weights = True\n",
        "\n",
        "\n",
        "# merge weights - new merging method from peft\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "\n",
        "merged_model.train(False)\n",
        "\n",
        "# did we do anything?\n",
        "# assert not torch.allclose(first_weight_old, first_weight)\n",
        "\n",
        "lora_model_sd = merged_model.state_dict()\n",
        "deloreanized_sd = {\n",
        "    k.replace(\"base_model.model.\", \"\"): v\n",
        "    for k, v in lora_model_sd.items()\n",
        "    if \"lora\" not in k\n",
        "}\n",
        "\n",
        "os.makedirs(f'{output_path}/hf_ckpt', exist_ok=True)\n",
        "GPTNeoXForCausalLM.save_pretrained(\n",
        "    base_model,\n",
        "    save_directory=f\"{output_path}/hf_ckpt\", state_dict=deloreanized_sd, max_shard_size=\"400MB\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCyouPi1RMxl",
        "outputId": "bdd4e647-1952-4bca-98b5-4f1e85e65452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/models/calm_dolly_7B\n"
          ]
        }
      ]
    }
  ]
}